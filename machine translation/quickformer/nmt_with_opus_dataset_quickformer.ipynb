{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA4A3Az6obfO"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2e76qcRCobfP"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# import os\n",
        "# os.environ[\"CUDA_DIR\"] = \"/home/tuk/anaconda3/pkgs/cuda-nvcc-11.6.124-hbba6d2d_0/nvvm\"\n",
        "# !echo ${CUDA_DIR}\\\n",
        "\n",
        "# print(tf.sysconfig.get_build_info() )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrZrEYICobfQ",
        "outputId": "f0ecd18c-8e71-4533-e683-80d779a37021"
      },
      "outputs": [],
      "source": [
        "# Install the most re version of TensorFlow to use the improved\n",
        "# masking support for `tf.keras.layers.MultiHeadAttention`.\n",
        "# !apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
        "# !pip install protobuf~=3.20.3\n",
        "# !pip install -q tensorflow_datasets\n",
        "# !pip install -q -U tensorflow-text tensorflow\n",
        "\n",
        "# !pip install transformers\n",
        "# !pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2Z8D_-yqobfQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2024-01-30 18:09:35.585668: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-30 18:09:35.585696: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-30 18:09:35.586391: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-30 18:09:35.591499: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-30 18:09:36.268977: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_text\n",
        "\n",
        "import transformers\n",
        "import datasets\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/tuk/anaconda3/envs/.cache/huggingface/datasets\n"
          ]
        }
      ],
      "source": [
        "!export HF_DATASETS_CACHE=\"/home/tuk/anaconda3/.cache/huggingface/datasets\"\n",
        "!echo ${HF_DATASETS_CACHE} "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKgXqJvMobfQ"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275,
          "referenced_widgets": [
            "6acfd8bfffba491ab26bd0cd5d7b9b5a",
            "c52bede33b9b48f890611a21e3c7f815",
            "75b5cd22834746daba6455c0bdd0eb3a",
            "636c29e4113e41c5ac544452c8c881da",
            "30bc82e80fb64e1c8f54137f03deb2ce",
            "8a90f328279449ee845230728c90c1c6",
            "101214c744ca48e9a891440cffb6be04",
            "bb7ce590adce48a9966b0bab35dd99ff",
            "839408a55582449fb57e26d1c63c9578",
            "9396ddc885df46248ad3eb26ae26e114",
            "37e41a95c2e44348a5efa428518b6087",
            "ac58fbaf420143828f2ccdf7824aca44",
            "19078d674b40454db42eef4613312cc8",
            "87667ad986a643c8a64226656a90a93c",
            "844e668e1206401f9fe3bf8e95694288",
            "d0b8215ea74a4bf98bdbcab831d97344",
            "85f4ed22ef0c4fe5bc238ba79b455282",
            "694af1363f6b4a30b4c94ef7738994fd",
            "d523043fe3f7494eb7324f01a5b5ce00",
            "3c92b27dc60741da9be9ccc99f73d378",
            "26547b5811e54faebe60a26534223d99",
            "c16d473c9c0c462faa4b9bce88aa7689",
            "b545a45778be4e419d7be3eff31c11a7",
            "44caa122c7b34b73ae89bda1987dc51d",
            "2386868cd2294786a0a7f7303f4915e0",
            "5a9f54b9007c48aab2aa0df019afef85",
            "63941e4f655e46659f725befa5b3d6ce",
            "486a9cd8a6be4ec08eb6b45cc0f5b364",
            "4a219508a82a47a89270fceb1c5e3417",
            "6e7273229bb3478f9a0f27f78ed99dbe",
            "c2892bcbaf5d4d62b0637250b824f386",
            "048473bedaf54822a40891dec1051d46",
            "d96b2eee1e7f4f1885c74dce5adc0e1e",
            "afd0dd2fc56046af8f23a554ac7c6ed7",
            "2d87b691ad7a450b9073b91b35bec33a",
            "1f2a526f41e0418aa30f933cffd55df4",
            "783f52bdd0cc423198b3e7b7c4262bfe",
            "08ef4bbd6b1a4b77b1e8fc14a1814de7",
            "8aa478c2c0b545daa39e831ffc87f695",
            "d3f9cc3967314482bfbedb394f3b0f12",
            "8e9d5fad71d043a896128c05ad757b02",
            "90bc1de718a1447c9a013366d946771a",
            "1d3693eb2d3a41c98e8dd529076fdfbc",
            "ee551a94920b4f9693a850e68ed7572d",
            "2b2c473652a54596a2a9a4248d30af39",
            "a4cd7b655b5649ca9453dcfaf033df63",
            "0a923ebe2caf43509f2d98c056e3afd3",
            "531282d34fdd48f98578f55adc427d62",
            "f87e449cbcad4129a3a7f5c3a426cc02",
            "3bbfdc5583184d2087aab8887d27d677",
            "eacc07572c86432d9acc5f0c5612776a",
            "0f9925afbff445dd85e7211c2053526c",
            "7501ce04f2c9484ead067312cfea1f86",
            "b7a4e18f53664877b03ac26e13741162",
            "04ca613a16974e96b5e6f5a5ddcbc730"
          ]
        },
        "id": "-JghimPqobfR",
        "outputId": "3046c014-e0ed-4b09-a73f-e09c968d0fea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/tensorflow_datasets/core/dataset_builders/huggingface_dataset_builder.py:160: FutureWarning: list_datasets is deprecated and will be removed in the next major version of datasets. Use 'huggingface_hub.list_datasets' instead.\n",
            "  hf_names = hf_datasets.list_datasets()\n",
            "2024-01-30 18:10:25.133858: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-30 18:10:25.471068: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-30 18:10:25.471631: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-30 18:10:25.475698: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-30 18:10:25.476250: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-30 18:10:25.476704: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-30 18:10:25.568269: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-30 18:10:25.568395: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-30 18:10:25.568487: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-30 18:10:25.568546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22172 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
          ]
        }
      ],
      "source": [
        "examples, metadata = tfds.load('huggingface:opus_books/en-fr', with_info=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1dXeN0gobfR",
        "outputId": "62fd2701-826e-45ff-baf2-36b7d1c5f232"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<_PrefetchDataset element_spec={'id': TensorSpec(shape=(), dtype=tf.string, name=None), 'translation': {'en': TensorSpec(shape=(), dtype=tf.string, name=None), 'fr': TensorSpec(shape=(), dtype=tf.string, name=None)}}>\n"
          ]
        }
      ],
      "source": [
        "examples = examples['train']\n",
        "print(examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MbR10B3obfR"
      },
      "source": [
        "## Make train test splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPVdi641obfR",
        "outputId": "55c88358-5ece-48c0-b5e6-da48f1dd341e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "101668 127085\n"
          ]
        }
      ],
      "source": [
        "# Get the total number of examples\n",
        "total_examples = metadata.splits['train'].num_examples\n",
        "\n",
        "# Define the split ratios (e.g., 80% train, 20% test)\n",
        "train_ratio = 0.8\n",
        "test_ratio = 1.0 - train_ratio\n",
        "\n",
        "# Calculate the number of examples for each split\n",
        "train_size = int(train_ratio * total_examples)\n",
        "print(train_size, total_examples)\n",
        "\n",
        "# # Split the dataset into train and test dictionaries\n",
        "# train_examples = list(examples)[:train_size]\n",
        "# test_examples = list(examples)[train_size:]\n",
        "\n",
        "# HARD CODING\n",
        "# Split the dataset into train and test dictionaries\n",
        "train_examples = list(examples)[:10000]\n",
        "test_examples = list(examples)[10000:13000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrLqirLYobfR",
        "outputId": "eadfd620-3985-4ff6-9801-a27f27a4cc6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000\n",
            "3000\n"
          ]
        }
      ],
      "source": [
        "# Print lengths\n",
        "print(len(train_examples))\n",
        "print(len(test_examples))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR-eifW2obfR"
      },
      "source": [
        "## Process dataset for training the tensorflow model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PHNNzjGDobfR"
      },
      "outputs": [],
      "source": [
        "formatted_train_examples = []  # list of tuples\n",
        "formatted_test_examples = []  # list of tuples\n",
        "for record in train_examples:\n",
        "    formatted_train_examples.append(\n",
        "        (record['translation']['fr'].numpy(), record['translation']['en'].numpy()))\n",
        "\n",
        "for record in test_examples:\n",
        "    formatted_test_examples.append(\n",
        "        (record['translation']['fr'].numpy(), record['translation']['en'].numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t24DLwssobfS"
      },
      "outputs": [],
      "source": [
        "# Separate the pairs into two lists\n",
        "fr_train_list, en_train_list = zip(*formatted_train_examples)\n",
        "fr_test_list, en_test_list = zip(*formatted_test_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ihbooxB4obfS"
      },
      "outputs": [],
      "source": [
        "# Create TensorFlow tensors from the values\n",
        "fr_train_tensor = tf.stack(fr_train_list)\n",
        "en_train_tensor = tf.stack(en_train_list)\n",
        "fr_test_tensor = tf.stack(fr_test_list)\n",
        "en_test_tensor = tf.stack(en_test_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XuVOx0SGobfS"
      },
      "outputs": [],
      "source": [
        "# Create a dataset from the tensors\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((fr_train_tensor, en_train_tensor))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((fr_test_tensor, en_test_tensor))\n",
        "\n",
        "# Prefetch the datasets for better performance\n",
        "train_examples = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test_examples = test_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7__ikf2kobfS",
        "outputId": "66294584-f28e-47bd-ce12-173d5b947464"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Examples in French:\n",
            "Le grand Meaulnes\n",
            "Alain-Fournier\n",
            "PREMIÃˆRE PARTIE\n",
            "\n",
            "> Examples in English:\n",
            "The Wanderer\n",
            "Alain-Fournier\n",
            "First Part\n"
          ]
        }
      ],
      "source": [
        "for fr_examples, en_examples in train_examples.batch(3).take(1):\n",
        "  print('> Examples in French:')\n",
        "  for fr in fr_examples.numpy():\n",
        "    print(fr.decode('utf-8'))\n",
        "  print()\n",
        "\n",
        "  print('> Examples in English:')\n",
        "  for en in en_examples.numpy():\n",
        "    print(en.decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzeOXVH3obfS"
      },
      "source": [
        "## Tokenize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 340,
      "metadata": {
        "id": "LPw4NJmQobfS"
      },
      "outputs": [],
      "source": [
        "# Convert tensors to numpy arrays\n",
        "fr_train_array = fr_train_tensor.numpy()\n",
        "en_train_array = en_train_tensor.numpy()\n",
        "fr_test_array = fr_test_tensor.numpy()\n",
        "en_test_array = en_test_tensor.numpy()\n",
        "\n",
        "# Convert numpy arrays to lists of strings\n",
        "fr_train_texts = ['START ' + fr.decode('utf-8') + ' END' for fr in fr_train_array]\n",
        "en_train_texts = ['START ' + en.decode('utf-8') + ' END' for en in en_train_array]\n",
        "fr_test_texts = ['START ' + fr.decode('utf-8') + ' END' for fr in fr_test_array]\n",
        "en_test_texts = ['START ' + en.decode('utf-8') + ' END' for en in en_test_array]\n",
        "\n",
        "# Tokenizer for English\n",
        "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "en_tokenizer.fit_on_texts(en_train_texts + en_test_texts)\n",
        "\n",
        "# Tokenizer for French\n",
        "fr_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "fr_tokenizer.fit_on_texts(fr_train_texts + fr_test_texts)\n",
        "\n",
        "# Concatenate train and test texts for padding\n",
        "all_en_texts = en_train_texts + en_test_texts\n",
        "all_fr_texts = fr_train_texts + fr_test_texts\n",
        "\n",
        "# Tokenize and pad the sequences for English\n",
        "en_sequences = en_tokenizer.texts_to_sequences(all_en_texts)\n",
        "en_max_length = max(len(seq) for seq in en_sequences)\n",
        "en_padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    en_sequences,\n",
        "    maxlen=en_max_length,\n",
        "    padding='post'\n",
        ")\n",
        "\n",
        "# Tokenize and pad the sequences for French\n",
        "fr_sequences = fr_tokenizer.texts_to_sequences(all_fr_texts)\n",
        "fr_max_length = max(len(seq) for seq in fr_sequences)\n",
        "fr_padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    fr_sequences,\n",
        "    maxlen=fr_max_length,\n",
        "    padding='post'\n",
        ")\n",
        "\n",
        "# Split padded sequences back into train and test sets for English\n",
        "en_train_sequences = en_padded_sequences[:len(en_train_texts)]\n",
        "en_test_sequences = en_padded_sequences[len(en_train_texts):]\n",
        "\n",
        "# Split padded sequences back into train and test sets for French\n",
        "fr_train_sequences = fr_padded_sequences[:len(fr_train_texts)]\n",
        "fr_test_sequences = fr_padded_sequences[len(fr_train_texts):]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 345,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_9OhZP7obfT",
        "outputId": "f39252f7-16a8-4063-acd0-e13a50f9c93b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(207,)\n",
            "(207,)\n",
            "(156,)\n",
            "(156,)\n"
          ]
        }
      ],
      "source": [
        "# Note: This cell has nothing to do with the cells below\n",
        "en = en_train_sequences[0]\n",
        "fr = fr_train_sequences[0]\n",
        "en_test = en_test_sequences[0]\n",
        "fr_test = fr_test_sequences[0]\n",
        "\n",
        "print(en_test.shape)\n",
        "print(en.shape)\n",
        "print(fr_test.shape)\n",
        "print(fr.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 346,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Tr1M8BbobfT",
        "outputId": "d5d723b7-dba4-404b-e980-4bffee3d0324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized English training sequences:\n",
            "207\n",
            "[   3    1 6720    2    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0]\n",
            "\n",
            "Tokenized French training sequences:\n",
            "[  1   6 104 120   2   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "\n",
            "Tokenized English test sequences:\n",
            "[    3    56    16   903     6 24760     4    12  2838    22   226    21\n",
            "   215    14     7   133    34  1897 24761     4 24762     2     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0]\n",
            "\n",
            "Tokenized French test sequences:\n",
            "[    1  3885     9 27585     4    13  3541    45   288    34    26   125\n",
            "    23    22  4512  6821 27586     4  7257     2     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0]\n"
          ]
        }
      ],
      "source": [
        "# Print the tokenized sequences\n",
        "print(\"Tokenized English training sequences:\")\n",
        "for seq in en_train_sequences:\n",
        "    print(seq.shape[0])\n",
        "    print(seq)\n",
        "    break\n",
        "\n",
        "print(\"\\nTokenized French training sequences:\")\n",
        "for seq in fr_train_sequences:\n",
        "    print(seq)\n",
        "    break\n",
        "\n",
        "print(\"\\nTokenized English test sequences:\")\n",
        "for seq in en_test_sequences:\n",
        "    print(seq)\n",
        "    break\n",
        "\n",
        "print(\"\\nTokenized French test sequences:\")\n",
        "for seq in fr_test_sequences:\n",
        "    print(seq)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 347,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_mmr6VTobfT",
        "outputId": "14ef4b7c-cb91-4775-924a-9607473b824c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example Input (French): (32, 156)\n",
            "Example Input (English - Input): (32, 206)\n",
            "Example Label (English - Output): (32, 206)\n",
            "Example Input (French): (32, 156)\n",
            "Example Input (English - Input): (32, 206)\n",
            "Example Label (English - Output): (32, 206)\n"
          ]
        }
      ],
      "source": [
        "# Set batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create TensorFlow datasets for training and testing with batching\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(((fr_train_sequences, en_train_sequences[:, :-1]), en_train_sequences[:, 1:]))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(fr_train_sequences)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(((fr_test_sequences, en_test_sequences[:, :-1]), en_test_sequences[:, 1:]))\n",
        "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Print the first example from the training dataset\n",
        "for (fr, en), en_label in train_dataset.take(1):\n",
        "    print(\"Example Input (French):\", fr.shape)\n",
        "    print(\"Example Input (English - Input):\", en.shape)\n",
        "    print(\"Example Label (English - Output):\", en_label.shape)\n",
        "\n",
        "for (fr_test, en_test), en_label_test in test_dataset.take(1):\n",
        "    print(\"Example Input (French):\", fr_test.shape)\n",
        "    print(\"Example Input (English - Input):\", en_test.shape)\n",
        "    print(\"Example Label (English - Output):\", en_label_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHZ5SzQpobfT"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 350,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctTpfBhOVdw-",
        "outputId": "071cc29a-edb7-4c0b-e3e0-aaabe501d3a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English (decoder) vocabulary size: 31052\n",
            "French (encoder) vocabulary size: 33294\n"
          ]
        }
      ],
      "source": [
        "decoder_vocab_size =  len(en_tokenizer.word_index) + 1\n",
        "encoder_vocab_size =  len(fr_tokenizer.word_index) + 1\n",
        "\n",
        "print('English (decoder) vocabulary size:', decoder_vocab_size)\n",
        "print('French (encoder) vocabulary size:', encoder_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 351,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eojCiRq5Vdw_",
        "outputId": "8a660b90-6a51-471c-a534-cb616a55c33e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train\n",
            "156\n",
            "206\n",
            "test\n",
            "156\n",
            "206\n"
          ]
        }
      ],
      "source": [
        "print('train')\n",
        "encoder_block_size = fr.shape[1]  # FOR TRAIN ONLY\n",
        "decoder_block_size = en.shape[1]  # FOR TRAIN ONLY\n",
        "print(encoder_block_size)\n",
        "print(decoder_block_size)\n",
        "\n",
        "print('test')\n",
        "encoder_block_size_test = fr_test.shape[1]  # FOR TEST ONLY\n",
        "decoder_block_size_test = en_test.shape[1]  # FOR TEST ONLY\n",
        "print(encoder_block_size_test)\n",
        "print(decoder_block_size_test)\n",
        "\n",
        "sqrt_d = 24  # Should be divisible by number of heads\n",
        "d_model = 576  # CAUTION: this must be divisible by 2 (required by positional encoding function)\n",
        "# d model should also be a perfect square\n",
        "\n",
        "num_layers = 4\n",
        "dff = 512\n",
        "num_heads = 4\n",
        "dropout_rate = 0.3\n",
        "batch_size = batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 395,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEW6ve9JobfT",
        "outputId": "76339f28-f0c6-4b22-a5fc-4959fb14fc90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([5, 10])"
            ]
          },
          "execution_count": 395,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def positional_encoding(length, depth):\n",
        "    depth = depth / 2\n",
        "    # Step 1: Create a 1D array of positions from 0 to length - 1\n",
        "    positions = np.arange(length)[:, np.newaxis]  # Shape: (length, 1)\n",
        "    # Step 2: Create a 1D array of depths from 0 to depth-1, normalized by depth\n",
        "    depths = np.arange(depth)[np.newaxis, :] / depth  # Shape: (1, depth)\n",
        "    # Step 3: Compute angle rates for each depth\n",
        "    angle_rates = 1 / (10000**depths)  # Shape: (1, depth)\n",
        "    # Step 4: Compute angle radians for each position and depth\n",
        "    angle_rads = positions * angle_rates  # Shape: (length, depth)\n",
        "    # Step 5: Concatenate sin and cos of angle_rads along the last axis\n",
        "    pos_encoding = np.concatenate(\n",
        "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "        axis=-1)  # Shape: (length, 2 * depth)\n",
        "    # Step 6: Convert the resulting array to float32 using TensorFlow\n",
        "    \n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "\n",
        "positional_encoding(5,9).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 396,
      "metadata": {
        "id": "x9p9WmkOobfU"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, block_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.block_size = block_size\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)  # it's output is of size: (block_size, d_model)\n",
        "    # print('debugging:')\n",
        "    # print(block_size)\n",
        "    # print(d_model)\n",
        "    self.pos_encoding = positional_encoding(block_size, self.d_model)  # output: (block_size, d_model*2)\n",
        "    # print('and the position encoding matrix is of shape:')\n",
        "    # print(self.pos_encoding.shape)\n",
        "    \n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]  # No of words (or block size)\n",
        "    x = self.embedding(x)  # (batch_size, block_size, d_model)\n",
        "    # print('i am here...')\n",
        "    # print(x.shape)\n",
        "\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    print(self.pos_encoding.shape)\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :self.d_model]\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 397,
      "metadata": {
        "id": "CygXk9QnobfU"
      },
      "outputs": [],
      "source": [
        "embed_fr = PositionalEmbedding(vocab_size=encoder_vocab_size, block_size=encoder_block_size, d_model=d_model)\n",
        "embed_en = PositionalEmbedding(vocab_size=decoder_vocab_size, block_size=decoder_block_size, d_model=d_model)\n",
        "\n",
        "# fr = tf.stack(fr_train_sequences[0:batch_size])\n",
        "# en = tf.stack(en_train_sequences[0:batch_size])\n",
        "\n",
        "# print('=============================================')\n",
        "# print(french_examples.shape, english_examples.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 398,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FwxuaxvobfU",
        "outputId": "1e65decb-71ee-40dc-cd5f-882dcfd56540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "French input shape: (32, 156)\n",
            "(156, 576)\n",
            "french embedded output: (32, 156, 576)\n",
            "English input shape: (32, 206)\n",
            "(206, 576)\n",
            "(32, 206, 576)\n"
          ]
        }
      ],
      "source": [
        "print('French input shape:', fr.shape)\n",
        "fr_emb = embed_fr(fr)\n",
        "print('french embedded output:', fr_emb.shape)\n",
        "\n",
        "print('English input shape:', en.shape)\n",
        "en_emb = embed_en(en)\n",
        "print(en_emb.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 399,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLh928lbobfU",
        "outputId": "8199177c-1c1e-4c4b-cb11-392e32441428"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(32, 206), dtype=bool, numpy=\n",
              "array([[ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       ...,\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False]])>"
            ]
          },
          "execution_count": 399,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "en_emb._keras_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlDeYsyMVdxD"
      },
      "source": [
        "### Feebler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 400,
      "metadata": {
        "id": "LHufSgFKVdxD"
      },
      "outputs": [],
      "source": [
        "class Feebler(tf.keras.layers.Layer):\n",
        "    def __init__(self, batch_size, sqrt_d, block_size):\n",
        "        super(Feebler, self).__init__()  # Call the superclass initializer\n",
        "        self.weis = self.add_weight(shape=(sqrt_d, sqrt_d, block_size),\n",
        "                                    initializer='random_normal',\n",
        "                                    trainable=True)\n",
        "        self.batch_size = batch_size\n",
        "        self.sqrt_d = sqrt_d\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __call__(self, data):\n",
        "        # incoming tensor is of shape: (b, words, d)\n",
        "        # Feebler works with shape: (b, d, words)\n",
        "        batch_size = data.shape[0]\n",
        "        block_size = data.shape[1]\n",
        "        # print('--------------------------------------------')\n",
        "        # print('I am inside feebler')\n",
        "        # print(data.shape)\n",
        "        # print(batch_size)\n",
        "        # print('--------------------------------------------')\n",
        "        data = tf.transpose(data, (0, 2, 1))\n",
        "        # print('transpose taken')\n",
        "        # print(data.shape)\n",
        "        # print(batch_size, self.sqrt_d, self.sqrt_d, block_size) \n",
        "\n",
        "        data_reshaped = tf.reshape(data, (batch_size, self.sqrt_d, self.sqrt_d, block_size))\n",
        "        product = data_reshaped * self.weis\n",
        "        # print(product.shape)\n",
        "        updated_product = tf.reduce_sum(product, axis=2)\n",
        "        # print(updated_product.shape)\n",
        "        return tf.reshape(updated_product, (batch_size, block_size, self.sqrt_d))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 401,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxXDOPsBVdxE",
        "outputId": "111f1708-f01e-47db-98fe-61b5c4da2329"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 206, 576)\n",
            "(32, 206, 24)\n"
          ]
        }
      ],
      "source": [
        "data = tf.random.normal((batch_size, decoder_block_size, d_model))\n",
        "print(data.shape)\n",
        "feebler = Feebler(batch_size, sqrt_d, decoder_block_size)\n",
        "output = feebler(data)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDnlDoXIVdxF"
      },
      "source": [
        "### Booster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 402,
      "metadata": {
        "id": "uzmYJDRVVdxG"
      },
      "outputs": [],
      "source": [
        "class Booster(tf.keras.layers.Layer):\n",
        "    def __init__(self, batch_size, sqrt_d, block_size, n_embd):\n",
        "        super(Booster, self).__init__()  # Call the superclass initializer\n",
        "        self.weis = self.add_weight(shape=(sqrt_d, sqrt_d, block_size),\n",
        "                                    initializer='random_normal',\n",
        "                                    trainable=True)\n",
        "        self.batch_size = batch_size\n",
        "        self.sqrt_d = sqrt_d\n",
        "        self.block_size = block_size\n",
        "        self.n_embd = n_embd\n",
        "    def __call__(self, attention_output):\n",
        "        batch_size = attention_output.shape[0]\n",
        "        block_size = attention_output.shape[1]\n",
        "        attention_output = tf.reshape(attention_output, (batch_size, self.sqrt_d, block_size))\n",
        "        attention_output_reshaped = tf.reshape(attention_output, (batch_size, 1, -1))\n",
        "        attention_output_reshaped = tf.tile(attention_output_reshaped, [1, self.sqrt_d, 1])\n",
        "        attention_output_reshaped = tf.reshape(attention_output_reshaped, (batch_size, self.sqrt_d, self.sqrt_d, block_size))\n",
        "        revived_output = self.weis * attention_output_reshaped\n",
        "        revived_output = tf.reshape(revived_output, (-1, block_size))\n",
        "        return tf.reshape(revived_output, (batch_size, block_size, self.n_embd))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 403,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8qEd_h4VdxG",
        "outputId": "e7fca08d-40e2-40e5-da5b-7903b46941f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 206, 24)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TensorShape([32, 206, 576])"
            ]
          },
          "execution_count": 403,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(output.shape)\n",
        "booster = Booster(batch_size, sqrt_d, decoder_block_size, d_model)\n",
        "booster(output).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRjeGHCLVdxH"
      },
      "source": [
        "### Quick Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 404,
      "metadata": {
        "id": "rOSsWUM1VdxH"
      },
      "outputs": [],
      "source": [
        "# I'll come for you dear, don't worry!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 405,
      "metadata": {
        "id": "kk4r-sWuobfU"
      },
      "outputs": [],
      "source": [
        "# MultiHeadAttention Layer\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_heads, key_dim, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.key_dim = key_dim\n",
        "        self.dropout = dropout\n",
        "\n",
        "        assert key_dim % num_heads == 0\n",
        "\n",
        "        self.depth = key_dim // num_heads\n",
        "\n",
        "        self.query_dense = tf.keras.layers.Dense(key_dim)\n",
        "        self.key_dense = tf.keras.layers.Dense(key_dim)\n",
        "        self.value_dense = tf.keras.layers.Dense(key_dim)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(key_dim)\n",
        "\n",
        "    def _masked_softmax(self, attention_scores, attention_mask):\n",
        "        \"\"\"Computes a softmax with explicit padding.\n",
        "\n",
        "        Args:\n",
        "            attention_scores: A tensor with shape [batch_size, num_heads,\n",
        "                query_seq_length, memory_seq_length]\n",
        "            attention_mask: A tensor with shape [batch_size, 1, query_seq_length,\n",
        "                memory_seq_length], the original attention mask.\n",
        "\n",
        "        Returns:\n",
        "            attention_probs: A tensor with shape [batch_size, num_heads,\n",
        "                query_seq_length, memory_seq_length], the new attention mask.\n",
        "        \"\"\"\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        # `attention_scores` = [B, N, T, S]\n",
        "        attention_mask = tf.expand_dims(\n",
        "            attention_mask, axis=1\n",
        "        )\n",
        "\n",
        "        # Add a large negative value to masked positions to make their attention scores close to zero\n",
        "        attention_scores = attention_scores + (1.0 - tf.cast(attention_mask, tf.float32)) * -1e9\n",
        "\n",
        "        # Calculate softmax along the last dimension (memory_seq_length)\n",
        "        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\n",
        "\n",
        "        return attention_probs\n",
        "\n",
        "    def split_heads(self, inputs, batch_size):\n",
        "        inputs = tf.reshape(inputs, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "        attention_weights = self._masked_softmax(scaled_attention_logits, mask)\n",
        "        output = tf.matmul(attention_weights, v)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "    def call(self, query, key, value, decoder_mask, encoder_mask, use_causal_mask=False):\n",
        "\n",
        "        batch_size = tf.shape(query)[0]\n",
        "\n",
        "        mask = self._compute_attention_mask(decoder_mask, encoder_mask, encoder_mask, use_causal_mask=use_causal_mask)\n",
        "\n",
        "        query = self.query_dense(query)\n",
        "        key = self.key_dense(key)\n",
        "        value = self.value_dense(value)\n",
        "\n",
        "        query = self.split_heads(query, batch_size)\n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = self.scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.key_dim))\n",
        "        output = self.dense(concat_attention)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    def _compute_attention_mask(\n",
        "        self, query_mask, value_mask, key_mask=None, attention_mask=None, use_causal_mask=False\n",
        "    ):\n",
        "        \"\"\" Computes the attention mask, using the Keras masks of the inputs.\n",
        "            E: number of words in encoder\n",
        "            D: number of words in decoder\n",
        "         \"\"\"\n",
        "\n",
        "        auto_mask = None\n",
        "        if query_mask is not None:\n",
        "            # On input from decoder\n",
        "            query_mask = tf.cast(query_mask, tf.bool)  # defensive casting\n",
        "            # B = batch size, D = max query length\n",
        "            auto_mask = query_mask[:, :, tf.newaxis]  # shape is [B, D, 1]\n",
        "        if value_mask is not None:\n",
        "            # On input from encoder\n",
        "            value_mask = tf.cast(value_mask, tf.bool)  # defensive casting\n",
        "            # B = batch size, E == max value length\n",
        "            mask = value_mask[:, tf.newaxis, :]  # shape is [B, 1, E]\n",
        "            auto_mask = mask if auto_mask is None else auto_mask & mask\n",
        "        if key_mask is not None:\n",
        "            # On input from encoder\n",
        "            key_mask = tf.cast(key_mask, tf.bool)  # defensive casting\n",
        "            # B == batch size, E == max key length == max value length\n",
        "            mask = key_mask[:, tf.newaxis, :]  # shape is [B, 1, E]\n",
        "            auto_mask = mask if auto_mask is None else auto_mask & mask\n",
        "        if use_causal_mask:\n",
        "            # the shape of the causal mask is [1, D, E]\n",
        "            mask = self._compute_causal_mask(query_mask, value_mask)\n",
        "            auto_mask = mask if auto_mask is None else auto_mask & mask\n",
        "        if auto_mask is not None:\n",
        "            # merge attention_mask & automatic mask, to shape [B, D, E]\n",
        "            attention_mask = (\n",
        "                auto_mask\n",
        "                if attention_mask is None\n",
        "                else tf.cast(attention_mask, bool) & auto_mask\n",
        "            )\n",
        "        return attention_mask\n",
        "\n",
        "    def _compute_causal_mask(self, query_mask, value_mask=None):\n",
        "        \"\"\" Computes the causal attention mask. \"\"\"\n",
        "\n",
        "        q_seq_length = tf.shape(query_mask)[1]\n",
        "        v_seq_length = q_seq_length if value_mask is None else tf.shape(value_mask)[1]\n",
        "        return tf.linalg.band_part(  # creates a lower triangular matrix\n",
        "            tf.ones((1, q_seq_length, v_seq_length), tf.bool), -1, 0\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 406,
      "metadata": {
        "id": "TaHo3VmsobfV"
      },
      "outputs": [],
      "source": [
        "# All its child layers need num_heads and key_dim(or dmodel) for initialization\n",
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 407,
      "metadata": {
        "id": "4jHz6kVZobfV"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context, decoder_mask, encoder_mask):\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        key=context,\n",
        "        value=context,\n",
        "        decoder_mask=decoder_mask,\n",
        "        encoder_mask=encoder_mask)\n",
        "\n",
        "    # Cache the attention scores for plotting later.\n",
        "    self.last_attn_scores = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 408,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dSHtrYuobfV",
        "outputId": "64064cef-4980-4030-86ab-c16246df32ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 156, 576)\n",
            "(32, 206, 576)\n",
            "(32, 206, 576)\n"
          ]
        }
      ],
      "source": [
        "sample_ca = CrossAttention(num_heads=2, key_dim=d_model)\n",
        "encoder_mask = getattr(fr_emb, \"_keras_mask\", None)\n",
        "decoder_mask = getattr(en_emb, \"_keras_mask\", None)\n",
        "print(fr_emb.shape)\n",
        "print(en_emb.shape)\n",
        "print(sample_ca(en_emb, fr_emb, decoder_mask, encoder_mask).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 409,
      "metadata": {
        "id": "-XIE4HJbobfW"
      },
      "outputs": [],
      "source": [
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x, encoder_mask):\n",
        "\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        decoder_mask=encoder_mask,\n",
        "        encoder_mask=encoder_mask)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 410,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX6NEPJJobfW",
        "outputId": "2275b238-1dc6-4c9b-e9a1-1719a87cd12f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 156, 576)\n",
            "(32, 156, 576)\n"
          ]
        }
      ],
      "source": [
        "sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=576)\n",
        "\n",
        "print(fr_emb.shape)\n",
        "encoder_mask = getattr(fr_emb, \"_keras_mask\", None)\n",
        "print(sample_gsa(fr_emb, encoder_mask).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 411,
      "metadata": {
        "id": "VKBgvHnhobfW"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x, decoder_mask):\n",
        "\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        decoder_mask=decoder_mask,\n",
        "        encoder_mask=decoder_mask,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 412,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPLw331oobfW",
        "outputId": "fec60126-e240-4a65-cee9-c614997eb9f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 206, 576)\n",
            "(32, 206, 576)\n"
          ]
        }
      ],
      "source": [
        "sample_csa = CausalSelfAttention(num_heads=2, key_dim=576)\n",
        "print(en_emb.shape)\n",
        "decoder_mask = getattr(en_emb, \"_keras_mask\", None)\n",
        "print(sample_csa(en_emb, decoder_mask).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 413,
      "metadata": {
        "id": "a1fcoWxdobfW"
      },
      "outputs": [],
      "source": [
        "# out1 = sample_csa(embed_en(en[:, :3]))\n",
        "# out2 = sample_csa(embed_en(en))[:, :3]\n",
        "\n",
        "# tf.reduce_max(abs(out1 - out2)).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 414,
      "metadata": {
        "id": "sTZVFb2RobfW"
      },
      "outputs": [],
      "source": [
        "# This layer needs dmodel and dff for initialization\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 415,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuYuCUktobfW",
        "outputId": "4a31e1f2-ca45-4e93-b6e8-e40e47bacfae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 206, 576)\n",
            "(32, 206, 576)\n"
          ]
        }
      ],
      "source": [
        "sample_ffn = FeedForward(576, 2048)\n",
        "\n",
        "print(en_emb.shape)\n",
        "print(sample_ffn(en_emb).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 416,
      "metadata": {
        "id": "Q7lWKLObobfX"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, batch_size, d_model, num_heads, dff, sqrt_d, block_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.feebler = Feebler(batch_size=batch_size, sqrt_d=sqrt_d, block_size=block_size)\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=sqrt_d,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(sqrt_d, dff)\n",
        "\n",
        "    self.booster = Booster(batch_size=batch_size, sqrt_d=sqrt_d, block_size=block_size, n_embd=d_model)\n",
        "\n",
        "  def call(self, x, encoder_mask):\n",
        "    '''\n",
        "    x is of shape: (b, words, d)\n",
        "    output is of shape: (b, words, d)\n",
        "    '''\n",
        "    x = self.feebler(x)  # output of this line: (b, words, sqrt(d))\n",
        "    x = self.self_attention(x, encoder_mask=encoder_mask)\n",
        "    x = self.ffn(x)\n",
        "    x = self.booster(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 417,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "K7nKhDXwobfX",
        "outputId": "b9fd08a9-6917-4cd8-fe7f-060b2e8e5bde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "156\n",
            "(32, 156, 576)\n",
            "(32, 156, 576)\n"
          ]
        }
      ],
      "source": [
        "sample_enc_layer = EncoderLayer(batch_size=batch_size, d_model=576, num_heads=4, dff=2048, sqrt_d=24, block_size=encoder_block_size, dropout_rate=0.1)\n",
        "print(encoder_block_size)\n",
        "print(fr_emb.shape)\n",
        "\n",
        "encoder_mask = getattr(fr_emb, \"_keras_mask\", None)\n",
        "print(sample_enc_layer(fr_emb, encoder_mask).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 418,
      "metadata": {
        "id": "l5kpTCPcobfX"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, batch_size, num_layers, d_model, num_heads,\n",
        "               dff, block_size, vocab_size, sqrt_d, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # print('inside encoder')\n",
        "    # print(vocab_size, block_size, d_model)\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, block_size=block_size, d_model=d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(batch_size=batch_size,\n",
        "                     d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     sqrt_d=sqrt_d,\n",
        "                     block_size=block_size,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "    # Since we are passing x through feebler, hence\n",
        "    # we need to extract the masks before, and pass those\n",
        "    # explicity to the mha layer.\n",
        "    encoder_mask = getattr(x, \"_keras_mask\", None)\n",
        "\n",
        "    # Add dropout.\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    # print('i am inside encoder')\n",
        "    # print(x.shape)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, encoder_mask)\n",
        "\n",
        "    return x  # Shape `(batch_size, seq_len, d_model)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 419,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "156"
            ]
          },
          "execution_count": 419,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoder_block_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 420,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "F0Be4h7pobfX",
        "outputId": "b2981156-51f6-46eb-8d9b-448b1fb586d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 156)\n",
            "(156, 576)\n",
            "(32, 156, 576)\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the encoder.\n",
        "sample_encoder = Encoder(batch_size=batch_size,\n",
        "                         num_layers=4,\n",
        "                         d_model=576,\n",
        "                         num_heads=8,\n",
        "                         dff=2048,\n",
        "                         block_size=encoder_block_size,\n",
        "                         vocab_size=encoder_vocab_size,\n",
        "                         sqrt_d=sqrt_d)\n",
        "\n",
        "# Print the shape.\n",
        "print(fr.shape)\n",
        "sample_encoder_output = sample_encoder(fr, training=False)\n",
        "\n",
        "print(sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 421,
      "metadata": {
        "id": "jFx5XtwzobfX"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               batch_size,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               sqrt_d,\n",
        "               block_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.feebler = Feebler(batch_size=batch_size, sqrt_d=sqrt_d, block_size=block_size)\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "    self.booster = Booster(batch_size=batch_size, sqrt_d=sqrt_d, block_size=block_size, n_embd=d_model)\n",
        "\n",
        "  def call(self, x, context, decoder_mask, encoder_mask):\n",
        "    x = self.causal_self_attention(x=x, decoder_mask=decoder_mask)\n",
        "    x = self.cross_attention(x=x, context=context, decoder_mask=decoder_mask, encoder_mask=encoder_mask)\n",
        "\n",
        "    # Cache the last attention scores for plotting later\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 422,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbVMwWY8obfX",
        "outputId": "06c23a43-5ac8-4e21-a9de-54d535793066"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 206, 576)\n"
          ]
        }
      ],
      "source": [
        "sample_decoder_layer = DecoderLayer(batch_size=batch_size, d_model=576, num_heads=8, dff=2048, sqrt_d=sqrt_d, block_size=decoder_block_size, dropout_rate=0.1)\n",
        "\n",
        "decoder_mask = getattr(en_emb, \"_keras_mask\", None)\n",
        "encoder_mask = getattr(fr_emb, \"_keras_mask\", None)\n",
        "\n",
        "sample_decoder_layer_output = sample_decoder_layer(\n",
        "    x=en_emb, context=fr_emb, decoder_mask=decoder_mask, encoder_mask=encoder_mask\n",
        ")\n",
        "\n",
        "print(sample_decoder_layer_output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 423,
      "metadata": {
        "id": "8CPBX0G7obfY"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, batch_size, num_layers, d_model, num_heads, dff, block_size, vocab_size, sqrt_d, dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             block_size=block_size,\n",
        "                                             d_model=d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "      DecoderLayer(batch_size=batch_size,\n",
        "                   d_model=d_model,\n",
        "                   num_heads=num_heads,\n",
        "                   dff=dff,\n",
        "                   sqrt_d=sqrt_d,\n",
        "                   block_size=block_size,\n",
        "                   dropout_rate=dropout_rate)\n",
        "      for _ in range(num_layers)]\n",
        "\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    decoder_mask = getattr(x, \"_keras_mask\", None)\n",
        "    encoder_mask = getattr(context, \"_keras_mask\", None)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context, decoder_mask=decoder_mask, encoder_mask=encoder_mask)\n",
        "\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 424,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HL3BdVm1obfY",
        "outputId": "43f850ba-910a-4972-f387-afeda6cb037b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(206, 576)\n",
            "(32, 206)\n",
            "(32, 156, 576)\n",
            "(32, 206, 576)\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the decoder.\n",
        "sample_decoder = Decoder(batch_size=batch_size,\n",
        "                         num_layers=4,\n",
        "                         d_model=576,\n",
        "                         num_heads=8,\n",
        "                         dff=2048,\n",
        "                         block_size=decoder_block_size,\n",
        "                         vocab_size=decoder_vocab_size,\n",
        "                         sqrt_d=sqrt_d)\n",
        "\n",
        "output = sample_decoder(\n",
        "    x=en,\n",
        "    context=fr_emb)\n",
        "\n",
        "# Print the shapes.\n",
        "print(en.shape)\n",
        "print(fr_emb.shape)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 425,
      "metadata": {
        "id": "l2w_OR8CobfY"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, batch_size, num_layers, d_model, sqrt_d, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, input_block_size, target_block_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(batch_size=batch_size, num_layers=num_layers,\n",
        "                            d_model=d_model,\n",
        "                            num_heads=num_heads,\n",
        "                            dff=dff,\n",
        "                            sqrt_d=sqrt_d,\n",
        "                            block_size=input_vocab_size,\n",
        "                            vocab_size=input_vocab_size,\n",
        "                            dropout_rate=dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(batch_size=batch_size, num_layers=num_layers,\n",
        "                           d_model=d_model,\n",
        "                           num_heads=num_heads,\n",
        "                           dff=dff,\n",
        "                           sqrt_d=sqrt_d,\n",
        "                           block_size=target_vocab_size,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
        "    # first argument.\n",
        "    context, x  = inputs\n",
        "    # print(\"i am inside the transformer\")\n",
        "    # print(context.shape)\n",
        "    # print(x.shape)\n",
        "\n",
        "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
        "\n",
        "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
        "\n",
        "    # Final linear layer output.\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    try:\n",
        "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
        "      # b/250038731\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 426,
      "metadata": {
        "id": "uspQkoCqobfY",
        "outputId": "44247a08-c70c-41cf-b7ea-eded7dc4f263"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    batch_size=batch_size,\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    sqrt_d=sqrt_d,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=encoder_block_size,\n",
        "    target_vocab_size=decoder_block_size,\n",
        "    input_block_size=encoder_block_size,\n",
        "    target_block_size=decoder_block_size,\n",
        "    dropout_rate=dropout_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 427,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8KaLfdDobfY",
        "outputId": "94ade62a-cbfd-45af-9a39-b4241d682f06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(156, 576)\n",
            "(206, 576)\n",
            "(32, 206)\n",
            "(32, 156)\n",
            "(32, 206, 206)\n"
          ]
        }
      ],
      "source": [
        "output = transformer((fr, en))\n",
        "\n",
        "print(en.shape)\n",
        "print(fr.shape)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 428,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFLl3VOPobfY",
        "outputId": "ed22c51f-5941-42e2-ccf1-b6250264c1b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 4, 206, 156)\n"
          ]
        }
      ],
      "source": [
        "attn_scores = transformer.decoder.dec_layers[-1].last_attn_scores\n",
        "print(attn_scores.shape)  # (batch, heads, target_seq, input_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 429,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h7HrdiZobfZ",
        "outputId": "475d09d4-e4b8-4ac8-dbe8-e1e1200cc467"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"transformer_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder_23 (Encoder)        multiple                  919136    \n",
            "                                                                 \n",
            " decoder_18 (Decoder)        multiple                  14080640  \n",
            "                                                                 \n",
            " dense_1534 (Dense)          multiple                  118862    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15118638 (57.67 MB)\n",
            "Trainable params: 15118638 (57.67 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "transformer.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W91fHiB0obfZ"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 387,
      "metadata": {
        "id": "ZqwT9qhGobfZ"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 388,
      "metadata": {
        "id": "y3Y2P2QIobfZ"
      },
      "outputs": [],
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 389,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "JJdJDQriobfZ",
        "outputId": "caa74ee5-bfcc-4398-9f11-9233fb1552f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "execution_count": 389,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.xlabel('Train Step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 390,
      "metadata": {
        "id": "6SxrtMh-obfZ"
      },
      "outputs": [],
      "source": [
        "def masked_loss(label, pred):\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none'\n",
        "  )\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  mask = label!=0\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "\n",
        "  mask = label!=0\n",
        "  match = label==pred\n",
        "  match  = match & mask\n",
        "\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 391,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MfNMho-obfZ",
        "outputId": "8eddce4f-ef93-40c8-b4f3-4ab6495642c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 392,
      "metadata": {
        "id": "BUVa51U9obfa"
      },
      "outputs": [],
      "source": [
        "transformer.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[masked_accuracy])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 393,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V4ni6C-obfa",
        "outputId": "694733e3-9341-430b-c544-e0823bbea923"
      },
      "outputs": [],
      "source": [
        "# transformer.fit(train_dataset,\n",
        "#                 epochs=100,\n",
        "#                 validation_data=test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 430,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/100:   0%|          | 0/313 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(156, 576)\n",
            "(206, 576)\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(156, 576)\n",
            "(206, 576)\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "Epoch 1/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 312/313 [00:41<00:00,  9.20it/s, loss=nan, accuracy=2.08e-5]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(156, 576)\n",
            "(206, 576)\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "Epoch 1/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:45<00:00,  6.82it/s, loss=nan, accuracy=2.07e-5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(156, 576)\n",
            "(206, 576)\n",
            "(156, 576)\n",
            "(206, 576)\n",
            "Epoch 1/100, Loss: nan, Accuracy: 1.593386878084857e-05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [00:31<00:00,  9.83it/s, loss=nan, accuracy=0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/100, Loss: nan, Accuracy: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/100:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 283/313 [00:28<00:03,  9.97it/s, loss=nan, accuracy=0]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 72\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y151sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m progress_bar \u001b[39m=\u001b[39m tqdm(train_dataset, desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y151sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mfor\u001b[39;00m inputs, targets \u001b[39min\u001b[39;00m progress_bar:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y151sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     loss, accuracy \u001b[39m=\u001b[39m train_step(inputs, targets)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y151sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     epoch_loss_avg(loss)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y151sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     epoch_accuracy_avg(accuracy)\n",
            "File \u001b[0;32m~/anaconda3/envs/trans/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/anaconda3/envs/trans/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m~/anaconda3/envs/trans/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[39mreturn\u001b[39;00m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    869\u001b[0m       args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_config\n\u001b[1;32m    870\u001b[0m   )\n\u001b[1;32m    871\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[0;32m~/anaconda3/envs/trans/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:132\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    130\u001b[0m args \u001b[39m=\u001b[39m args \u001b[39mif\u001b[39;00m args \u001b[39melse\u001b[39;00m ()\n\u001b[1;32m    131\u001b[0m kwargs \u001b[39m=\u001b[39m kwargs \u001b[39mif\u001b[39;00m kwargs \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m--> 132\u001b[0m function \u001b[39m=\u001b[39m trace_function(\n\u001b[1;32m    133\u001b[0m     args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs, tracing_options\u001b[39m=\u001b[39;49mtracing_options\n\u001b[1;32m    134\u001b[0m )\n\u001b[1;32m    136\u001b[0m \u001b[39m# Bind it ourselves to skip unnecessary canonicalization of default call.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/envs/trans/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    175\u001b[0m     args \u001b[39m=\u001b[39m tracing_options\u001b[39m.\u001b[39minput_signature\n\u001b[1;32m    176\u001b[0m     kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 178\u001b[0m   concrete_function \u001b[39m=\u001b[39m _maybe_define_function(\n\u001b[1;32m    179\u001b[0m       args, kwargs, tracing_options\n\u001b[1;32m    180\u001b[0m   )\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tracing_options\u001b[39m.\u001b[39mbind_graph_to_function:\n\u001b[1;32m    183\u001b[0m   concrete_function\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/trans/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:239\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    229\u001b[0m lookup_func_type, lookup_func_context \u001b[39m=\u001b[39m (\n\u001b[1;32m    230\u001b[0m     function_type_utils\u001b[39m.\u001b[39mmake_canonicalized_monomorphic_type(\n\u001b[1;32m    231\u001b[0m         args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m     )\n\u001b[1;32m    236\u001b[0m )\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m tracing_options\u001b[39m.\u001b[39mfunction_cache \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m   concrete_function \u001b[39m=\u001b[39m tracing_options\u001b[39m.\u001b[39;49mfunction_cache\u001b[39m.\u001b[39;49mlookup(\n\u001b[1;32m    240\u001b[0m       lookup_func_type, current_func_context\n\u001b[1;32m    241\u001b[0m   )\n\u001b[1;32m    242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m   concrete_function \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/trans/lib/python3.11/site-packages/tensorflow/core/function/polymorphism/function_cache.py:50\u001b[0m, in \u001b[0;36mFunctionCache.lookup\u001b[0;34m(self, function_type, context)\u001b[0m\n\u001b[1;32m     48\u001b[0m   dispatch_type \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dispatch_dict[context]\u001b[39m.\u001b[39mdispatch(function_type)\n\u001b[1;32m     49\u001b[0m   \u001b[39mif\u001b[39;00m dispatch_type:\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_primary[(context, dispatch_type)]\n\u001b[1;32m     52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/trans/lib/python3.11/site-packages/tensorflow/core/function/polymorphism/function_type.py:456\u001b[0m, in \u001b[0;36mFunctionType.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__hash__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m--> 456\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mhash\u001b[39m((\u001b[39mtuple\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mitems()), \u001b[39mtuple\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcaptures\u001b[39m.\u001b[39mitems())))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
        "\n",
        "# Define your custom training loop\n",
        "@tf.function\n",
        "def train_step(inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = transformer(inputs, training=True)\n",
        "        loss = masked_loss(targets, predictions)\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "    accuracy = masked_accuracy(targets, predictions)\n",
        "    return loss, accuracy\n",
        "\n",
        "# Define your validation function\n",
        "@tf.function\n",
        "def validation_step(inputs, targets):\n",
        "    predictions = transformer(inputs, training=False)\n",
        "    loss = masked_loss(targets, predictions)\n",
        "    accuracy = masked_accuracy(targets, predictions)\n",
        "    return loss, accuracy\n",
        "\n",
        "# Define the training loop\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "    epoch_accuracy_avg = tf.keras.metrics.Mean()\n",
        "    \n",
        "    # Training loop with tqdm\n",
        "    progress_bar = tqdm(train_dataset, desc=f'Epoch {epoch+1}/{epochs}')\n",
        "    for inputs, targets in progress_bar:\n",
        "        loss, accuracy = train_step(inputs, targets)\n",
        "        epoch_loss_avg(loss)\n",
        "        epoch_accuracy_avg(accuracy)\n",
        "        progress_bar.set_postfix({'loss': epoch_loss_avg.result().numpy(), 'accuracy': epoch_accuracy_avg.result().numpy()})\n",
        "    \n",
        "    # Validation loop\n",
        "    for inputs, targets in test_dataset:\n",
        "        loss, accuracy = validation_step(inputs, targets)\n",
        "        epoch_loss_avg(loss)\n",
        "        epoch_accuracy_avg(accuracy)\n",
        "    \n",
        "    # Print the results for this epoch\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss_avg.result()}, Accuracy: {epoch_accuracy_avg.result()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPy298u7obfa"
      },
      "source": [
        "## Translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 431,
      "metadata": {
        "id": "DLPkWeLoobfa"
      },
      "outputs": [],
      "source": [
        "def print_translation(sentence, text, ground_truth):\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "  print(f'{\"Prediction\":15s}: {text}')\n",
        "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 432,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZF-PSWTobfa",
        "outputId": "4958d51d-e484-4247-b7c2-49dd4dd3020d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token for 'Start': tf.Tensor(3, shape=(), dtype=int64)\n",
            "Token for 'End': tf.Tensor(2, shape=(), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "# Get the token for 'Start'\n",
        "start_token = en_tokenizer.word_index['start']\n",
        "start_token = tf.constant(start_token, dtype=tf.int64)\n",
        "\n",
        "# Get the token for 'End'\n",
        "end_token = en_tokenizer.word_index['end']\n",
        "end_token = tf.constant(end_token, dtype=tf.int64)\n",
        "\n",
        "print(\"Token for 'Start':\", start_token)\n",
        "print(\"Token for 'End':\", end_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 433,
      "metadata": {
        "id": "NM96ALuEobfa"
      },
      "outputs": [],
      "source": [
        "class Translator(tf.Module):\n",
        "  def __init__(self, en_tokenizer, fr_tokenizer, transformer):\n",
        "    self.en_tokenizer = en_tokenizer\n",
        "    self.fr_tokenizer = fr_tokenizer\n",
        "    self.transformer = transformer\n",
        "\n",
        "  def __call__(self, sentence, max_length=206):\n",
        "\n",
        "    encoder_input = sentence  # (1, 11)\n",
        "    start = start_token  # shape=(1,)\n",
        "    end = end_token  # shape=(1,)\n",
        "\n",
        "    # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
        "    # dynamic-loop can be traced by `tf.function`.\n",
        "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "    output_array = output_array.write(0, start)\n",
        "\n",
        "    for i in tf.range(max_length):\n",
        "      output = tf.transpose(output_array.stack())  # (1, X)\n",
        "      output = tf.expand_dims(output, 0)\n",
        "      predictions = self.transformer([encoder_input, output], training=False)\n",
        "\n",
        "      # Select the last token from the `seq_len` dimension.\n",
        "      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
        "      predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "      # Concatenate the `predicted_id` to the output which is given to the\n",
        "      # decoder as its input.\n",
        "      output_array = output_array.write(i+1, predicted_id[0][0])  # Writing a tensor of shape 1\n",
        "\n",
        "      if predicted_id == end:\n",
        "        break\n",
        "\n",
        "    output = tf.transpose(output_array.stack())\n",
        "    # The output shape is `(1, tokens)`.\n",
        "    # text = tokenizers.en.detokenize(output)[0]  # Shape: `()`.\n",
        "\n",
        "    # tokens = tokenizers.en.lookup(output)[0]\n",
        "    text = en_tokenizer.sequences_to_texts([output.numpy()])[0]\n",
        "    # text = text.split(' ')[1:-1]\n",
        "    # text = ' '.join(text)\n",
        "\n",
        "    return text\n",
        "\n",
        "translator = Translator(en_tokenizer, fr_tokenizer, transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 434,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEn798B77cRY",
        "outputId": "e22830d5-ecf3-430b-c8ef-5933c4c14d77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(156, 576)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-30 20:05:50.844543: W tensorflow/core/framework/op_kernel.cc:1827] INVALID_ARGUMENT: required broadcastable shapes\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "Exception encountered when calling layer 'encoder_layer_96' (type EncoderLayer).\n\n{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:Mul] name: \n\nCall arguments received by layer 'encoder_layer_96' (type EncoderLayer):\n  â€¢ x=tf.Tensor(shape=(1, 2, 576), dtype=float32)\n  â€¢ encoder_mask=tf.Tensor(shape=(1, 2), dtype=bool)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 77\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m encoded_ground_truth \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mpreprocessing\u001b[39m.\u001b[39msequence\u001b[39m.\u001b[39mpad_sequences(en_tokenizer\u001b[39m.\u001b[39mtexts_to_sequences([ground_truth]), padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Print the encoded sequences\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# print(\"Encoded Sentence:\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# print(encoded_sentence)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# print(\"\\nEncoded Ground Truth:\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# print(encoded_ground_truth)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m translated_text \u001b[39m=\u001b[39m translator(encoded_sentence, max_length\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m print_translation(sentence, translated_text, ground_truth)\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 77\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtranspose(output_array\u001b[39m.\u001b[39mstack())  \u001b[39m# (1, X)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexpand_dims(output, \u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer([encoder_input, output], training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Select the last token from the `seq_len` dimension.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m predictions \u001b[39m=\u001b[39m predictions[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:, :]  \u001b[39m# Shape `(batch_size, 1, vocab_size)`.\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/trans/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 77\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m context, x  \u001b[39m=\u001b[39m inputs\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# print(\"i am inside the transformer\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# print(context.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(context)  \u001b[39m# (batch_size, context_len, d_model)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x, context)  \u001b[39m# (batch_size, target_len, d_model)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Final linear layer output.\u001b[39;00m\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 77\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# print('i am inside encoder')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menc_layers[i](x, encoder_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 77\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, x, encoder_mask):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m  \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m  x is of shape: (b, words, d)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m  output is of shape: (b, words, d)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m  '''\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeebler(x)  \u001b[39m# output of this line: (b, words, sqrt(d))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attention(x, encoder_mask\u001b[39m=\u001b[39mencoder_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffn(x)\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 77\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# print('transpose taken')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# print(data.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# print(batch_size, self.sqrt_d, self.sqrt_d, block_size) \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m data_reshaped \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreshape(data, (batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msqrt_d, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msqrt_d, block_size))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m product \u001b[39m=\u001b[39m data_reshaped \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweis\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# print(product.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y135sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m updated_product \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_sum(product, axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'encoder_layer_96' (type EncoderLayer).\n\n{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:Mul] name: \n\nCall arguments received by layer 'encoder_layer_96' (type EncoderLayer):\n  â€¢ x=tf.Tensor(shape=(1, 2, 576), dtype=float32)\n  â€¢ encoder_mask=tf.Tensor(shape=(1, 2), dtype=bool)"
          ]
        }
      ],
      "source": [
        "sentence = 'PREMIÃˆRE PARTIE'\n",
        "ground_truth = 'First Part'\n",
        "\n",
        "# sentence = 'Le grand Meaulnes'\n",
        "# ground_truth = 'The Wanderer'\n",
        "\n",
        "# sentence = 'Alain-Fournier'\n",
        "# ground_truth = 'Alain-Fournier'\n",
        "\n",
        "# Tokenize the sentences using the tokenizers\n",
        "encoded_sentence = tf.keras.preprocessing.sequence.pad_sequences(fr_tokenizer.texts_to_sequences([sentence]), padding='post')\n",
        "encoded_ground_truth = tf.keras.preprocessing.sequence.pad_sequences(en_tokenizer.texts_to_sequences([ground_truth]), padding='post')\n",
        "\n",
        "# Print the encoded sequences\n",
        "# print(\"Encoded Sentence:\")\n",
        "# print(encoded_sentence)\n",
        "# print(\"\\nEncoded Ground Truth:\")\n",
        "# print(encoded_ground_truth)\n",
        "\n",
        "translated_text = translator(encoded_sentence, max_length=2)\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m56DS55Yysvh"
      },
      "source": [
        "> Examples in French:\n",
        "\n",
        "    Le grand Meaulnes\n",
        "    Alain-Fournier\n",
        "    PREMIÃˆRE PARTIE\n",
        "\n",
        "> Examples in English:\n",
        "\n",
        "    The Wanderer\n",
        "    Alain-Fournier\n",
        "    First Part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLKNDTXeobfb"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAXW3hxG_not",
        "outputId": "9f93bd0a-5d8b-4191-b790-73f5bb511629"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU Score: 0.11428571428571427\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ],
      "source": [
        "references_list = [['are', 'you', 'fond', 'of', 'and', 'he', 'searched',\n",
        "   'my', 'face', 'with', 'eyes', 'that', 'i', 'saw', 'were',\n",
        "     'dark,', 'and', 'end']]\n",
        "\n",
        "hypothesis = ['start', 'they', 'were', 'waiting', 'for', 'a', 'disposition',\n",
        "               'of', 'the', 'house;', 'and', 'one', 'hand', 'passed', 'her',\n",
        "                 'windows', 'in', 'the', 'did', 'not', 'dwell', 'on', 'the',\n",
        "                 'should', 'quit', 'garden', 'gardiner,', 'whose', 'bright',\n",
        "                 'church', 'bright', '.', '.', \".'\", 'end']\n",
        "\n",
        "# # Reference and hypothesis sentences\n",
        "# references_list = [['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'for']]\n",
        "# hypothesis = ['the', 'fast', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'the', 'lazy', 'dog']\n",
        "\n",
        "# Compute BLEU score for a single sentence\n",
        "bleu_score = corpus_bleu([references_list], [hypothesis], weights=(1, 0, 0, 0))\n",
        "print(f'BLEU Score: {bleu_score}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 309,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGLLNwFSASpF",
        "outputId": "1b9ddc1b-710a-4546-91ee-632f91d596fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU Score: 9.418382295637229e-232\n"
          ]
        }
      ],
      "source": [
        "# Reference and hypothesis sentences for multiple examples\n",
        "# DOES NOT WORK\n",
        "references = [\n",
        "    ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'],\n",
        "    ['the', 'cat', 'in', 'the', 'hat']\n",
        "]\n",
        "\n",
        "hypotheses = [\n",
        "    ['the', 'fast', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'],\n",
        "    ['a', 'cat', 'in', 'a', 'hat']\n",
        "]\n",
        "\n",
        "# Compute BLEU score for multiple sentences\n",
        "bleu_score = corpus_bleu(references, hypotheses)\n",
        "print(f'BLEU Score: {bleu_score}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDjtJF0KVdyK"
      },
      "source": [
        "### Evaluation of bleu score on train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {
        "id": "t3ALcYLJVdyL",
        "outputId": "8c18b0e4-ec6e-4f8e-ee90-1ed8b31a8a77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating BLEU Score:   0%|          | 0/313 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "Exception encountered when calling layer 'encoder_layer_51' (type EncoderLayer).\n\n{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 39936 values, but the requested shape has 1277952 [Op:Reshape]\n\nCall arguments received by layer 'encoder_layer_51' (type EncoderLayer):\n  â€¢ x=tf.Tensor(shape=(1, 156, 256), dtype=float32)\n  â€¢ encoder_mask=tf.Tensor(shape=(1, 156), dtype=bool)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 81\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         pbar\u001b[39m.\u001b[39mset_postfix({\u001b[39m'\u001b[39m\u001b[39mBleu Score\u001b[39m\u001b[39m'\u001b[39m: overall_bleu_score\u001b[39m/\u001b[39mcount})\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m overall_bleu_score\u001b[39m/\u001b[39mcount\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m bleu_score \u001b[39m=\u001b[39m calculate_bleu_score(translator, train_dataset)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBLEU Score on train Dataset:\u001b[39m\u001b[39m\"\u001b[39m, bleu_score)\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 81\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m ground_truth \u001b[39m=\u001b[39m en_tokenizer\u001b[39m.\u001b[39msequences_to_texts(en_label_sentence\u001b[39m.\u001b[39mnumpy())[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# ground_truth = ground_truth.split(' ')[1:-1]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# ground_truth = ' '.join(ground_truth)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m translated_text \u001b[39m=\u001b[39m translator(fr_sentence, max_length)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Bleu score evaluation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m references \u001b[39m=\u001b[39m []\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 81\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtranspose(output_array\u001b[39m.\u001b[39mstack())  \u001b[39m# (1, X)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexpand_dims(output, \u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer([encoder_input, output], training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Select the last token from the `seq_len` dimension.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m predictions \u001b[39m=\u001b[39m predictions[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:, :]  \u001b[39m# Shape `(batch_size, 1, vocab_size)`.\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/trans/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 81\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m context, x  \u001b[39m=\u001b[39m inputs\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# print(\"i am inside the transformer\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# print(context.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(context)  \u001b[39m# (batch_size, context_len, d_model)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x, context)  \u001b[39m# (batch_size, target_len, d_model)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Final linear layer output.\u001b[39;00m\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 81\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menc_layers[i](x, encoder_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 81\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, x, encoder_mask):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m  \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m  x is of shape: (b, words, d)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m  output is of shape: (b, words, d)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m  '''\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeebler(x)  \u001b[39m# output of this line: (b, words, sqrt(d))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attention(x, encoder_mask\u001b[39m=\u001b[39mencoder_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffn(x)\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 81\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# incoming tensor is of shape: (b, words, d)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Feebler works with shape: (b, d, words)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtranspose(data, (\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     data_reshaped \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mreshape(data, (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msqrt_d, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msqrt_d, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock_size))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     product \u001b[39m=\u001b[39m data_reshaped \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     updated_product \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_sum(product, axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'encoder_layer_51' (type EncoderLayer).\n\n{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 39936 values, but the requested shape has 1277952 [Op:Reshape]\n\nCall arguments received by layer 'encoder_layer_51' (type EncoderLayer):\n  â€¢ x=tf.Tensor(shape=(1, 156, 256), dtype=float32)\n  â€¢ encoder_mask=tf.Tensor(shape=(1, 156), dtype=bool)"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "# Disable specific warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "def calculate_bleu_score(translator, train_dataset, max_length=206):\n",
        "    overall_bleu_score = 0.0\n",
        "    count = 0\n",
        "\n",
        "    # Use tqdm for progress bar\n",
        "    pbar = tqdm(train_dataset, desc=\"Calculating BLEU Score\")\n",
        "\n",
        "    for (fr, en), en_label in pbar:\n",
        "        for i in range(fr.shape[0]):\n",
        "            fr_sentence = fr[i:i+1, :]\n",
        "            en_label_sentence = en_label[i:i+1, :]\n",
        "            ground_truth = en_tokenizer.sequences_to_texts(en_label_sentence.numpy())[0]\n",
        "            # ground_truth = ground_truth.split(' ')[1:-1]\n",
        "            # ground_truth = ' '.join(ground_truth)\n",
        "\n",
        "            translated_text = translator(fr_sentence, max_length)\n",
        "\n",
        "            # Bleu score evaluation\n",
        "            references = []\n",
        "            references.append(ground_truth.split(' '))\n",
        "            translated_text = translated_text.split(' ')\n",
        "            overall_bleu_score += corpus_bleu([references], [translated_text], weights=(1, 0, 0, 0))\n",
        "            count += 1\n",
        "\n",
        "        # Display progress in tqdm\n",
        "        pbar.set_postfix({'Bleu Score': overall_bleu_score/count})\n",
        "\n",
        "    return overall_bleu_score/count\n",
        "\n",
        "bleu_score = calculate_bleu_score(translator, train_dataset)\n",
        "print(\"BLEU Score on train Dataset:\", bleu_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46SGk2OnVdyM"
      },
      "source": [
        "### Evaluation of bleu score on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 311,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P9BTZYABlfo",
        "outputId": "09bc20ad-ace6-42be-fe13-9ec02e134715"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating BLEU Score:   0%|          | 0/94 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "Exception encountered when calling layer 'encoder_layer_51' (type EncoderLayer).\n\n{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 34048 values, but the requested shape has 1277952 [Op:Reshape]\n\nCall arguments received by layer 'encoder_layer_51' (type EncoderLayer):\n  â€¢ x=tf.Tensor(shape=(1, 133, 256), dtype=float32)\n  â€¢ encoder_mask=tf.Tensor(shape=(1, 133), dtype=bool)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 83\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         pbar\u001b[39m.\u001b[39mset_postfix({\u001b[39m'\u001b[39m\u001b[39mBleu Score\u001b[39m\u001b[39m'\u001b[39m: overall_bleu_score\u001b[39m/\u001b[39mcount})\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m overall_bleu_score\u001b[39m/\u001b[39mcount\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m bleu_score \u001b[39m=\u001b[39m calculate_bleu_score(translator, test_dataset)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBLEU Score on Test Dataset:\u001b[39m\u001b[39m\"\u001b[39m, bleu_score)\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 83\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m ground_truth \u001b[39m=\u001b[39m en_tokenizer\u001b[39m.\u001b[39msequences_to_texts(en_label_sentence\u001b[39m.\u001b[39mnumpy())[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# ground_truth = ground_truth.split(' ')[1:-1]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# ground_truth = ' '.join(ground_truth)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m translated_text \u001b[39m=\u001b[39m translator(fr_sentence, max_length)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Bleu score evaluation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m references \u001b[39m=\u001b[39m []\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 83\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtranspose(output_array\u001b[39m.\u001b[39mstack())  \u001b[39m# (1, X)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexpand_dims(output, \u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer([encoder_input, output], training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Select the last token from the `seq_len` dimension.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m predictions \u001b[39m=\u001b[39m predictions[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:, :]  \u001b[39m# Shape `(batch_size, 1, vocab_size)`.\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/trans/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 83\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m context, x  \u001b[39m=\u001b[39m inputs\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# print(\"i am inside the transformer\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# print(context.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(context)  \u001b[39m# (batch_size, context_len, d_model)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x, context)  \u001b[39m# (batch_size, target_len, d_model)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Final linear layer output.\u001b[39;00m\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 83\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menc_layers[i](x, encoder_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 83\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, x, encoder_mask):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m  \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m  x is of shape: (b, words, d)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m  output is of shape: (b, words, d)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m  '''\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeebler(x)  \u001b[39m# output of this line: (b, words, sqrt(d))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attention(x, encoder_mask\u001b[39m=\u001b[39mencoder_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffn(x)\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 83\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# incoming tensor is of shape: (b, words, d)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Feebler works with shape: (b, d, words)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtranspose(data, (\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     data_reshaped \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mreshape(data, (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msqrt_d, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msqrt_d, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock_size))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     product \u001b[39m=\u001b[39m data_reshaped \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     updated_product \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_sum(product, axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'encoder_layer_51' (type EncoderLayer).\n\n{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 34048 values, but the requested shape has 1277952 [Op:Reshape]\n\nCall arguments received by layer 'encoder_layer_51' (type EncoderLayer):\n  â€¢ x=tf.Tensor(shape=(1, 133, 256), dtype=float32)\n  â€¢ encoder_mask=tf.Tensor(shape=(1, 133), dtype=bool)"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "# Disable specific warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "def calculate_bleu_score(translator, test_dataset, max_length=206):\n",
        "    overall_bleu_score = 0.0\n",
        "    count = 0\n",
        "\n",
        "    # Use tqdm for progress bar\n",
        "    pbar = tqdm(test_dataset, desc=\"Calculating BLEU Score\")\n",
        "\n",
        "    for (fr, en), en_label in pbar:\n",
        "        for i in range(fr.shape[0]):\n",
        "            fr_sentence = fr[i:i+1, :]\n",
        "            en_label_sentence = en_label[i:i+1, :]\n",
        "            ground_truth = en_tokenizer.sequences_to_texts(en_label_sentence.numpy())[0]\n",
        "            # ground_truth = ground_truth.split(' ')[1:-1]\n",
        "            # ground_truth = ' '.join(ground_truth)\n",
        "\n",
        "            translated_text = translator(fr_sentence, max_length)\n",
        "\n",
        "            # Bleu score evaluation\n",
        "            references = []\n",
        "            references.append(ground_truth.split(' '))\n",
        "            translated_text = translated_text.split(' ')\n",
        "            overall_bleu_score += corpus_bleu([references], [translated_text], weights=(1, 0, 0, 0))\n",
        "            count += 1\n",
        "\n",
        "        # Display progress in tqdm\n",
        "        pbar.set_postfix({'Bleu Score': overall_bleu_score/count})\n",
        "\n",
        "    return overall_bleu_score/count\n",
        "\n",
        "bleu_score = calculate_bleu_score(translator, test_dataset)\n",
        "print(\"BLEU Score on Test Dataset:\", bleu_score)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "048473bedaf54822a40891dec1051d46": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04ca613a16974e96b5e6f5a5ddcbc730": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08ef4bbd6b1a4b77b1e8fc14a1814de7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "0a923ebe2caf43509f2d98c056e3afd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f9925afbff445dd85e7211c2053526c",
            "max": 127085,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7501ce04f2c9484ead067312cfea1f86",
            "value": 127085
          }
        },
        "0f9925afbff445dd85e7211c2053526c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "101214c744ca48e9a891440cffb6be04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19078d674b40454db42eef4613312cc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85f4ed22ef0c4fe5bc238ba79b455282",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_694af1363f6b4a30b4c94ef7738994fd",
            "value": "Generating train split: 100%"
          }
        },
        "1d3693eb2d3a41c98e8dd529076fdfbc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f2a526f41e0418aa30f933cffd55df4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e9d5fad71d043a896128c05ad757b02",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90bc1de718a1447c9a013366d946771a",
            "value": 1
          }
        },
        "2386868cd2294786a0a7f7303f4915e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e7273229bb3478f9a0f27f78ed99dbe",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2892bcbaf5d4d62b0637250b824f386",
            "value": 1
          }
        },
        "26547b5811e54faebe60a26534223d99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b2c473652a54596a2a9a4248d30af39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a4cd7b655b5649ca9453dcfaf033df63",
              "IPY_MODEL_0a923ebe2caf43509f2d98c056e3afd3",
              "IPY_MODEL_531282d34fdd48f98578f55adc427d62"
            ],
            "layout": "IPY_MODEL_f87e449cbcad4129a3a7f5c3a426cc02"
          }
        },
        "2d87b691ad7a450b9073b91b35bec33a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aa478c2c0b545daa39e831ffc87f695",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d3f9cc3967314482bfbedb394f3b0f12",
            "value": "Generating train examples...: "
          }
        },
        "30bc82e80fb64e1c8f54137f03deb2ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37e41a95c2e44348a5efa428518b6087": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bbfdc5583184d2087aab8887d27d677": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c92b27dc60741da9be9ccc99f73d378": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44caa122c7b34b73ae89bda1987dc51d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_486a9cd8a6be4ec08eb6b45cc0f5b364",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4a219508a82a47a89270fceb1c5e3417",
            "value": "Generating splits...: 100%"
          }
        },
        "486a9cd8a6be4ec08eb6b45cc0f5b364": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a219508a82a47a89270fceb1c5e3417": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "531282d34fdd48f98578f55adc427d62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7a4e18f53664877b03ac26e13741162",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_04ca613a16974e96b5e6f5a5ddcbc730",
            "value": " 93865/127085 [00:00&lt;00:00, 496210.37 examples/s]"
          }
        },
        "5a9f54b9007c48aab2aa0df019afef85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_048473bedaf54822a40891dec1051d46",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d96b2eee1e7f4f1885c74dce5adc0e1e",
            "value": " 1/1 [00:21&lt;00:00, 21.20s/ splits]"
          }
        },
        "636c29e4113e41c5ac544452c8c881da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9396ddc885df46248ad3eb26ae26e114",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_37e41a95c2e44348a5efa428518b6087",
            "value": " 21.0M/21.0M [00:01&lt;00:00, 13.4MB/s]"
          }
        },
        "63941e4f655e46659f725befa5b3d6ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "694af1363f6b4a30b4c94ef7738994fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6acfd8bfffba491ab26bd0cd5d7b9b5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c52bede33b9b48f890611a21e3c7f815",
              "IPY_MODEL_75b5cd22834746daba6455c0bdd0eb3a",
              "IPY_MODEL_636c29e4113e41c5ac544452c8c881da"
            ],
            "layout": "IPY_MODEL_30bc82e80fb64e1c8f54137f03deb2ce"
          }
        },
        "6e7273229bb3478f9a0f27f78ed99dbe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7501ce04f2c9484ead067312cfea1f86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "75b5cd22834746daba6455c0bdd0eb3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb7ce590adce48a9966b0bab35dd99ff",
            "max": 20985324,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_839408a55582449fb57e26d1c63c9578",
            "value": 20985324
          }
        },
        "783f52bdd0cc423198b3e7b7c4262bfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d3693eb2d3a41c98e8dd529076fdfbc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ee551a94920b4f9693a850e68ed7572d",
            "value": " 125311/? [00:20&lt;00:00, 6163.07 examples/s]"
          }
        },
        "839408a55582449fb57e26d1c63c9578": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "844e668e1206401f9fe3bf8e95694288": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26547b5811e54faebe60a26534223d99",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c16d473c9c0c462faa4b9bce88aa7689",
            "value": " 127085/127085 [00:00&lt;00:00, 533723.23 examples/s]"
          }
        },
        "85f4ed22ef0c4fe5bc238ba79b455282": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87667ad986a643c8a64226656a90a93c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d523043fe3f7494eb7324f01a5b5ce00",
            "max": 127085,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3c92b27dc60741da9be9ccc99f73d378",
            "value": 127085
          }
        },
        "8a90f328279449ee845230728c90c1c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aa478c2c0b545daa39e831ffc87f695": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e9d5fad71d043a896128c05ad757b02": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "90bc1de718a1447c9a013366d946771a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9396ddc885df46248ad3eb26ae26e114": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4cd7b655b5649ca9453dcfaf033df63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bbfdc5583184d2087aab8887d27d677",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_eacc07572c86432d9acc5f0c5612776a",
            "value": "Shuffling /root/tensorflow_datasets/opus_books/en-fr/1.0.0.incompletePXSUGA/opus_books-train.tfrecord*...:  74%"
          }
        },
        "ac58fbaf420143828f2ccdf7824aca44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_19078d674b40454db42eef4613312cc8",
              "IPY_MODEL_87667ad986a643c8a64226656a90a93c",
              "IPY_MODEL_844e668e1206401f9fe3bf8e95694288"
            ],
            "layout": "IPY_MODEL_d0b8215ea74a4bf98bdbcab831d97344"
          }
        },
        "afd0dd2fc56046af8f23a554ac7c6ed7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d87b691ad7a450b9073b91b35bec33a",
              "IPY_MODEL_1f2a526f41e0418aa30f933cffd55df4",
              "IPY_MODEL_783f52bdd0cc423198b3e7b7c4262bfe"
            ],
            "layout": "IPY_MODEL_08ef4bbd6b1a4b77b1e8fc14a1814de7"
          }
        },
        "b545a45778be4e419d7be3eff31c11a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44caa122c7b34b73ae89bda1987dc51d",
              "IPY_MODEL_2386868cd2294786a0a7f7303f4915e0",
              "IPY_MODEL_5a9f54b9007c48aab2aa0df019afef85"
            ],
            "layout": "IPY_MODEL_63941e4f655e46659f725befa5b3d6ce"
          }
        },
        "b7a4e18f53664877b03ac26e13741162": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb7ce590adce48a9966b0bab35dd99ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c16d473c9c0c462faa4b9bce88aa7689": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2892bcbaf5d4d62b0637250b824f386": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c52bede33b9b48f890611a21e3c7f815": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a90f328279449ee845230728c90c1c6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_101214c744ca48e9a891440cffb6be04",
            "value": "Downloading data: 100%"
          }
        },
        "d0b8215ea74a4bf98bdbcab831d97344": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3f9cc3967314482bfbedb394f3b0f12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d523043fe3f7494eb7324f01a5b5ce00": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d96b2eee1e7f4f1885c74dce5adc0e1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eacc07572c86432d9acc5f0c5612776a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee551a94920b4f9693a850e68ed7572d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f87e449cbcad4129a3a7f5c3a426cc02": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
