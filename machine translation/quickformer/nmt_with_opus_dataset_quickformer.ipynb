{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA4A3Az6obfO"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2e76qcRCobfP"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# import os\n",
        "# os.environ[\"CUDA_DIR\"] = \"/home/tuk/anaconda3/pkgs/cuda-nvcc-11.6.124-hbba6d2d_0/nvvm\"\n",
        "# !echo ${CUDA_DIR}\\\n",
        "\n",
        "# print(tf.sysconfig.get_build_info() )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrZrEYICobfQ",
        "outputId": "f0ecd18c-8e71-4533-e683-80d779a37021"
      },
      "outputs": [],
      "source": [
        "# Install the most re version of TensorFlow to use the improved\n",
        "# masking support for `tf.keras.layers.MultiHeadAttention`.\n",
        "# !apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
        "# !pip install protobuf~=3.20.3\n",
        "# !pip install -q tensorflow_datasets\n",
        "# !pip install -q -U tensorflow-text tensorflow\n",
        "\n",
        "# !pip install transformers\n",
        "# !pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2Z8D_-yqobfQ"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_text\n",
        "\n",
        "import transformers\n",
        "import datasets\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKgXqJvMobfQ"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275,
          "referenced_widgets": [
            "6acfd8bfffba491ab26bd0cd5d7b9b5a",
            "c52bede33b9b48f890611a21e3c7f815",
            "75b5cd22834746daba6455c0bdd0eb3a",
            "636c29e4113e41c5ac544452c8c881da",
            "30bc82e80fb64e1c8f54137f03deb2ce",
            "8a90f328279449ee845230728c90c1c6",
            "101214c744ca48e9a891440cffb6be04",
            "bb7ce590adce48a9966b0bab35dd99ff",
            "839408a55582449fb57e26d1c63c9578",
            "9396ddc885df46248ad3eb26ae26e114",
            "37e41a95c2e44348a5efa428518b6087",
            "ac58fbaf420143828f2ccdf7824aca44",
            "19078d674b40454db42eef4613312cc8",
            "87667ad986a643c8a64226656a90a93c",
            "844e668e1206401f9fe3bf8e95694288",
            "d0b8215ea74a4bf98bdbcab831d97344",
            "85f4ed22ef0c4fe5bc238ba79b455282",
            "694af1363f6b4a30b4c94ef7738994fd",
            "d523043fe3f7494eb7324f01a5b5ce00",
            "3c92b27dc60741da9be9ccc99f73d378",
            "26547b5811e54faebe60a26534223d99",
            "c16d473c9c0c462faa4b9bce88aa7689",
            "b545a45778be4e419d7be3eff31c11a7",
            "44caa122c7b34b73ae89bda1987dc51d",
            "2386868cd2294786a0a7f7303f4915e0",
            "5a9f54b9007c48aab2aa0df019afef85",
            "63941e4f655e46659f725befa5b3d6ce",
            "486a9cd8a6be4ec08eb6b45cc0f5b364",
            "4a219508a82a47a89270fceb1c5e3417",
            "6e7273229bb3478f9a0f27f78ed99dbe",
            "c2892bcbaf5d4d62b0637250b824f386",
            "048473bedaf54822a40891dec1051d46",
            "d96b2eee1e7f4f1885c74dce5adc0e1e",
            "afd0dd2fc56046af8f23a554ac7c6ed7",
            "2d87b691ad7a450b9073b91b35bec33a",
            "1f2a526f41e0418aa30f933cffd55df4",
            "783f52bdd0cc423198b3e7b7c4262bfe",
            "08ef4bbd6b1a4b77b1e8fc14a1814de7",
            "8aa478c2c0b545daa39e831ffc87f695",
            "d3f9cc3967314482bfbedb394f3b0f12",
            "8e9d5fad71d043a896128c05ad757b02",
            "90bc1de718a1447c9a013366d946771a",
            "1d3693eb2d3a41c98e8dd529076fdfbc",
            "ee551a94920b4f9693a850e68ed7572d",
            "2b2c473652a54596a2a9a4248d30af39",
            "a4cd7b655b5649ca9453dcfaf033df63",
            "0a923ebe2caf43509f2d98c056e3afd3",
            "531282d34fdd48f98578f55adc427d62",
            "f87e449cbcad4129a3a7f5c3a426cc02",
            "3bbfdc5583184d2087aab8887d27d677",
            "eacc07572c86432d9acc5f0c5612776a",
            "0f9925afbff445dd85e7211c2053526c",
            "7501ce04f2c9484ead067312cfea1f86",
            "b7a4e18f53664877b03ac26e13741162",
            "04ca613a16974e96b5e6f5a5ddcbc730"
          ]
        },
        "id": "-JghimPqobfR",
        "outputId": "3046c014-e0ed-4b09-a73f-e09c968d0fea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/tensorflow_datasets/core/dataset_builders/huggingface_dataset_builder.py:160: FutureWarning: list_datasets is deprecated and will be removed in the next major version of datasets. Use 'huggingface_hub.list_datasets' instead.\n",
            "  hf_names = hf_datasets.list_datasets()\n",
            "2024-01-25 13:40:21.667384: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-25 13:40:21.977903: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-25 13:40:21.978051: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-25 13:40:21.979784: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-25 13:40:21.979931: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-25 13:40:21.980023: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-25 13:40:22.043956: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-25 13:40:22.044099: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-25 13:40:22.044192: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-25 13:40:22.044254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22078 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
          ]
        }
      ],
      "source": [
        "examples, metadata = tfds.load('huggingface:opus_books/en-fr', with_info=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1dXeN0gobfR",
        "outputId": "62fd2701-826e-45ff-baf2-36b7d1c5f232"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<_PrefetchDataset element_spec={'id': TensorSpec(shape=(), dtype=tf.string, name=None), 'translation': {'en': TensorSpec(shape=(), dtype=tf.string, name=None), 'fr': TensorSpec(shape=(), dtype=tf.string, name=None)}}>\n"
          ]
        }
      ],
      "source": [
        "examples = examples['train']\n",
        "print(examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MbR10B3obfR"
      },
      "source": [
        "## Make train test splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPVdi641obfR",
        "outputId": "55c88358-5ece-48c0-b5e6-da48f1dd341e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "101668 127085\n"
          ]
        }
      ],
      "source": [
        "# Get the total number of examples\n",
        "total_examples = metadata.splits['train'].num_examples\n",
        "\n",
        "# Define the split ratios (e.g., 80% train, 20% test)\n",
        "train_ratio = 0.8\n",
        "test_ratio = 1.0 - train_ratio\n",
        "\n",
        "# Calculate the number of examples for each split\n",
        "train_size = int(train_ratio * total_examples)\n",
        "print(train_size, total_examples)\n",
        "\n",
        "# # Split the dataset into train and test dictionaries\n",
        "# train_examples = list(examples)[:train_size]\n",
        "# test_examples = list(examples)[train_size:]\n",
        "\n",
        "# HARD CODING\n",
        "# Split the dataset into train and test dictionaries\n",
        "train_examples = list(examples)[:10000]\n",
        "test_examples = list(examples)[10000:13000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrLqirLYobfR",
        "outputId": "eadfd620-3985-4ff6-9801-a27f27a4cc6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000\n",
            "3000\n"
          ]
        }
      ],
      "source": [
        "# Print lengths\n",
        "print(len(train_examples))\n",
        "print(len(test_examples))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR-eifW2obfR"
      },
      "source": [
        "## Process dataset for training the tensorflow model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PHNNzjGDobfR"
      },
      "outputs": [],
      "source": [
        "formatted_train_examples = []  # list of tuples\n",
        "formatted_test_examples = []  # list of tuples\n",
        "for record in train_examples:\n",
        "    formatted_train_examples.append(\n",
        "        (record['translation']['fr'].numpy(), record['translation']['en'].numpy()))\n",
        "\n",
        "for record in test_examples:\n",
        "    formatted_test_examples.append(\n",
        "        (record['translation']['fr'].numpy(), record['translation']['en'].numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "t24DLwssobfS"
      },
      "outputs": [],
      "source": [
        "# Separate the pairs into two lists\n",
        "fr_train_list, en_train_list = zip(*formatted_train_examples)\n",
        "fr_test_list, en_test_list = zip(*formatted_test_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ihbooxB4obfS"
      },
      "outputs": [],
      "source": [
        "# Create TensorFlow tensors from the values\n",
        "fr_train_tensor = tf.stack(fr_train_list)\n",
        "en_train_tensor = tf.stack(en_train_list)\n",
        "fr_test_tensor = tf.stack(fr_test_list)\n",
        "en_test_tensor = tf.stack(en_test_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XuVOx0SGobfS"
      },
      "outputs": [],
      "source": [
        "# Create a dataset from the tensors\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((fr_train_tensor, en_train_tensor))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((fr_test_tensor, en_test_tensor))\n",
        "\n",
        "# Prefetch the datasets for better performance\n",
        "train_examples = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test_examples = test_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7__ikf2kobfS",
        "outputId": "66294584-f28e-47bd-ce12-173d5b947464"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Examples in French:\n",
            "Le grand Meaulnes\n",
            "Alain-Fournier\n",
            "PREMIÈRE PARTIE\n",
            "\n",
            "> Examples in English:\n",
            "The Wanderer\n",
            "Alain-Fournier\n",
            "First Part\n"
          ]
        }
      ],
      "source": [
        "for fr_examples, en_examples in train_examples.batch(3).take(1):\n",
        "  print('> Examples in French:')\n",
        "  for fr in fr_examples.numpy():\n",
        "    print(fr.decode('utf-8'))\n",
        "  print()\n",
        "\n",
        "  print('> Examples in English:')\n",
        "  for en in en_examples.numpy():\n",
        "    print(en.decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzeOXVH3obfS"
      },
      "source": [
        "## Tokenize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LPw4NJmQobfS"
      },
      "outputs": [],
      "source": [
        "# Convert tensors to numpy arrays\n",
        "fr_train_array = fr_train_tensor.numpy()\n",
        "en_train_array = en_train_tensor.numpy()\n",
        "fr_test_array = fr_test_tensor.numpy()\n",
        "en_test_array = en_test_tensor.numpy()\n",
        "\n",
        "# Convert numpy arrays to lists of strings\n",
        "fr_train_texts = ['START ' + fr.decode('utf-8') + ' END' for fr in fr_train_array]\n",
        "en_train_texts = ['START ' + en.decode('utf-8') + ' END' for en in en_train_array]\n",
        "fr_test_texts = ['START ' + fr.decode('utf-8') + ' END' for fr in fr_test_array]\n",
        "en_test_texts = ['START ' + en.decode('utf-8') + ' END' for en in en_test_array]\n",
        "\n",
        "# Tokenizer for English\n",
        "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "en_tokenizer.fit_on_texts(en_train_texts)\n",
        "\n",
        "# Tokenizer for French\n",
        "fr_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "fr_tokenizer.fit_on_texts(fr_train_texts)\n",
        "\n",
        "# Tokenize and pad the sequences\n",
        "en_train_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    en_tokenizer.texts_to_sequences(en_train_texts),\n",
        "    padding='post'\n",
        ")\n",
        "fr_train_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    fr_tokenizer.texts_to_sequences(fr_train_texts),\n",
        "    padding='post'\n",
        ")\n",
        "en_test_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    en_tokenizer.texts_to_sequences(en_test_texts),\n",
        "    padding='post'\n",
        ")\n",
        "fr_test_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    fr_tokenizer.texts_to_sequences(fr_test_texts),\n",
        "    padding='post'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_9OhZP7obfT",
        "outputId": "f39252f7-16a8-4063-acd0-e13a50f9c93b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([   3,    1, 7037,    2,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "en = en_train_sequences[0]\n",
        "fr = fr_train_sequences[0]\n",
        "en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Tr1M8BbobfT",
        "outputId": "d5d723b7-dba4-404b-e980-4bffee3d0324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized English training sequences:\n",
            "[   3    1 7037    2    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0]\n",
            "\n",
            "Tokenized French training sequences:\n",
            "[ 1  6 90 93  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            "\n",
            "Tokenized English test sequences:\n",
            "[   3   57   23 1110    6    5   12 2491   26  283   18  253   14    8\n",
            "  136   33 2757    5    2    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n",
            "\n",
            "Tokenized French test sequences:\n",
            "[    1  7208    10     5    11  3186    51   304    33    26   162    23\n",
            "    30 10861 11249     5  7894     2     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0]\n"
          ]
        }
      ],
      "source": [
        "# Print the tokenized sequences\n",
        "print(\"Tokenized English training sequences:\")\n",
        "for seq in en_train_sequences:\n",
        "    print(seq)\n",
        "    break\n",
        "\n",
        "print(\"\\nTokenized French training sequences:\")\n",
        "for seq in fr_train_sequences:\n",
        "    print(seq)\n",
        "    break\n",
        "\n",
        "print(\"\\nTokenized English test sequences:\")\n",
        "for seq in en_test_sequences:\n",
        "    print(seq)\n",
        "    break\n",
        "\n",
        "print(\"\\nTokenized French test sequences:\")\n",
        "for seq in fr_test_sequences:\n",
        "    print(seq)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_mmr6VTobfT",
        "outputId": "14ef4b7c-cb91-4775-924a-9607473b824c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example Input (French): (32, 156)\n",
            "Example Input (English - Input): (32, 206)\n",
            "Example Label (English - Output): (32, 206)\n"
          ]
        }
      ],
      "source": [
        "# Set batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create TensorFlow datasets for training and testing with batching\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(((fr_train_sequences, en_train_sequences[:, :-1]), en_train_sequences[:, 1:]))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(fr_train_sequences)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(((fr_test_sequences, en_test_sequences[:, :-1]), en_test_sequences[:, 1:]))\n",
        "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Print the first example from the training dataset\n",
        "for (fr, en), en_label in train_dataset.take(1):\n",
        "    print(\"Example Input (French):\", fr.shape)\n",
        "    print(\"Example Input (English - Input):\", en.shape)\n",
        "    print(\"Example Label (English - Output):\", en_label.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHZ5SzQpobfT"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctTpfBhOVdw-",
        "outputId": "071cc29a-edb7-4c0b-e3e0-aaabe501d3a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English (decoder) vocabulary size: 23917\n",
            "French (encoder) vocabulary size: 26646\n"
          ]
        }
      ],
      "source": [
        "decoder_vocab_size =  len(en_tokenizer.word_index) + 1\n",
        "encoder_vocab_size =  len(fr_tokenizer.word_index) + 1\n",
        "\n",
        "print('English (decoder) vocabulary size:', decoder_vocab_size)\n",
        "print('French (encoder) vocabulary size:', encoder_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 403,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eojCiRq5Vdw_",
        "outputId": "8a660b90-6a51-471c-a534-cb616a55c33e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "french\n",
            "156\n",
            "english\n",
            "207\n"
          ]
        }
      ],
      "source": [
        "encoder_block_size = fr.shape[1]\n",
        "print('french')\n",
        "print(encoder_block_size)\n",
        "\n",
        "decoder_block_size = en.shape[1]\n",
        "print('english')\n",
        "print(decoder_block_size)\n",
        "\n",
        "sqrt_d = 24  # Should be divisible by number of heads\n",
        "d_model = 576  # CAUTION: this must be divisible by 2 (required by positional encoding function)\n",
        "# d model should also be a perfect square"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 404,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEW6ve9JobfT",
        "outputId": "76339f28-f0c6-4b22-a5fc-4959fb14fc90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([5, 10])"
            ]
          },
          "execution_count": 404,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1)\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "positional_encoding(5,9).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 405,
      "metadata": {
        "id": "x9p9WmkOobfU"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "    self.pos_encoding = positional_encoding(vocab_size, d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]  # No of words\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 406,
      "metadata": {
        "id": "CygXk9QnobfU"
      },
      "outputs": [],
      "source": [
        "embed_fr = PositionalEmbedding(vocab_size=encoder_vocab_size, d_model=d_model)\n",
        "embed_en = PositionalEmbedding(vocab_size=decoder_vocab_size, d_model=d_model)\n",
        "\n",
        "fr = tf.stack(fr_train_sequences[0:batch_size])\n",
        "en = tf.stack(en_train_sequences[0:batch_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 407,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FwxuaxvobfU",
        "outputId": "1e65decb-71ee-40dc-cd5f-882dcfd56540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "French input shape: (32, 156)\n",
            "(32, 156, 576)\n",
            "English input shape: (32, 207)\n",
            "(32, 207, 576)\n"
          ]
        }
      ],
      "source": [
        "print('French input shape:', fr.shape)\n",
        "fr_emb = embed_fr(fr)\n",
        "print(fr_emb.shape)\n",
        "\n",
        "print('English input shape:', en.shape)\n",
        "en_emb = embed_en(en)\n",
        "print(en_emb.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 408,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLh928lbobfU",
        "outputId": "8199177c-1c1e-4c4b-cb11-392e32441428"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(32, 207), dtype=bool, numpy=\n",
              "array([[ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       ...,\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False],\n",
              "       [ True,  True,  True, ..., False, False, False]])>"
            ]
          },
          "execution_count": 408,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "en_emb._keras_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlDeYsyMVdxD"
      },
      "source": [
        "### Feebler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 409,
      "metadata": {
        "id": "LHufSgFKVdxD"
      },
      "outputs": [],
      "source": [
        "class Feebler(tf.Module):\n",
        "    def __init__(self, batch_size, sqrt_d, block_size):\n",
        "        self.weights = tf.Variable(tf.random.normal((sqrt_d, sqrt_d, block_size)), trainable=True)\n",
        "        self.batch_size = batch_size\n",
        "        self.sqrt_d = sqrt_d\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __call__(self, data):\n",
        "        # incoming tensor is of shape: (b, words, d)\n",
        "        # Feebler works with shape: (b, d, words)\n",
        "        print('I am inside feebler....')\n",
        "        print(data.shape)\n",
        "        print(self.batch_size)\n",
        "        print(self.sqrt_d)\n",
        "        print(self.block_size)\n",
        "        data = tf.transpose(data, (0, 2, 1))\n",
        "\n",
        "        data_reshaped = tf.reshape(data, (self.batch_size, self.sqrt_d, self.sqrt_d, self.block_size))\n",
        "        product = data_reshaped * self.weights\n",
        "        updated_product = tf.reduce_sum(product, axis=2)\n",
        "        return tf.reshape(updated_product, (self.batch_size, self.block_size, self.sqrt_d))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 410,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxXDOPsBVdxE",
        "outputId": "111f1708-f01e-47db-98fe-61b5c4da2329"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 374, 625)\n",
            "I am inside feebler....\n",
            "(32, 374, 625)\n",
            "32\n",
            "25\n",
            "374\n",
            "(32, 374, 25)\n"
          ]
        }
      ],
      "source": [
        "data = tf.random.normal((32, 374, 625))\n",
        "print(data.shape)\n",
        "feebler = Feebler(batch_size, 25, 374)\n",
        "output = feebler(data)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDnlDoXIVdxF"
      },
      "source": [
        "### Booster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 411,
      "metadata": {
        "id": "uzmYJDRVVdxG"
      },
      "outputs": [],
      "source": [
        "class Booster(tf.Module):\n",
        "    def __init__(self, batch_size, sqrt_d, block_size, n_embd):\n",
        "        self.weights = tf.Variable(tf.random.normal((sqrt_d, sqrt_d, block_size)), trainable=True)\n",
        "        self.batch_size = batch_size\n",
        "        self.sqrt_d = sqrt_d\n",
        "        self.block_size = block_size\n",
        "        self.n_embd = n_embd\n",
        "\n",
        "    def __call__(self, attention_output):\n",
        "        print('HERE 1 ----------------------------------')\n",
        "        attention_output = tf.reshape(attention_output, (self.batch_size, self.sqrt_d, self.block_size))\n",
        "        print('HERE 2 ----------------------------------')\n",
        "        attention_output_reshaped = tf.reshape(attention_output, (self.batch_size, 1, -1))\n",
        "        attention_output_reshaped = tf.tile(attention_output_reshaped, [1, self.sqrt_d, 1])\n",
        "        print('HERE 3 ----------------------------------')\n",
        "        attention_output_reshaped = tf.reshape(attention_output_reshaped, (self.batch_size, self.sqrt_d, self.sqrt_d, self.block_size))\n",
        "        revived_output = self.weights * attention_output_reshaped\n",
        "        print('HERE 4 ----------------------------------')\n",
        "        revived_output = tf.reshape(revived_output, (-1, self.block_size))\n",
        "        print('HERE 5 ----------------------------------')\n",
        "        return tf.reshape(revived_output, (self.batch_size, self.block_size, self.n_embd))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 412,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8qEd_h4VdxG",
        "outputId": "e7fca08d-40e2-40e5-da5b-7903b46941f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 374, 25)\n",
            "HERE 1 ----------------------------------\n",
            "HERE 2 ----------------------------------\n",
            "HERE 3 ----------------------------------\n",
            "HERE 4 ----------------------------------\n",
            "HERE 5 ----------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TensorShape([32, 374, 625])"
            ]
          },
          "execution_count": 412,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(output.shape)\n",
        "booster = Booster(batch_size, 25, 374, 625)\n",
        "booster(output).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRjeGHCLVdxH"
      },
      "source": [
        "### Quick Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 413,
      "metadata": {
        "id": "rOSsWUM1VdxH"
      },
      "outputs": [],
      "source": [
        "# I'll come for you dear, don't worry!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 414,
      "metadata": {
        "id": "kk4r-sWuobfU"
      },
      "outputs": [],
      "source": [
        "# MultiHeadAttention Layer\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_heads, key_dim, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.key_dim = key_dim\n",
        "        self.dropout = dropout\n",
        "\n",
        "        assert key_dim % num_heads == 0\n",
        "\n",
        "        self.depth = key_dim // num_heads\n",
        "\n",
        "        self.query_dense = tf.keras.layers.Dense(key_dim)\n",
        "        self.key_dense = tf.keras.layers.Dense(key_dim)\n",
        "        self.value_dense = tf.keras.layers.Dense(key_dim)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(key_dim)\n",
        "\n",
        "    def _masked_softmax(self, attention_scores, attention_mask):\n",
        "        \"\"\"Computes a softmax with explicit padding.\n",
        "\n",
        "        Args:\n",
        "            attention_scores: A tensor with shape [batch_size, num_heads,\n",
        "                query_seq_length, memory_seq_length]\n",
        "            attention_mask: A tensor with shape [batch_size, 1, query_seq_length,\n",
        "                memory_seq_length], the original attention mask.\n",
        "\n",
        "        Returns:\n",
        "            attention_probs: A tensor with shape [batch_size, num_heads,\n",
        "                query_seq_length, memory_seq_length], the new attention mask.\n",
        "        \"\"\"\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        # `attention_scores` = [B, N, T, S]\n",
        "        attention_mask = tf.expand_dims(\n",
        "            attention_mask, axis=1\n",
        "        )\n",
        "\n",
        "        # Add a large negative value to masked positions to make their attention scores close to zero\n",
        "        attention_scores = attention_scores + (1.0 - tf.cast(attention_mask, tf.float32)) * -1e9\n",
        "\n",
        "        # Calculate softmax along the last dimension (memory_seq_length)\n",
        "        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\n",
        "\n",
        "        return attention_probs\n",
        "\n",
        "    def split_heads(self, inputs, batch_size):\n",
        "        inputs = tf.reshape(inputs, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "        attention_weights = self._masked_softmax(scaled_attention_logits, mask)\n",
        "        output = tf.matmul(attention_weights, v)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "    def call(self, query, key, value, decoder_mask, encoder_mask, use_causal_mask=False):\n",
        "\n",
        "        batch_size = tf.shape(query)[0]\n",
        "\n",
        "        mask = self._compute_attention_mask(decoder_mask, encoder_mask, encoder_mask, use_causal_mask=use_causal_mask)\n",
        "\n",
        "        query = self.query_dense(query)\n",
        "        key = self.key_dense(key)\n",
        "        value = self.value_dense(value)\n",
        "\n",
        "        query = self.split_heads(query, batch_size)\n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = self.scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.key_dim))\n",
        "        output = self.dense(concat_attention)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    def _compute_attention_mask(\n",
        "        self, query_mask, value_mask, key_mask=None, attention_mask=None, use_causal_mask=False\n",
        "    ):\n",
        "        \"\"\" Computes the attention mask, using the Keras masks of the inputs.\n",
        "            E: number of words in encoder\n",
        "            D: number of words in decoder\n",
        "         \"\"\"\n",
        "\n",
        "        auto_mask = None\n",
        "        if query_mask is not None:\n",
        "            # On input from decoder\n",
        "            query_mask = tf.cast(query_mask, tf.bool)  # defensive casting\n",
        "            # B = batch size, D = max query length\n",
        "            auto_mask = query_mask[:, :, tf.newaxis]  # shape is [B, D, 1]\n",
        "        if value_mask is not None:\n",
        "            # On input from encoder\n",
        "            value_mask = tf.cast(value_mask, tf.bool)  # defensive casting\n",
        "            # B = batch size, E == max value length\n",
        "            mask = value_mask[:, tf.newaxis, :]  # shape is [B, 1, E]\n",
        "            auto_mask = mask if auto_mask is None else auto_mask & mask\n",
        "        if key_mask is not None:\n",
        "            # On input from encoder\n",
        "            key_mask = tf.cast(key_mask, tf.bool)  # defensive casting\n",
        "            # B == batch size, E == max key length == max value length\n",
        "            mask = key_mask[:, tf.newaxis, :]  # shape is [B, 1, E]\n",
        "            auto_mask = mask if auto_mask is None else auto_mask & mask\n",
        "        if use_causal_mask:\n",
        "            # the shape of the causal mask is [1, D, E]\n",
        "            mask = self._compute_causal_mask(query_mask, value_mask)\n",
        "            auto_mask = mask if auto_mask is None else auto_mask & mask\n",
        "        if auto_mask is not None:\n",
        "            # merge attention_mask & automatic mask, to shape [B, D, E]\n",
        "            attention_mask = (\n",
        "                auto_mask\n",
        "                if attention_mask is None\n",
        "                else tf.cast(attention_mask, bool) & auto_mask\n",
        "            )\n",
        "        return attention_mask\n",
        "\n",
        "    def _compute_causal_mask(self, query_mask, value_mask=None):\n",
        "        \"\"\" Computes the causal attention mask. \"\"\"\n",
        "\n",
        "        q_seq_length = tf.shape(query_mask)[1]\n",
        "        v_seq_length = q_seq_length if value_mask is None else tf.shape(value_mask)[1]\n",
        "        return tf.linalg.band_part(  # creates a lower triangular matrix\n",
        "            tf.ones((1, q_seq_length, v_seq_length), tf.bool), -1, 0\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 415,
      "metadata": {
        "id": "TaHo3VmsobfV"
      },
      "outputs": [],
      "source": [
        "# All its child layers need num_heads and key_dim(or dmodel) for initialization\n",
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 416,
      "metadata": {
        "id": "4jHz6kVZobfV"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context, decoder_mask, encoder_mask):\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        key=context,\n",
        "        value=context,\n",
        "        decoder_mask=decoder_mask,\n",
        "        encoder_mask=encoder_mask)\n",
        "\n",
        "    # Cache the attention scores for plotting later.\n",
        "    self.last_attn_scores = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 417,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dSHtrYuobfV",
        "outputId": "64064cef-4980-4030-86ab-c16246df32ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 156, 576)\n",
            "(32, 207, 576)\n",
            "(32, 207, 576)\n"
          ]
        }
      ],
      "source": [
        "sample_ca = CrossAttention(num_heads=2, key_dim=576)\n",
        "encoder_mask = getattr(fr_emb, \"_keras_mask\", None)\n",
        "decoder_mask = getattr(en_emb, \"_keras_mask\", None)\n",
        "print(fr_emb.shape)\n",
        "print(en_emb.shape)\n",
        "print(sample_ca(en_emb, fr_emb, decoder_mask, encoder_mask).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 418,
      "metadata": {
        "id": "-XIE4HJbobfW"
      },
      "outputs": [],
      "source": [
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x, encoder_mask):\n",
        "\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        decoder_mask=encoder_mask,\n",
        "        encoder_mask=encoder_mask)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 419,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX6NEPJJobfW",
        "outputId": "2275b238-1dc6-4c9b-e9a1-1719a87cd12f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 156, 576)\n",
            "(32, 156, 576)\n"
          ]
        }
      ],
      "source": [
        "sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=576)\n",
        "\n",
        "print(fr_emb.shape)\n",
        "encoder_mask = getattr(fr_emb, \"_keras_mask\", None)\n",
        "print(sample_gsa(fr_emb, encoder_mask).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 420,
      "metadata": {
        "id": "VKBgvHnhobfW"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x, decoder_mask):\n",
        "\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        decoder_mask=decoder_mask,\n",
        "        encoder_mask=decoder_mask,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 421,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPLw331oobfW",
        "outputId": "fec60126-e240-4a65-cee9-c614997eb9f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 207, 576)\n",
            "(32, 207, 576)\n"
          ]
        }
      ],
      "source": [
        "sample_csa = CausalSelfAttention(num_heads=2, key_dim=576)\n",
        "print(en_emb.shape)\n",
        "decoder_mask = getattr(en_emb, \"_keras_mask\", None)\n",
        "print(sample_csa(en_emb, decoder_mask).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 422,
      "metadata": {
        "id": "a1fcoWxdobfW"
      },
      "outputs": [],
      "source": [
        "# out1 = sample_csa(embed_en(en[:, :3]))\n",
        "# out2 = sample_csa(embed_en(en))[:, :3]\n",
        "\n",
        "# tf.reduce_max(abs(out1 - out2)).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 423,
      "metadata": {
        "id": "sTZVFb2RobfW"
      },
      "outputs": [],
      "source": [
        "# This layer needs dmodel and dff for initialization\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 424,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuYuCUktobfW",
        "outputId": "4a31e1f2-ca45-4e93-b6e8-e40e47bacfae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 207, 576)\n",
            "(32, 207, 576)\n"
          ]
        }
      ],
      "source": [
        "sample_ffn = FeedForward(576, 2048)\n",
        "\n",
        "print(en_emb.shape)\n",
        "print(sample_ffn(en_emb).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 425,
      "metadata": {
        "id": "Q7lWKLObobfX"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, batch_size, d_model, num_heads, dff, sqrt_d, block_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.feebler = Feebler(batch_size=batch_size, sqrt_d=sqrt_d, block_size=block_size)\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=sqrt_d,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(sqrt_d, dff)\n",
        "\n",
        "    self.booster = Booster(batch_size=batch_size, sqrt_d=sqrt_d, block_size=block_size, n_embd=d_model)\n",
        "\n",
        "  def call(self, x, encoder_mask):\n",
        "    '''\n",
        "    x is of shape: (b, words, d)\n",
        "    output is of shape: (b, words, d)\n",
        "    '''\n",
        "    print('I am inside encoder layer...')\n",
        "    print(x.shape)\n",
        "    x = self.feebler(x)  # output of this line: (b, words, sqrt(d))\n",
        "    x = self.self_attention(x, encoder_mask=encoder_mask)\n",
        "    x = self.ffn(x)\n",
        "    x = self.booster(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 426,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "K7nKhDXwobfX",
        "outputId": "b9fd08a9-6917-4cd8-fe7f-060b2e8e5bde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "156\n",
            "(32, 156, 576)\n",
            "I am inside encoder layer...\n",
            "(32, 156, 576)\n",
            "I am inside feebler....\n",
            "(32, 156, 576)\n",
            "32\n",
            "24\n",
            "156\n",
            "HERE 1 ----------------------------------\n",
            "HERE 2 ----------------------------------\n",
            "HERE 3 ----------------------------------\n",
            "HERE 4 ----------------------------------\n",
            "HERE 5 ----------------------------------\n",
            "(32, 156, 576)\n"
          ]
        }
      ],
      "source": [
        "sample_enc_layer = EncoderLayer(batch_size=batch_size, d_model=576, num_heads=4, dff=2048, sqrt_d=24, block_size=encoder_block_size, dropout_rate=0.1)\n",
        "print(encoder_block_size)\n",
        "print(fr_emb.shape)\n",
        "\n",
        "encoder_mask = getattr(fr_emb, \"_keras_mask\", None)\n",
        "print(sample_enc_layer(fr_emb, encoder_mask).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 427,
      "metadata": {
        "id": "l5kpTCPcobfX"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, batch_size, num_layers, d_model, num_heads,\n",
        "               dff, block_size, sqrt_d, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=block_size, d_model=d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(batch_size=batch_size,\n",
        "                     d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     sqrt_d=sqrt_d,\n",
        "                     block_size=block_size,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "    # Since we are passing x through feebler, hence\n",
        "    # we need to extract the masks before, and pass those\n",
        "    # explicity to the mha layer.\n",
        "    encoder_mask = getattr(x, \"_keras_mask\", None)\n",
        "\n",
        "    # Add dropout.\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    print('i am inside encoder')\n",
        "    print(x.shape)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, encoder_mask)\n",
        "\n",
        "    return x  # Shape `(batch_size, seq_len, d_model)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 429,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "F0Be4h7pobfX",
        "outputId": "b2981156-51f6-46eb-8d9b-448b1fb586d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 156)\n",
            "i am inside encoder\n",
            "(32, 156, 576)\n",
            "I am inside encoder layer...\n",
            "(32, 156, 576)\n",
            "I am inside feebler....\n",
            "(32, 156, 576)\n",
            "32\n",
            "24\n",
            "156\n",
            "HERE 1 ----------------------------------\n",
            "HERE 2 ----------------------------------\n",
            "HERE 3 ----------------------------------\n",
            "HERE 4 ----------------------------------\n",
            "HERE 5 ----------------------------------\n",
            "I am inside encoder layer...\n",
            "(32, 156, 576)\n",
            "I am inside feebler....\n",
            "(32, 156, 576)\n",
            "32\n",
            "24\n",
            "156\n",
            "HERE 1 ----------------------------------\n",
            "HERE 2 ----------------------------------\n",
            "HERE 3 ----------------------------------\n",
            "HERE 4 ----------------------------------\n",
            "HERE 5 ----------------------------------\n",
            "I am inside encoder layer...\n",
            "(32, 156, 576)\n",
            "I am inside feebler....\n",
            "(32, 156, 576)\n",
            "32\n",
            "24\n",
            "156\n",
            "HERE 1 ----------------------------------\n",
            "HERE 2 ----------------------------------\n",
            "HERE 3 ----------------------------------\n",
            "HERE 4 ----------------------------------\n",
            "HERE 5 ----------------------------------\n",
            "I am inside encoder layer...\n",
            "(32, 156, 576)\n",
            "I am inside feebler....\n",
            "(32, 156, 576)\n",
            "32\n",
            "24\n",
            "156\n",
            "HERE 1 ----------------------------------\n",
            "HERE 2 ----------------------------------\n",
            "HERE 3 ----------------------------------\n",
            "HERE 4 ----------------------------------\n",
            "HERE 5 ----------------------------------\n",
            "(32, 156, 576)\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the encoder.\n",
        "sample_encoder = Encoder(batch_size=batch_size,\n",
        "                         num_layers=4,\n",
        "                         d_model=576,\n",
        "                         num_heads=8,\n",
        "                         dff=2048,\n",
        "                         block_size=encoder_block_size,\n",
        "                         sqrt_d=sqrt_d)\n",
        "\n",
        "# Print the shape.\n",
        "print(fr.shape)\n",
        "sample_encoder_output = sample_encoder(fr, training=False)\n",
        "\n",
        "print(sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 430,
      "metadata": {
        "id": "jFx5XtwzobfX"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               batch_size,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               sqrt_d,\n",
        "               block_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.feebler = Feebler(batch_size=batch_size, sqrt_d=sqrt_d, block_size=block_size)\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "    self.booster = Booster(batch_size=batch_size, sqrt_d=sqrt_d, block_size=block_size, n_embd=d_model)\n",
        "\n",
        "  def call(self, x, context, decoder_mask, encoder_mask):\n",
        "    x = self.causal_self_attention(x=x, decoder_mask=decoder_mask)\n",
        "    x = self.cross_attention(x=x, context=context, decoder_mask=decoder_mask, encoder_mask=encoder_mask)\n",
        "\n",
        "    # Cache the last attention scores for plotting later\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 431,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbVMwWY8obfX",
        "outputId": "06c23a43-5ac8-4e21-a9de-54d535793066"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 207, 576)\n"
          ]
        }
      ],
      "source": [
        "sample_decoder_layer = DecoderLayer(batch_size=batch_size, d_model=576, num_heads=8, dff=2048, sqrt_d=sqrt_d, block_size=decoder_block_size, dropout_rate=0.1)\n",
        "\n",
        "decoder_mask = getattr(en_emb, \"_keras_mask\", None)\n",
        "encoder_mask = getattr(fr_emb, \"_keras_mask\", None)\n",
        "\n",
        "sample_decoder_layer_output = sample_decoder_layer(\n",
        "    x=en_emb, context=fr_emb, decoder_mask=decoder_mask, encoder_mask=encoder_mask\n",
        ")\n",
        "\n",
        "print(sample_decoder_layer_output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 432,
      "metadata": {
        "id": "8CPBX0G7obfY"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, batch_size, num_layers, d_model, num_heads, dff, block_size, sqrt_d, dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=block_size,\n",
        "                                             d_model=d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "      DecoderLayer(batch_size=batch_size,\n",
        "                   d_model=d_model,\n",
        "                   num_heads=num_heads,\n",
        "                   dff=dff,\n",
        "                   sqrt_d=sqrt_d,\n",
        "                   block_size=block_size,\n",
        "                   dropout_rate=dropout_rate)\n",
        "      for _ in range(num_layers)]\n",
        "\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    decoder_mask = getattr(x, \"_keras_mask\", None)\n",
        "    encoder_mask = getattr(context, \"_keras_mask\", None)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context, decoder_mask=decoder_mask, encoder_mask=encoder_mask)\n",
        "\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 433,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HL3BdVm1obfY",
        "outputId": "43f850ba-910a-4972-f387-afeda6cb037b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 207)\n",
            "(32, 156, 576)\n",
            "(32, 207, 576)\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the decoder.\n",
        "sample_decoder = Decoder(batch_size=batch_size,\n",
        "                         num_layers=4,\n",
        "                         d_model=576,\n",
        "                         num_heads=8,\n",
        "                         dff=2048,\n",
        "                         block_size=decoder_block_size,\n",
        "                         sqrt_d=sqrt_d)\n",
        "\n",
        "output = sample_decoder(\n",
        "    x=en,\n",
        "    context=fr_emb)\n",
        "\n",
        "# Print the shapes.\n",
        "print(en.shape)\n",
        "print(fr_emb.shape)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 434,
      "metadata": {
        "id": "l2w_OR8CobfY"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, batch_size, num_layers, d_model, sqrt_d, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(batch_size=batch_size, num_layers=num_layers,\n",
        "                            d_model=d_model,\n",
        "                            num_heads=num_heads,\n",
        "                            dff=dff,\n",
        "                            sqrt_d=sqrt_d,\n",
        "                            block_size=input_vocab_size,\n",
        "                            dropout_rate=dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(batch_size=batch_size, num_layers=num_layers,\n",
        "                           d_model=d_model,\n",
        "                           num_heads=num_heads,\n",
        "                           dff=dff,\n",
        "                           sqrt_d=sqrt_d,\n",
        "                           block_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
        "    # first argument.\n",
        "    context, x  = inputs\n",
        "    # print(\"i am inside the transformer\")\n",
        "    # print(context.shape)\n",
        "    # print(x.shape)\n",
        "\n",
        "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
        "\n",
        "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
        "\n",
        "    # Final linear layer output.\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    try:\n",
        "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
        "      # b/250038731\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 435,
      "metadata": {
        "id": "B_iC09cOobfY"
      },
      "outputs": [],
      "source": [
        "num_layers = 4\n",
        "d_model = 256\n",
        "sqrt_d = 16\n",
        "dff = 512\n",
        "num_heads = 4\n",
        "dropout_rate = 0.3\n",
        "batch_size = batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 436,
      "metadata": {
        "id": "uspQkoCqobfY",
        "outputId": "44247a08-c70c-41cf-b7ea-eded7dc4f263"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    batch_size=batch_size,\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    sqrt_d=sqrt_d,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=encoder_block_size,\n",
        "    target_vocab_size=decoder_block_size,\n",
        "    dropout_rate=dropout_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 437,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8KaLfdDobfY",
        "outputId": "94ade62a-cbfd-45af-9a39-b4241d682f06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i am inside encoder\n",
            "(32, 156, 256)\n",
            "I am inside encoder layer...\n",
            "(32, 156, 256)\n",
            "I am inside feebler....\n",
            "(32, 156, 256)\n",
            "32\n",
            "16\n",
            "156\n",
            "HERE 1 ----------------------------------\n",
            "HERE 2 ----------------------------------\n",
            "HERE 3 ----------------------------------\n",
            "HERE 4 ----------------------------------\n",
            "HERE 5 ----------------------------------\n",
            "I am inside encoder layer...\n",
            "(32, 156, 256)\n",
            "I am inside feebler....\n",
            "(32, 156, 256)\n",
            "32\n",
            "16\n",
            "156\n",
            "HERE 1 ----------------------------------\n",
            "HERE 2 ----------------------------------\n",
            "HERE 3 ----------------------------------\n",
            "HERE 4 ----------------------------------\n",
            "HERE 5 ----------------------------------\n",
            "I am inside encoder layer...\n",
            "(32, 156, 256)\n",
            "I am inside feebler....\n",
            "(32, 156, 256)\n",
            "32\n",
            "16\n",
            "156\n",
            "HERE 1 ----------------------------------\n",
            "HERE 2 ----------------------------------\n",
            "HERE 3 ----------------------------------\n",
            "HERE 4 ----------------------------------\n",
            "HERE 5 ----------------------------------\n",
            "I am inside encoder layer...\n",
            "(32, 156, 256)\n",
            "I am inside feebler....\n",
            "(32, 156, 256)\n",
            "32\n",
            "16\n",
            "156\n",
            "HERE 1 ----------------------------------\n",
            "HERE 2 ----------------------------------\n",
            "HERE 3 ----------------------------------\n",
            "HERE 4 ----------------------------------\n",
            "HERE 5 ----------------------------------\n",
            "(32, 207)\n",
            "(32, 156)\n",
            "(32, 207, 207)\n"
          ]
        }
      ],
      "source": [
        "output = transformer((fr, en))\n",
        "\n",
        "print(en.shape)\n",
        "print(fr.shape)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 438,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFLl3VOPobfY",
        "outputId": "ed22c51f-5941-42e2-ccf1-b6250264c1b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 4, 207, 156)\n"
          ]
        }
      ],
      "source": [
        "attn_scores = transformer.decoder.dec_layers[-1].last_attn_scores\n",
        "print(attn_scores.shape)  # (batch, heads, target_seq, input_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 439,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h7HrdiZobfZ",
        "outputId": "475d09d4-e4b8-4ac8-dbe8-e1e1200cc467"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"transformer_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder_23 (Encoder)        multiple                  431680    \n",
            "                                                                 \n",
            " decoder_32 (Decoder)        multiple                  3640064   \n",
            "                                                                 \n",
            " dense_2056 (Dense)          multiple                  53199     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4124943 (15.74 MB)\n",
            "Trainable params: 4124943 (15.74 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "transformer.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W91fHiB0obfZ"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 440,
      "metadata": {
        "id": "ZqwT9qhGobfZ"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 441,
      "metadata": {
        "id": "y3Y2P2QIobfZ"
      },
      "outputs": [],
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 442,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "JJdJDQriobfZ",
        "outputId": "caa74ee5-bfcc-4398-9f11-9233fb1552f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "execution_count": 442,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.xlabel('Train Step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 443,
      "metadata": {
        "id": "6SxrtMh-obfZ"
      },
      "outputs": [],
      "source": [
        "def masked_loss(label, pred):\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none'\n",
        "  )\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  mask = label!=0\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "\n",
        "  mask = label!=0\n",
        "  match = label==pred\n",
        "  match  = match & mask\n",
        "\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 444,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MfNMho-obfZ",
        "outputId": "8eddce4f-ef93-40c8-b4f3-4ab6495642c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 445,
      "metadata": {
        "id": "BUVa51U9obfa"
      },
      "outputs": [],
      "source": [
        "transformer.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[masked_accuracy])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 446,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V4ni6C-obfa",
        "outputId": "694733e3-9341-430b-c544-e0823bbea923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "i am inside encoder\n",
            "(None, 156, 256)\n",
            "I am inside encoder layer...\n",
            "(None, 156, 256)\n",
            "I am inside feebler....\n",
            "(None, 156, 256)\n",
            "32\n",
            "16\n",
            "156\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HERE 1 ----------------------------------\n",
            "HERE 2 ----------------------------------\n",
            "HERE 3 ----------------------------------\n",
            "HERE 4 ----------------------------------\n",
            "HERE 5 ----------------------------------\n",
            "I am inside encoder layer...\n",
            "(32, 156, 256)\n",
            "I am inside feebler....\n",
            "(32, 156, 256)\n",
            "32\n",
            "16\n",
            "156\n",
            "HERE 1 ----------------------------------\n",
            "HERE 2 ----------------------------------\n",
            "HERE 3 ----------------------------------\n",
            "HERE 4 ----------------------------------\n",
            "HERE 5 ----------------------------------\n",
            "I am inside encoder layer...\n",
            "(32, 156, 256)\n",
            "I am inside feebler....\n",
            "(32, 156, 256)\n",
            "32\n",
            "16\n",
            "156\n",
            "HERE 1 ----------------------------------\n",
            "HERE 2 ----------------------------------\n",
            "HERE 3 ----------------------------------\n",
            "HERE 4 ----------------------------------\n",
            "HERE 5 ----------------------------------\n",
            "I am inside encoder layer...\n",
            "(32, 156, 256)\n",
            "I am inside feebler....\n",
            "(32, 156, 256)\n",
            "32\n",
            "16\n",
            "156\n",
            "HERE 1 ----------------------------------\n",
            "HERE 2 ----------------------------------\n",
            "HERE 3 ----------------------------------\n",
            "HERE 4 ----------------------------------\n",
            "HERE 5 ----------------------------------\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i am inside encoder\n",
            "(None, 156, 256)\n",
            "I am inside encoder layer...\n",
            "(None, 156, 256)\n",
            "I am inside feebler....\n",
            "(None, 156, 256)\n",
            "32\n",
            "16\n",
            "156\n",
            "HERE 1 ----------------------------------\n",
            "HERE 2 ----------------------------------\n",
            "HERE 3 ----------------------------------\n",
            "HERE 4 ----------------------------------\n",
            "HERE 5 ----------------------------------\n",
            "I am inside encoder layer...\n",
            "(32, 156, 256)\n",
            "I am inside feebler....\n",
            "(32, 156, 256)\n",
            "32\n",
            "16\n",
            "156\n",
            "HERE 1 ----------------------------------\n",
            "HERE 2 ----------------------------------\n",
            "HERE 3 ----------------------------------\n",
            "HERE 4 ----------------------------------\n",
            "HERE 5 ----------------------------------\n",
            "I am inside encoder layer...\n",
            "(32, 156, 256)\n",
            "I am inside feebler....\n",
            "(32, 156, 256)\n",
            "32\n",
            "16\n",
            "156\n",
            "HERE 1 ----------------------------------\n",
            "HERE 2 ----------------------------------\n",
            "HERE 3 ----------------------------------\n",
            "HERE 4 ----------------------------------\n",
            "HERE 5 ----------------------------------\n",
            "I am inside encoder layer...\n",
            "(32, 156, 256)\n",
            "I am inside feebler....\n",
            "(32, 156, 256)\n",
            "32\n",
            "16\n",
            "156\n",
            "HERE 1 ----------------------------------\n",
            "HERE 2 ----------------------------------\n",
            "HERE 3 ----------------------------------\n",
            "HERE 4 ----------------------------------\n",
            "HERE 5 ----------------------------------\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "312/313 [============================>.] - ETA: 0s - loss: nan - masked_accuracy: 0.0000e+00"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-25 16:00:24.082665: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 1286027209508421945\n",
            "2024-01-25 16:00:24.082749: W tensorflow/core/framework/op_kernel.cc:1827] INVALID_ARGUMENT: required broadcastable shapes\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node gradient_tape/transformer_11/encoder_23/positional_embedding_67/embedding_67/embedding_lookup/Reshape_1 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/asyncio/events.py\", line 80, in _run\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 542, in dispatch_queue\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 531, in process_one\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 775, in execute_request\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipykernel_3676/645993415.py\", line 1, in <module>\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1154, in train_step\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py\", line 543, in minimize\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py\", line 276, in compute_gradients\n\nInput to reshape is a tensor with 2496 values, but the requested shape has 4992\n\t [[{{node gradient_tape/transformer_11/encoder_23/positional_embedding_67/embedding_67/embedding_lookup/Reshape_1}}]] [Op:__inference_train_function_941348]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 70\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y126sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m transformer\u001b[39m.\u001b[39;49mfit(train_dataset,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y126sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                 epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y126sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                 validation_data\u001b[39m=\u001b[39;49mtest_dataset)\n",
            "File \u001b[0;32m~/anaconda3/envs/trans/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/anaconda3/envs/trans/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node gradient_tape/transformer_11/encoder_23/positional_embedding_67/embedding_67/embedding_lookup/Reshape_1 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/asyncio/events.py\", line 80, in _run\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 542, in dispatch_queue\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 531, in process_one\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 775, in execute_request\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipykernel_3676/645993415.py\", line 1, in <module>\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1154, in train_step\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py\", line 543, in minimize\n\n  File \"/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/keras/src/optimizers/optimizer.py\", line 276, in compute_gradients\n\nInput to reshape is a tensor with 2496 values, but the requested shape has 4992\n\t [[{{node gradient_tape/transformer_11/encoder_23/positional_embedding_67/embedding_67/embedding_lookup/Reshape_1}}]] [Op:__inference_train_function_941348]"
          ]
        }
      ],
      "source": [
        "transformer.fit(train_dataset,\n",
        "                epochs=100,\n",
        "                validation_data=test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPy298u7obfa"
      },
      "source": [
        "## Translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {
        "id": "DLPkWeLoobfa"
      },
      "outputs": [],
      "source": [
        "def print_translation(sentence, text, ground_truth):\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "  print(f'{\"Prediction\":15s}: {text}')\n",
        "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 305,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZF-PSWTobfa",
        "outputId": "4958d51d-e484-4247-b7c2-49dd4dd3020d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token for 'Start': tf.Tensor(3, shape=(), dtype=int64)\n",
            "Token for 'End': tf.Tensor(2, shape=(), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "# Get the token for 'Start'\n",
        "start_token = en_tokenizer.word_index['start']\n",
        "start_token = tf.constant(start_token, dtype=tf.int64)\n",
        "\n",
        "# Get the token for 'End'\n",
        "end_token = en_tokenizer.word_index['end']\n",
        "end_token = tf.constant(end_token, dtype=tf.int64)\n",
        "\n",
        "print(\"Token for 'Start':\", start_token)\n",
        "print(\"Token for 'End':\", end_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 306,
      "metadata": {
        "id": "NM96ALuEobfa"
      },
      "outputs": [],
      "source": [
        "class Translator(tf.Module):\n",
        "  def __init__(self, en_tokenizer, fr_tokenizer, transformer):\n",
        "    self.en_tokenizer = en_tokenizer\n",
        "    self.fr_tokenizer = fr_tokenizer\n",
        "    self.transformer = transformer\n",
        "\n",
        "  def __call__(self, sentence, max_length=206):\n",
        "\n",
        "    encoder_input = sentence  # (1, 11)\n",
        "    start = start_token  # shape=(1,)\n",
        "    end = end_token  # shape=(1,)\n",
        "\n",
        "    # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
        "    # dynamic-loop can be traced by `tf.function`.\n",
        "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "    output_array = output_array.write(0, start)\n",
        "\n",
        "    for i in tf.range(max_length):\n",
        "      output = tf.transpose(output_array.stack())  # (1, X)\n",
        "      output = tf.expand_dims(output, 0)\n",
        "      predictions = self.transformer([encoder_input, output], training=False)\n",
        "\n",
        "      # Select the last token from the `seq_len` dimension.\n",
        "      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
        "      predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "      # Concatenate the `predicted_id` to the output which is given to the\n",
        "      # decoder as its input.\n",
        "      output_array = output_array.write(i+1, predicted_id[0][0])  # Writing a tensor of shape 1\n",
        "\n",
        "      if predicted_id == end:\n",
        "        break\n",
        "\n",
        "    output = tf.transpose(output_array.stack())\n",
        "    # The output shape is `(1, tokens)`.\n",
        "    # text = tokenizers.en.detokenize(output)[0]  # Shape: `()`.\n",
        "\n",
        "    # tokens = tokenizers.en.lookup(output)[0]\n",
        "    text = en_tokenizer.sequences_to_texts([output.numpy()])[0]\n",
        "    # text = text.split(' ')[1:-1]\n",
        "    # text = ' '.join(text)\n",
        "\n",
        "    return text\n",
        "\n",
        "translator = Translator(en_tokenizer, fr_tokenizer, transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEn798B77cRY",
        "outputId": "e22830d5-ecf3-430b-c8ef-5933c4c14d77"
      },
      "outputs": [
        {
          "ename": "InvalidArgumentError",
          "evalue": "Exception encountered when calling layer 'encoder_layer_51' (type EncoderLayer).\n\n{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 512 values, but the requested shape has 1277952 [Op:Reshape]\n\nCall arguments received by layer 'encoder_layer_51' (type EncoderLayer):\n  • x=tf.Tensor(shape=(1, 2, 256), dtype=float32)\n  • encoder_mask=tf.Tensor(shape=(1, 2), dtype=bool)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 75\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m encoded_ground_truth \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mpreprocessing\u001b[39m.\u001b[39msequence\u001b[39m.\u001b[39mpad_sequences(en_tokenizer\u001b[39m.\u001b[39mtexts_to_sequences([ground_truth]), padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Print the encoded sequences\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# print(\"Encoded Sentence:\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# print(encoded_sentence)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# print(\"\\nEncoded Ground Truth:\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# print(encoded_ground_truth)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m translated_text \u001b[39m=\u001b[39m translator(encoded_sentence, max_length\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m print_translation(sentence, translated_text, ground_truth)\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 75\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtranspose(output_array\u001b[39m.\u001b[39mstack())  \u001b[39m# (1, X)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexpand_dims(output, \u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer([encoder_input, output], training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Select the last token from the `seq_len` dimension.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m predictions \u001b[39m=\u001b[39m predictions[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:, :]  \u001b[39m# Shape `(batch_size, 1, vocab_size)`.\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/trans/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 75\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m context, x  \u001b[39m=\u001b[39m inputs\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# print(\"i am inside the transformer\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# print(context.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(context)  \u001b[39m# (batch_size, context_len, d_model)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x, context)  \u001b[39m# (batch_size, target_len, d_model)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Final linear layer output.\u001b[39;00m\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 75\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menc_layers[i](x, encoder_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 75\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, x, encoder_mask):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m  \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m  x is of shape: (b, words, d)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m  output is of shape: (b, words, d)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m  '''\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeebler(x)  \u001b[39m# output of this line: (b, words, sqrt(d))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attention(x, encoder_mask\u001b[39m=\u001b[39mencoder_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffn(x)\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 75\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# incoming tensor is of shape: (b, words, d)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Feebler works with shape: (b, d, words)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtranspose(data, (\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     data_reshaped \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mreshape(data, (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msqrt_d, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msqrt_d, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock_size))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     product \u001b[39m=\u001b[39m data_reshaped \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y134sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     updated_product \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_sum(product, axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'encoder_layer_51' (type EncoderLayer).\n\n{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 512 values, but the requested shape has 1277952 [Op:Reshape]\n\nCall arguments received by layer 'encoder_layer_51' (type EncoderLayer):\n  • x=tf.Tensor(shape=(1, 2, 256), dtype=float32)\n  • encoder_mask=tf.Tensor(shape=(1, 2), dtype=bool)"
          ]
        }
      ],
      "source": [
        "sentence = 'PREMIÈRE PARTIE'\n",
        "ground_truth = 'First Part'\n",
        "\n",
        "# sentence = 'Le grand Meaulnes'\n",
        "# ground_truth = 'The Wanderer'\n",
        "\n",
        "# sentence = 'Alain-Fournier'\n",
        "# ground_truth = 'Alain-Fournier'\n",
        "\n",
        "# Tokenize the sentences using the tokenizers\n",
        "encoded_sentence = tf.keras.preprocessing.sequence.pad_sequences(fr_tokenizer.texts_to_sequences([sentence]), padding='post')\n",
        "encoded_ground_truth = tf.keras.preprocessing.sequence.pad_sequences(en_tokenizer.texts_to_sequences([ground_truth]), padding='post')\n",
        "\n",
        "# Print the encoded sequences\n",
        "# print(\"Encoded Sentence:\")\n",
        "# print(encoded_sentence)\n",
        "# print(\"\\nEncoded Ground Truth:\")\n",
        "# print(encoded_ground_truth)\n",
        "\n",
        "translated_text = translator(encoded_sentence, max_length=2)\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m56DS55Yysvh"
      },
      "source": [
        "> Examples in French:\n",
        "\n",
        "    Le grand Meaulnes\n",
        "    Alain-Fournier\n",
        "    PREMIÈRE PARTIE\n",
        "\n",
        "> Examples in English:\n",
        "\n",
        "    The Wanderer\n",
        "    Alain-Fournier\n",
        "    First Part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLKNDTXeobfb"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAXW3hxG_not",
        "outputId": "9f93bd0a-5d8b-4191-b790-73f5bb511629"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU Score: 0.11428571428571427\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/home/tuk/anaconda3/envs/trans/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ],
      "source": [
        "references_list = [['are', 'you', 'fond', 'of', 'and', 'he', 'searched',\n",
        "   'my', 'face', 'with', 'eyes', 'that', 'i', 'saw', 'were',\n",
        "     'dark,', 'and', 'end']]\n",
        "\n",
        "hypothesis = ['start', 'they', 'were', 'waiting', 'for', 'a', 'disposition',\n",
        "               'of', 'the', 'house;', 'and', 'one', 'hand', 'passed', 'her',\n",
        "                 'windows', 'in', 'the', 'did', 'not', 'dwell', 'on', 'the',\n",
        "                 'should', 'quit', 'garden', 'gardiner,', 'whose', 'bright',\n",
        "                 'church', 'bright', '.', '.', \".'\", 'end']\n",
        "\n",
        "# # Reference and hypothesis sentences\n",
        "# references_list = [['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'for']]\n",
        "# hypothesis = ['the', 'fast', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'the', 'lazy', 'dog']\n",
        "\n",
        "# Compute BLEU score for a single sentence\n",
        "bleu_score = corpus_bleu([references_list], [hypothesis], weights=(1, 0, 0, 0))\n",
        "print(f'BLEU Score: {bleu_score}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 309,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGLLNwFSASpF",
        "outputId": "1b9ddc1b-710a-4546-91ee-632f91d596fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU Score: 9.418382295637229e-232\n"
          ]
        }
      ],
      "source": [
        "# Reference and hypothesis sentences for multiple examples\n",
        "# DOES NOT WORK\n",
        "references = [\n",
        "    ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'],\n",
        "    ['the', 'cat', 'in', 'the', 'hat']\n",
        "]\n",
        "\n",
        "hypotheses = [\n",
        "    ['the', 'fast', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'],\n",
        "    ['a', 'cat', 'in', 'a', 'hat']\n",
        "]\n",
        "\n",
        "# Compute BLEU score for multiple sentences\n",
        "bleu_score = corpus_bleu(references, hypotheses)\n",
        "print(f'BLEU Score: {bleu_score}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDjtJF0KVdyK"
      },
      "source": [
        "### Evaluation of bleu score on train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {
        "id": "t3ALcYLJVdyL",
        "outputId": "8c18b0e4-ec6e-4f8e-ee90-1ed8b31a8a77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating BLEU Score:   0%|          | 0/313 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "Exception encountered when calling layer 'encoder_layer_51' (type EncoderLayer).\n\n{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 39936 values, but the requested shape has 1277952 [Op:Reshape]\n\nCall arguments received by layer 'encoder_layer_51' (type EncoderLayer):\n  • x=tf.Tensor(shape=(1, 156, 256), dtype=float32)\n  • encoder_mask=tf.Tensor(shape=(1, 156), dtype=bool)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 81\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         pbar\u001b[39m.\u001b[39mset_postfix({\u001b[39m'\u001b[39m\u001b[39mBleu Score\u001b[39m\u001b[39m'\u001b[39m: overall_bleu_score\u001b[39m/\u001b[39mcount})\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m overall_bleu_score\u001b[39m/\u001b[39mcount\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m bleu_score \u001b[39m=\u001b[39m calculate_bleu_score(translator, train_dataset)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBLEU Score on train Dataset:\u001b[39m\u001b[39m\"\u001b[39m, bleu_score)\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 81\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m ground_truth \u001b[39m=\u001b[39m en_tokenizer\u001b[39m.\u001b[39msequences_to_texts(en_label_sentence\u001b[39m.\u001b[39mnumpy())[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# ground_truth = ground_truth.split(' ')[1:-1]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# ground_truth = ' '.join(ground_truth)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m translated_text \u001b[39m=\u001b[39m translator(fr_sentence, max_length)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Bleu score evaluation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m references \u001b[39m=\u001b[39m []\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 81\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtranspose(output_array\u001b[39m.\u001b[39mstack())  \u001b[39m# (1, X)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexpand_dims(output, \u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer([encoder_input, output], training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Select the last token from the `seq_len` dimension.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m predictions \u001b[39m=\u001b[39m predictions[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:, :]  \u001b[39m# Shape `(batch_size, 1, vocab_size)`.\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/trans/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 81\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m context, x  \u001b[39m=\u001b[39m inputs\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# print(\"i am inside the transformer\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# print(context.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(context)  \u001b[39m# (batch_size, context_len, d_model)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x, context)  \u001b[39m# (batch_size, target_len, d_model)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Final linear layer output.\u001b[39;00m\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 81\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menc_layers[i](x, encoder_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 81\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, x, encoder_mask):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m  \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m  x is of shape: (b, words, d)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m  output is of shape: (b, words, d)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m  '''\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeebler(x)  \u001b[39m# output of this line: (b, words, sqrt(d))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attention(x, encoder_mask\u001b[39m=\u001b[39mencoder_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffn(x)\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 81\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# incoming tensor is of shape: (b, words, d)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Feebler works with shape: (b, d, words)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtranspose(data, (\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     data_reshaped \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mreshape(data, (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msqrt_d, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msqrt_d, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock_size))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     product \u001b[39m=\u001b[39m data_reshaped \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y143sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     updated_product \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_sum(product, axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'encoder_layer_51' (type EncoderLayer).\n\n{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 39936 values, but the requested shape has 1277952 [Op:Reshape]\n\nCall arguments received by layer 'encoder_layer_51' (type EncoderLayer):\n  • x=tf.Tensor(shape=(1, 156, 256), dtype=float32)\n  • encoder_mask=tf.Tensor(shape=(1, 156), dtype=bool)"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "# Disable specific warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "def calculate_bleu_score(translator, train_dataset, max_length=206):\n",
        "    overall_bleu_score = 0.0\n",
        "    count = 0\n",
        "\n",
        "    # Use tqdm for progress bar\n",
        "    pbar = tqdm(train_dataset, desc=\"Calculating BLEU Score\")\n",
        "\n",
        "    for (fr, en), en_label in pbar:\n",
        "        for i in range(fr.shape[0]):\n",
        "            fr_sentence = fr[i:i+1, :]\n",
        "            en_label_sentence = en_label[i:i+1, :]\n",
        "            ground_truth = en_tokenizer.sequences_to_texts(en_label_sentence.numpy())[0]\n",
        "            # ground_truth = ground_truth.split(' ')[1:-1]\n",
        "            # ground_truth = ' '.join(ground_truth)\n",
        "\n",
        "            translated_text = translator(fr_sentence, max_length)\n",
        "\n",
        "            # Bleu score evaluation\n",
        "            references = []\n",
        "            references.append(ground_truth.split(' '))\n",
        "            translated_text = translated_text.split(' ')\n",
        "            overall_bleu_score += corpus_bleu([references], [translated_text], weights=(1, 0, 0, 0))\n",
        "            count += 1\n",
        "\n",
        "        # Display progress in tqdm\n",
        "        pbar.set_postfix({'Bleu Score': overall_bleu_score/count})\n",
        "\n",
        "    return overall_bleu_score/count\n",
        "\n",
        "bleu_score = calculate_bleu_score(translator, train_dataset)\n",
        "print(\"BLEU Score on train Dataset:\", bleu_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46SGk2OnVdyM"
      },
      "source": [
        "### Evaluation of bleu score on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 311,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P9BTZYABlfo",
        "outputId": "09bc20ad-ace6-42be-fe13-9ec02e134715"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating BLEU Score:   0%|          | 0/94 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "Exception encountered when calling layer 'encoder_layer_51' (type EncoderLayer).\n\n{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 34048 values, but the requested shape has 1277952 [Op:Reshape]\n\nCall arguments received by layer 'encoder_layer_51' (type EncoderLayer):\n  • x=tf.Tensor(shape=(1, 133, 256), dtype=float32)\n  • encoder_mask=tf.Tensor(shape=(1, 133), dtype=bool)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 83\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         pbar\u001b[39m.\u001b[39mset_postfix({\u001b[39m'\u001b[39m\u001b[39mBleu Score\u001b[39m\u001b[39m'\u001b[39m: overall_bleu_score\u001b[39m/\u001b[39mcount})\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m overall_bleu_score\u001b[39m/\u001b[39mcount\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m bleu_score \u001b[39m=\u001b[39m calculate_bleu_score(translator, test_dataset)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBLEU Score on Test Dataset:\u001b[39m\u001b[39m\"\u001b[39m, bleu_score)\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 83\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m ground_truth \u001b[39m=\u001b[39m en_tokenizer\u001b[39m.\u001b[39msequences_to_texts(en_label_sentence\u001b[39m.\u001b[39mnumpy())[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# ground_truth = ground_truth.split(' ')[1:-1]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# ground_truth = ' '.join(ground_truth)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m translated_text \u001b[39m=\u001b[39m translator(fr_sentence, max_length)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Bleu score evaluation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m references \u001b[39m=\u001b[39m []\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 83\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtranspose(output_array\u001b[39m.\u001b[39mstack())  \u001b[39m# (1, X)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexpand_dims(output, \u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer([encoder_input, output], training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Select the last token from the `seq_len` dimension.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m predictions \u001b[39m=\u001b[39m predictions[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:, :]  \u001b[39m# Shape `(batch_size, 1, vocab_size)`.\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/trans/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 83\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m context, x  \u001b[39m=\u001b[39m inputs\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# print(\"i am inside the transformer\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# print(context.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(context)  \u001b[39m# (batch_size, context_len, d_model)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x, context)  \u001b[39m# (batch_size, target_len, d_model)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Final linear layer output.\u001b[39;00m\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 83\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menc_layers[i](x, encoder_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 83\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, x, encoder_mask):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m  \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m  x is of shape: (b, words, d)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m  output is of shape: (b, words, d)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m  '''\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeebler(x)  \u001b[39m# output of this line: (b, words, sqrt(d))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attention(x, encoder_mask\u001b[39m=\u001b[39mencoder_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffn(x)\n",
            "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/machine translation/old transformer/nmt_with_opus_dataset_quickformer.ipynb Cell 83\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# incoming tensor is of shape: (b, words, d)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Feebler works with shape: (b, d, words)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     data \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtranspose(data, (\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     data_reshaped \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mreshape(data, (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msqrt_d, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msqrt_d, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock_size))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     product \u001b[39m=\u001b[39m data_reshaped \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/machine%20translation/old%20transformer/nmt_with_opus_dataset_quickformer.ipynb#Y145sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     updated_product \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_sum(product, axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'encoder_layer_51' (type EncoderLayer).\n\n{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 34048 values, but the requested shape has 1277952 [Op:Reshape]\n\nCall arguments received by layer 'encoder_layer_51' (type EncoderLayer):\n  • x=tf.Tensor(shape=(1, 133, 256), dtype=float32)\n  • encoder_mask=tf.Tensor(shape=(1, 133), dtype=bool)"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "# Disable specific warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "def calculate_bleu_score(translator, test_dataset, max_length=206):\n",
        "    overall_bleu_score = 0.0\n",
        "    count = 0\n",
        "\n",
        "    # Use tqdm for progress bar\n",
        "    pbar = tqdm(test_dataset, desc=\"Calculating BLEU Score\")\n",
        "\n",
        "    for (fr, en), en_label in pbar:\n",
        "        for i in range(fr.shape[0]):\n",
        "            fr_sentence = fr[i:i+1, :]\n",
        "            en_label_sentence = en_label[i:i+1, :]\n",
        "            ground_truth = en_tokenizer.sequences_to_texts(en_label_sentence.numpy())[0]\n",
        "            # ground_truth = ground_truth.split(' ')[1:-1]\n",
        "            # ground_truth = ' '.join(ground_truth)\n",
        "\n",
        "            translated_text = translator(fr_sentence, max_length)\n",
        "\n",
        "            # Bleu score evaluation\n",
        "            references = []\n",
        "            references.append(ground_truth.split(' '))\n",
        "            translated_text = translated_text.split(' ')\n",
        "            overall_bleu_score += corpus_bleu([references], [translated_text], weights=(1, 0, 0, 0))\n",
        "            count += 1\n",
        "\n",
        "        # Display progress in tqdm\n",
        "        pbar.set_postfix({'Bleu Score': overall_bleu_score/count})\n",
        "\n",
        "    return overall_bleu_score/count\n",
        "\n",
        "bleu_score = calculate_bleu_score(translator, test_dataset)\n",
        "print(\"BLEU Score on Test Dataset:\", bleu_score)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "048473bedaf54822a40891dec1051d46": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04ca613a16974e96b5e6f5a5ddcbc730": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08ef4bbd6b1a4b77b1e8fc14a1814de7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "0a923ebe2caf43509f2d98c056e3afd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f9925afbff445dd85e7211c2053526c",
            "max": 127085,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7501ce04f2c9484ead067312cfea1f86",
            "value": 127085
          }
        },
        "0f9925afbff445dd85e7211c2053526c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "101214c744ca48e9a891440cffb6be04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19078d674b40454db42eef4613312cc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85f4ed22ef0c4fe5bc238ba79b455282",
            "placeholder": "​",
            "style": "IPY_MODEL_694af1363f6b4a30b4c94ef7738994fd",
            "value": "Generating train split: 100%"
          }
        },
        "1d3693eb2d3a41c98e8dd529076fdfbc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f2a526f41e0418aa30f933cffd55df4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e9d5fad71d043a896128c05ad757b02",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90bc1de718a1447c9a013366d946771a",
            "value": 1
          }
        },
        "2386868cd2294786a0a7f7303f4915e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e7273229bb3478f9a0f27f78ed99dbe",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2892bcbaf5d4d62b0637250b824f386",
            "value": 1
          }
        },
        "26547b5811e54faebe60a26534223d99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b2c473652a54596a2a9a4248d30af39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a4cd7b655b5649ca9453dcfaf033df63",
              "IPY_MODEL_0a923ebe2caf43509f2d98c056e3afd3",
              "IPY_MODEL_531282d34fdd48f98578f55adc427d62"
            ],
            "layout": "IPY_MODEL_f87e449cbcad4129a3a7f5c3a426cc02"
          }
        },
        "2d87b691ad7a450b9073b91b35bec33a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aa478c2c0b545daa39e831ffc87f695",
            "placeholder": "​",
            "style": "IPY_MODEL_d3f9cc3967314482bfbedb394f3b0f12",
            "value": "Generating train examples...: "
          }
        },
        "30bc82e80fb64e1c8f54137f03deb2ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37e41a95c2e44348a5efa428518b6087": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bbfdc5583184d2087aab8887d27d677": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c92b27dc60741da9be9ccc99f73d378": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44caa122c7b34b73ae89bda1987dc51d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_486a9cd8a6be4ec08eb6b45cc0f5b364",
            "placeholder": "​",
            "style": "IPY_MODEL_4a219508a82a47a89270fceb1c5e3417",
            "value": "Generating splits...: 100%"
          }
        },
        "486a9cd8a6be4ec08eb6b45cc0f5b364": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a219508a82a47a89270fceb1c5e3417": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "531282d34fdd48f98578f55adc427d62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7a4e18f53664877b03ac26e13741162",
            "placeholder": "​",
            "style": "IPY_MODEL_04ca613a16974e96b5e6f5a5ddcbc730",
            "value": " 93865/127085 [00:00&lt;00:00, 496210.37 examples/s]"
          }
        },
        "5a9f54b9007c48aab2aa0df019afef85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_048473bedaf54822a40891dec1051d46",
            "placeholder": "​",
            "style": "IPY_MODEL_d96b2eee1e7f4f1885c74dce5adc0e1e",
            "value": " 1/1 [00:21&lt;00:00, 21.20s/ splits]"
          }
        },
        "636c29e4113e41c5ac544452c8c881da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9396ddc885df46248ad3eb26ae26e114",
            "placeholder": "​",
            "style": "IPY_MODEL_37e41a95c2e44348a5efa428518b6087",
            "value": " 21.0M/21.0M [00:01&lt;00:00, 13.4MB/s]"
          }
        },
        "63941e4f655e46659f725befa5b3d6ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "694af1363f6b4a30b4c94ef7738994fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6acfd8bfffba491ab26bd0cd5d7b9b5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c52bede33b9b48f890611a21e3c7f815",
              "IPY_MODEL_75b5cd22834746daba6455c0bdd0eb3a",
              "IPY_MODEL_636c29e4113e41c5ac544452c8c881da"
            ],
            "layout": "IPY_MODEL_30bc82e80fb64e1c8f54137f03deb2ce"
          }
        },
        "6e7273229bb3478f9a0f27f78ed99dbe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7501ce04f2c9484ead067312cfea1f86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "75b5cd22834746daba6455c0bdd0eb3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb7ce590adce48a9966b0bab35dd99ff",
            "max": 20985324,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_839408a55582449fb57e26d1c63c9578",
            "value": 20985324
          }
        },
        "783f52bdd0cc423198b3e7b7c4262bfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d3693eb2d3a41c98e8dd529076fdfbc",
            "placeholder": "​",
            "style": "IPY_MODEL_ee551a94920b4f9693a850e68ed7572d",
            "value": " 125311/? [00:20&lt;00:00, 6163.07 examples/s]"
          }
        },
        "839408a55582449fb57e26d1c63c9578": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "844e668e1206401f9fe3bf8e95694288": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26547b5811e54faebe60a26534223d99",
            "placeholder": "​",
            "style": "IPY_MODEL_c16d473c9c0c462faa4b9bce88aa7689",
            "value": " 127085/127085 [00:00&lt;00:00, 533723.23 examples/s]"
          }
        },
        "85f4ed22ef0c4fe5bc238ba79b455282": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87667ad986a643c8a64226656a90a93c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d523043fe3f7494eb7324f01a5b5ce00",
            "max": 127085,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3c92b27dc60741da9be9ccc99f73d378",
            "value": 127085
          }
        },
        "8a90f328279449ee845230728c90c1c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aa478c2c0b545daa39e831ffc87f695": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e9d5fad71d043a896128c05ad757b02": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "90bc1de718a1447c9a013366d946771a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9396ddc885df46248ad3eb26ae26e114": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4cd7b655b5649ca9453dcfaf033df63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bbfdc5583184d2087aab8887d27d677",
            "placeholder": "​",
            "style": "IPY_MODEL_eacc07572c86432d9acc5f0c5612776a",
            "value": "Shuffling /root/tensorflow_datasets/opus_books/en-fr/1.0.0.incompletePXSUGA/opus_books-train.tfrecord*...:  74%"
          }
        },
        "ac58fbaf420143828f2ccdf7824aca44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_19078d674b40454db42eef4613312cc8",
              "IPY_MODEL_87667ad986a643c8a64226656a90a93c",
              "IPY_MODEL_844e668e1206401f9fe3bf8e95694288"
            ],
            "layout": "IPY_MODEL_d0b8215ea74a4bf98bdbcab831d97344"
          }
        },
        "afd0dd2fc56046af8f23a554ac7c6ed7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d87b691ad7a450b9073b91b35bec33a",
              "IPY_MODEL_1f2a526f41e0418aa30f933cffd55df4",
              "IPY_MODEL_783f52bdd0cc423198b3e7b7c4262bfe"
            ],
            "layout": "IPY_MODEL_08ef4bbd6b1a4b77b1e8fc14a1814de7"
          }
        },
        "b545a45778be4e419d7be3eff31c11a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44caa122c7b34b73ae89bda1987dc51d",
              "IPY_MODEL_2386868cd2294786a0a7f7303f4915e0",
              "IPY_MODEL_5a9f54b9007c48aab2aa0df019afef85"
            ],
            "layout": "IPY_MODEL_63941e4f655e46659f725befa5b3d6ce"
          }
        },
        "b7a4e18f53664877b03ac26e13741162": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb7ce590adce48a9966b0bab35dd99ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c16d473c9c0c462faa4b9bce88aa7689": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2892bcbaf5d4d62b0637250b824f386": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c52bede33b9b48f890611a21e3c7f815": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a90f328279449ee845230728c90c1c6",
            "placeholder": "​",
            "style": "IPY_MODEL_101214c744ca48e9a891440cffb6be04",
            "value": "Downloading data: 100%"
          }
        },
        "d0b8215ea74a4bf98bdbcab831d97344": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3f9cc3967314482bfbedb394f3b0f12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d523043fe3f7494eb7324f01a5b5ce00": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d96b2eee1e7f4f1885c74dce5adc0e1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eacc07572c86432d9acc5f0c5612776a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee551a94920b4f9693a850e68ed7572d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f87e449cbcad4129a3a7f5c3a426cc02": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
