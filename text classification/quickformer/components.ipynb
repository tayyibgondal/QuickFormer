{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 512 # max number of words going into the model?\n",
    "\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "eval_iters = 200\n",
    "\n",
    "learning_rate = 1e-3\n",
    "device = 'cpu' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "n_embd = 64\n",
    "sqrt_d = torch.sqrt(torch.tensor(n_embd)).int().item()\n",
    "n_head = sqrt_d // 2\n",
    "n_layer = 4\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feebler(nn.Module):\n",
    "    ''' \n",
    "    T: Number of words going into the model\n",
    "    C: Embedding dimension\n",
    "    B: Batch size\n",
    "    \n",
    "    input: B, T, C\n",
    "    output: B, T, sqrt(C)\n",
    "    '''\n",
    "    def __init__(self, sqrt_d):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(\n",
    "            torch.randn(sqrt_d, sqrt_d, block_size)\n",
    "        )\n",
    "        self.sqrt_d = sqrt_d\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Data is of shape (b, n, d)\n",
    "        data_reshaped = data.view(batch_size, n_embd, block_size)  # set up data for feebler\n",
    "        data_reshaped = data.view(batch_size, self.sqrt_d, self.sqrt_d, block_size)  # reshape incoming data\n",
    "        product = data_reshaped * self.weights  # multiply data with weights\n",
    "        # perform columnwise sum inside each window\n",
    "        updated_product = torch.sum(product, dim=2, keepdim=False)  # finally we have converted from dxn to sqrt(d)xn\n",
    "        return updated_product.view(batch_size, block_size, self.sqrt_d)\n",
    "    \n",
    "\n",
    "class Booster(nn.Module):\n",
    "    ''' \n",
    "    input: B, T, sqrt(C)\n",
    "    output: B, T, C\n",
    "    '''\n",
    "    def __init__(self, sqrt_d):\n",
    "        super(Booster, self).__init__()\n",
    "        self.weights = nn.Parameter(\n",
    "            torch.randn(sqrt_d, sqrt_d, block_size)\n",
    "        )\n",
    "        self.sqrt_d = sqrt_d\n",
    "\n",
    "    def forward(self, attention_output):\n",
    "        # attention_output is of shape (batch, n, sqrt_d)\n",
    "        # set up data shape for the booster\n",
    "        attention_output = attention_output.view(batch_size, self.sqrt_d, block_size)\n",
    "        attention_output_reshaped = attention_output.view(batch_size, 1, -1) # flatten all rows into one row\n",
    "        attention_output_reshaped = attention_output_reshaped.repeat(1, self.sqrt_d, 1)  # repeat each row sqrt_d times\n",
    "        attention_output_reshaped = attention_output_reshaped.view(batch_size, self.sqrt_d, self.sqrt_d, block_size)\n",
    "        # multiply\n",
    "        revived_output = self.weights * attention_output_reshaped\n",
    "        revived_output = revived_output.view(-1, block_size)\n",
    "        return revived_output.view(batch_size, block_size, n_embd)\n",
    "\n",
    "class QuickHead(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(sqrt_d, head_size, bias=False)\n",
    "        self.query = nn.Linear(sqrt_d, head_size, bias=False)\n",
    "        self.value = nn.Linear(sqrt_d, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape (batch_size, n, sqrt_d)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "\n",
    "        collective_k = k.sum(1, keepdim=True)\n",
    "        # Broadcast explicitly\n",
    "        collective_k_bc = collective_k.repeat(1, block_size, 1)\n",
    "        # q multiply k\n",
    "        qk = q * collective_k_bc\n",
    "        attention_weights = torch.softmax(qk, dim=1)\n",
    "        collective_v = v.sum(dim=1, keepdim=True)\n",
    "        collective_v_bc = collective_v.repeat(1, block_size, 1)\n",
    "        output = collective_v_bc * attention_weights\n",
    "        return output\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([QuickHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(sqrt_d, sqrt_d) # global variable sqrt_d\n",
    "        self.dropout = nn.Dropout(dropout)  # global variable dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, sqrt_d):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(sqrt_d, 4 * sqrt_d),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * sqrt_d, sqrt_d),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = sqrt_d // n_head\n",
    "        self.feebler = Feebler(sqrt_d)\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(sqrt_d)\n",
    "        self.ln1 = nn.LayerNorm(sqrt_d)\n",
    "        self.ln2 = nn.LayerNorm(sqrt_d)\n",
    "        self.booster = Booster(sqrt_d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feebler(x)\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        x = self.booster(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512, 64])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = Block(n_embd, n_head)\n",
    "b(torch.rand(batch_size, block_size, n_embd)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100000\n",
    "\n",
    "# super simple quickformer model\n",
    "class Quickformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, 1)\n",
    "        self.logits_maker = nn.Linear(block_size, 1)\n",
    "        self.classifier = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        x = self.lm_head(x) # (B,T,1)\n",
    "        x = x.squeeze(2) # (B,T)\n",
    "        logits = self.logits_maker(x) # (B,1)\n",
    "        results = self.classifier(logits) # (B, 1)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.binary_cross_entropy(results, targets)\n",
    "\n",
    "        return results, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = Quickformer()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.rand(batch_size, block_size).long().to(device)\n",
    "print(inp.shape)\n",
    "l, ll = model(inp)\n",
    "l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Load IMDb dataset from Hugging Face\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Use a pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define a custom PyTorch Dataset\n",
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=block_size):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        text = item[\"text\"]\n",
    "        label = item[\"label\"]\n",
    "\n",
    "        # Tokenize and encode the text\n",
    "        inputs = self.tokenizer(\n",
    "            text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"].squeeze()\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze()\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"label\": torch.tensor(label)}\n",
    "\n",
    "# Create Train dataset (pytorch)\n",
    "imdb_dataset_train = IMDbDataset(dataset[\"train\"], tokenizer)\n",
    "# Create Test dataset (pytorch)\n",
    "imdb_dataset_test = IMDbDataset(dataset[\"test\"], tokenizer)\n",
    "\n",
    "# Create PyTorch DataLoader for train set\n",
    "dataloader_train = DataLoader(imdb_dataset_train, batch_size=batch_size, shuffle=True)\n",
    "# Create PyTorch DataLoader for test set\n",
    "dataloader_test = DataLoader(imdb_dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetInfo(description='', citation='', homepage='', license='', features={'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='imdb', config_name='plain_text', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=33435948, num_examples=25000, shard_lengths=None, dataset_name='imdb'), 'test': SplitInfo(name='test', num_bytes=32653810, num_examples=25000, shard_lengths=None, dataset_name='imdb'), 'unsupervised': SplitInfo(name='unsupervised', num_bytes=67113044, num_examples=50000, shard_lengths=None, dataset_name='imdb')}, download_checksums={'hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/train-00000-of-00001.parquet': {'num_bytes': 20979968, 'checksum': None}, 'hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/test-00000-of-00001.parquet': {'num_bytes': 20470363, 'checksum': None}, 'hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/unsupervised-00000-of-00001.parquet': {'num_bytes': 41996509, 'checksum': None}}, download_size=83446840, post_processing_size=None, dataset_size=133202802, size_in_bytes=216649642)\n",
      "dict_keys(['text', 'label'])\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dataset info to see the details\n",
    "print(dataset[\"train\"].info)  # shows 0 = neg, 1 = pos\n",
    "\n",
    "print(dataset[\"train\"][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input ids: torch.Size([16, 512])\n",
      "labels: torch.Size([16, 1])\n",
      "loss: tensor(0.6649, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Testing if my model works well with this dataset\n",
    "# Example of how to iterate through the dataloader\n",
    "for batch in dataloader_train:\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"label\"].unsqueeze(1).float()\n",
    "\n",
    "    print('input ids:', input_ids.shape)\n",
    "    print('labels:', labels.shape)\n",
    "\n",
    "    print('loss:', model(input_ids, labels)[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS DOES NOT WORK\n",
    "# # Testing if my model works well with this dataset\n",
    "# # Example of how to iterate through the dataloader\n",
    "# count = 0\n",
    "# for idx, targets in dataloader_train:\n",
    "#     idx = idx.to(device)\n",
    "#     targets = targets.to(device)\n",
    "#     print('idx:', idx.shape)\n",
    "#     print('targets:', targets.shape)\n",
    "\n",
    "#     print(model(idx, targets)[1])\n",
    "\n",
    "#     count += 1\n",
    "\n",
    "#     if count > 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 512])\n"
     ]
    }
   ],
   "source": [
    "input_ids = input_ids.to(device)\n",
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5038],\n",
       "         [0.5094],\n",
       "         [0.3700],\n",
       "         [0.5310],\n",
       "         [0.5326],\n",
       "         [0.4179],\n",
       "         [0.4615],\n",
       "         [0.4602],\n",
       "         [0.4958],\n",
       "         [0.5428],\n",
       "         [0.4960],\n",
       "         [0.4819],\n",
       "         [0.4572],\n",
       "         [0.4408],\n",
       "         [0.5642],\n",
       "         [0.5519],\n",
       "         [0.3988],\n",
       "         [0.4865],\n",
       "         [0.4255],\n",
       "         [0.4941],\n",
       "         [0.5305],\n",
       "         [0.4195],\n",
       "         [0.4113],\n",
       "         [0.4868],\n",
       "         [0.3796],\n",
       "         [0.4663],\n",
       "         [0.4657],\n",
       "         [0.5384],\n",
       "         [0.5043],\n",
       "         [0.4749],\n",
       "         [0.5132],\n",
       "         [0.5581],\n",
       "         [0.5017],\n",
       "         [0.4009],\n",
       "         [0.3565],\n",
       "         [0.5652],\n",
       "         [0.4897],\n",
       "         [0.4586],\n",
       "         [0.4856],\n",
       "         [0.4833],\n",
       "         [0.4004],\n",
       "         [0.5293],\n",
       "         [0.5752],\n",
       "         [0.5063],\n",
       "         [0.4934],\n",
       "         [0.4624],\n",
       "         [0.4528],\n",
       "         [0.4393],\n",
       "         [0.4138],\n",
       "         [0.5384],\n",
       "         [0.4643],\n",
       "         [0.5302],\n",
       "         [0.5006],\n",
       "         [0.4823],\n",
       "         [0.5817],\n",
       "         [0.5391],\n",
       "         [0.4701],\n",
       "         [0.5548],\n",
       "         [0.4832],\n",
       "         [0.3523],\n",
       "         [0.5571],\n",
       "         [0.5109],\n",
       "         [0.5721],\n",
       "         [0.5221]], grad_fn=<SigmoidBackward0>),\n",
       " None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training:   0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training: 100%|██████████| 1563/1563 [03:06<00:00,  8.39it/s, Train Loss=0.694]\n",
      "Epoch 1/10, Validation: 100%|██████████| 1563/1563 [00:48<00:00, 31.94it/s, Test Loss=0.69] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6943, Test Loss: 0.6900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Training: 100%|██████████| 1563/1563 [02:54<00:00,  8.98it/s, Train Loss=0.667]\n",
      "Epoch 2/10, Validation: 100%|██████████| 1563/1563 [00:44<00:00, 35.26it/s, Test Loss=0.675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 0.6671, Test Loss: 0.6745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Training: 100%|██████████| 1563/1563 [02:35<00:00, 10.03it/s, Train Loss=0.554]\n",
      "Epoch 3/10, Validation: 100%|██████████| 1563/1563 [00:45<00:00, 34.21it/s, Test Loss=0.631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 0.5541, Test Loss: 0.6306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Training: 100%|██████████| 1563/1563 [02:46<00:00,  9.38it/s, Train Loss=0.409]\n",
      "Epoch 4/10, Validation: 100%|██████████| 1563/1563 [00:49<00:00, 31.77it/s, Test Loss=0.668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 0.4095, Test Loss: 0.6678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Training: 100%|██████████| 1563/1563 [03:15<00:00,  7.99it/s, Train Loss=0.286]\n",
      "Epoch 5/10, Validation: 100%|██████████| 1563/1563 [00:54<00:00, 28.75it/s, Test Loss=0.712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 0.2862, Test Loss: 0.7120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Training: 100%|██████████| 1563/1563 [03:09<00:00,  8.23it/s, Train Loss=0.183]\n",
      "Epoch 6/10, Validation: 100%|██████████| 1563/1563 [00:57<00:00, 27.31it/s, Test Loss=0.811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 0.1828, Test Loss: 0.8111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Training:  50%|████▉     | 781/1563 [01:35<01:32,  8.45it/s, Train Loss=0.105]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Training:  50%|█████     | 782/1563 [01:35<01:36,  8.08it/s, Train Loss=0.105]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, test_loader, optimizer, loss_function, num_epochs=10, device='cuda'):\n",
    "    model.to(device)  # Move the model to the specified device (GPU or CPU)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        # Use tqdm for progress bar\n",
    "        train_loader = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}, Training')\n",
    "\n",
    "        for batch in train_loader:\n",
    "            try:\n",
    "                idx = batch[\"input_ids\"]\n",
    "                targets = batch[\"label\"].unsqueeze(1).float()\n",
    "                idx, targets = idx.to(device), targets.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                results, loss = model(idx)\n",
    "                loss = loss_function(results, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item() * len(idx)\n",
    "                total_samples += len(idx)\n",
    "\n",
    "                # Update tqdm progress bar\n",
    "                train_loader.set_postfix({'Train Loss': total_loss / total_samples})\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        average_train_loss = total_loss / total_samples\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            total_samples = 0\n",
    "\n",
    "            # Use tqdm for progress bar\n",
    "            test_loader = tqdm(test_loader, desc=f'Epoch {epoch + 1}/{num_epochs}, Validation')\n",
    "\n",
    "            for batch in test_loader:\n",
    "                try:\n",
    "                    idx = batch[\"input_ids\"]\n",
    "                    targets = batch[\"label\"].unsqueeze(1).float()\n",
    "                    idx, targets = idx.to(device), targets.to(device)\n",
    "\n",
    "                    results, loss = model(idx)\n",
    "                    loss = loss_function(results, targets)\n",
    "\n",
    "                    total_loss += loss.item() * len(idx)\n",
    "                    total_samples += len(idx)\n",
    "\n",
    "                    # Update tqdm progress bar\n",
    "                    test_loader.set_postfix({'Test Loss': total_loss / total_samples})\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        average_test_loss = total_loss / total_samples\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {average_train_loss:.4f}, Test Loss: {average_test_loss:.4f}')\n",
    "\n",
    "    print('Training complete!')\n",
    "\n",
    "model = Quickformer()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "train_model(model, dataloader_train, dataloader_test, optimizer, loss_function, num_epochs=10, device=device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SyedHamza",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
