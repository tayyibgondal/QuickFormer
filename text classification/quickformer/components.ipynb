{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 512 # max number of words going into the model?\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "learning_rate = 0.5e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "n_embd = 576  # It's square root should be divisible by n_head\n",
    "sqrt_d = torch.sqrt(torch.tensor(n_embd)).int().item()\n",
    "n_head = 4\n",
    "\n",
    "n_layer = 6\n",
    "dropout = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feebler(nn.Module):\n",
    "    ''' \n",
    "    T: Number of words going into the model\n",
    "    C: Embedding dimension\n",
    "    B: Batch size\n",
    "    \n",
    "    input: B, T, C\n",
    "    output: B, T, sqrt(C)\n",
    "    '''\n",
    "    def __init__(self, sqrt_d):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(\n",
    "            torch.randn(sqrt_d, sqrt_d, block_size)\n",
    "        )\n",
    "        self.sqrt_d = sqrt_d\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Data is of shape (b, n, d)\n",
    "        data_reshaped = data.view(batch_size, n_embd, block_size)  # set up data for feebler\n",
    "        data_reshaped = data.view(batch_size, self.sqrt_d, self.sqrt_d, block_size)  # reshape incoming data\n",
    "        product = data_reshaped * self.weights  # multiply data with weights\n",
    "        # perform columnwise sum inside each window\n",
    "        updated_product = torch.sum(product, dim=2, keepdim=False)  # finally we have converted from dxn to sqrt(d)xn\n",
    "        return updated_product.view(batch_size, block_size, self.sqrt_d)\n",
    "    \n",
    "\n",
    "class Booster(nn.Module):\n",
    "    ''' \n",
    "    input: B, T, sqrt(C)\n",
    "    output: B, T, C\n",
    "    '''\n",
    "    def __init__(self, sqrt_d):\n",
    "        super(Booster, self).__init__()\n",
    "        self.weights = nn.Parameter(\n",
    "            torch.randn(sqrt_d, sqrt_d, block_size)\n",
    "        )\n",
    "        self.sqrt_d = sqrt_d\n",
    "\n",
    "    def forward(self, attention_output):\n",
    "        # attention_output is of shape (batch, n, sqrt_d)\n",
    "        # set up data shape for the booster\n",
    "        attention_output = attention_output.view(batch_size, self.sqrt_d, block_size)\n",
    "        attention_output_reshaped = attention_output.view(batch_size, 1, -1) # flatten all rows into one row\n",
    "        attention_output_reshaped = attention_output_reshaped.repeat(1, self.sqrt_d, 1)  # repeat each row sqrt_d times\n",
    "        attention_output_reshaped = attention_output_reshaped.view(batch_size, self.sqrt_d, self.sqrt_d, block_size)\n",
    "        # multiply\n",
    "        revived_output = self.weights * attention_output_reshaped\n",
    "        revived_output = revived_output.view(-1, block_size)\n",
    "        return revived_output.view(batch_size, block_size, n_embd)\n",
    "\n",
    "class QuickHead(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(sqrt_d, head_size, bias=False)\n",
    "        self.query = nn.Linear(sqrt_d, head_size, bias=False)\n",
    "        self.value = nn.Linear(sqrt_d, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape (batch_size, n, sqrt_d)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "\n",
    "        collective_k = k.sum(1, keepdim=True)\n",
    "        # Broadcast explicitly\n",
    "        collective_k_bc = collective_k.repeat(1, block_size, 1)\n",
    "        # q multiply k\n",
    "        qk = q * collective_k_bc\n",
    "        attention_weights = torch.softmax(qk, dim=1)\n",
    "        collective_v = v.sum(dim=1, keepdim=True)\n",
    "        collective_v_bc = collective_v.repeat(1, block_size, 1)\n",
    "        output = collective_v_bc * attention_weights\n",
    "        return output\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([QuickHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(sqrt_d, sqrt_d) # global variable sqrt_d\n",
    "        self.dropout = nn.Dropout(dropout)  # global variable dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, sqrt_d):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(sqrt_d, 4 * sqrt_d),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * sqrt_d, sqrt_d),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = sqrt_d // n_head\n",
    "        self.feebler = Feebler(sqrt_d)\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(sqrt_d)\n",
    "        self.ln1 = nn.LayerNorm(sqrt_d)\n",
    "        self.ln2 = nn.LayerNorm(sqrt_d)\n",
    "        self.booster = Booster(sqrt_d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feebler(x)\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        x = self.booster(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512, 576])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = Block(n_embd, n_head)\n",
    "b(torch.rand(batch_size, block_size, n_embd)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100000\n",
    "\n",
    "# super simple quickformer model\n",
    "class Quickformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, 1)\n",
    "        self.logits_maker = nn.Linear(block_size, 1)\n",
    "        self.classifier = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        x = self.lm_head(x) # (B,T,1)\n",
    "        x = x.squeeze(2) # (B,T)\n",
    "        logits = self.logits_maker(x) # (B,1)\n",
    "        results = self.classifier(logits) # (B, 1)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.binary_cross_entropy(results, targets)\n",
    "\n",
    "        return results, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = Quickformer()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.rand(batch_size, block_size).long().to(device)\n",
    "print(inp.shape)\n",
    "l, ll = model(inp)\n",
    "l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Load IMDb dataset from Hugging Face\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Use a pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define a custom PyTorch Dataset\n",
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=block_size):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        text = item[\"text\"]\n",
    "        label = item[\"label\"]\n",
    "\n",
    "        # Tokenize and encode the text\n",
    "        inputs = self.tokenizer(\n",
    "            text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"].squeeze()\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze()\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"label\": torch.tensor(label)}\n",
    "\n",
    "# Create Train dataset (pytorch)\n",
    "imdb_dataset_train = IMDbDataset(dataset[\"train\"], tokenizer)\n",
    "# Create Test dataset (pytorch)\n",
    "imdb_dataset_test = IMDbDataset(dataset[\"test\"], tokenizer)\n",
    "\n",
    "# Create PyTorch DataLoader for train set\n",
    "dataloader_train = DataLoader(imdb_dataset_train, batch_size=batch_size, shuffle=True)\n",
    "# Create PyTorch DataLoader for test set\n",
    "dataloader_test = DataLoader(imdb_dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetInfo(description='', citation='', homepage='', license='', features={'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='imdb', config_name='plain_text', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=33435948, num_examples=25000, shard_lengths=None, dataset_name='imdb'), 'test': SplitInfo(name='test', num_bytes=32653810, num_examples=25000, shard_lengths=None, dataset_name='imdb'), 'unsupervised': SplitInfo(name='unsupervised', num_bytes=67113044, num_examples=50000, shard_lengths=None, dataset_name='imdb')}, download_checksums={'hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/train-00000-of-00001.parquet': {'num_bytes': 20979968, 'checksum': None}, 'hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/test-00000-of-00001.parquet': {'num_bytes': 20470363, 'checksum': None}, 'hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/unsupervised-00000-of-00001.parquet': {'num_bytes': 41996509, 'checksum': None}}, download_size=83446840, post_processing_size=None, dataset_size=133202802, size_in_bytes=216649642)\n",
      "dict_keys(['text', 'label'])\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dataset info to see the details\n",
    "print(dataset[\"train\"].info)  # shows 0 = neg, 1 = pos\n",
    "\n",
    "print(dataset[\"train\"][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input ids: torch.Size([16, 512])\n",
      "labels: torch.Size([16, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/text classification/quickformer/components.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/text%20classification/quickformer/components.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minput ids:\u001b[39m\u001b[39m'\u001b[39m, input_ids\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/text%20classification/quickformer/components.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mlabels:\u001b[39m\u001b[39m'\u001b[39m, labels\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/text%20classification/quickformer/components.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mloss:\u001b[39m\u001b[39m'\u001b[39m, model(input_ids, labels)[\u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/text%20classification/quickformer/components.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/SyedHamza/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/SyedHamza/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/tuk/Desktop/QuickFormer_Research_Project/text classification/quickformer/components.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/text%20classification/quickformer/components.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m B, T \u001b[39m=\u001b[39m idx\u001b[39m.\u001b[39mshape\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/text%20classification/quickformer/components.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# idx and targets are both (B,T) tensor of integers\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/text%20classification/quickformer/components.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m tok_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_embedding_table(idx) \u001b[39m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/text%20classification/quickformer/components.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m pos_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_table(torch\u001b[39m.\u001b[39marange(T, device\u001b[39m=\u001b[39mdevice)) \u001b[39m# (T,C)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tuk/Desktop/QuickFormer_Research_Project/text%20classification/quickformer/components.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m x \u001b[39m=\u001b[39m tok_emb \u001b[39m+\u001b[39m pos_emb \u001b[39m# (B,T,C)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/SyedHamza/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/SyedHamza/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/SyedHamza/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_idx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_type, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_grad_by_freq, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparse)\n",
      "File \u001b[0;32m~/anaconda3/envs/SyedHamza/lib/python3.11/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39membedding(weight, \u001b[39minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "# Testing if my model works well with this dataset\n",
    "# Example of how to iterate through the dataloader\n",
    "for batch in dataloader_train:\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"label\"].unsqueeze(1).float()\n",
    "\n",
    "    print('input ids:', input_ids.shape)\n",
    "    print('labels:', labels.shape)\n",
    "\n",
    "    print('loss:', model(input_ids, labels)[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS DOES NOT WORK\n",
    "# # Testing if my model works well with this dataset\n",
    "# # Example of how to iterate through the dataloader\n",
    "# count = 0\n",
    "# for idx, targets in dataloader_train:\n",
    "#     idx = idx.to(device)\n",
    "#     targets = targets.to(device)\n",
    "#     print('idx:', idx.shape)\n",
    "#     print('targets:', targets.shape)\n",
    "\n",
    "#     print(model(idx, targets)[1])\n",
    "\n",
    "#     count += 1\n",
    "\n",
    "#     if count > 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512])\n"
     ]
    }
   ],
   "source": [
    "input_ids = input_ids.to(device)\n",
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4877],\n",
       "         [0.4690],\n",
       "         [0.4618],\n",
       "         [0.4999],\n",
       "         [0.4327],\n",
       "         [0.3809],\n",
       "         [0.3770],\n",
       "         [0.4431],\n",
       "         [0.4776],\n",
       "         [0.4195],\n",
       "         [0.4573],\n",
       "         [0.4276],\n",
       "         [0.4817],\n",
       "         [0.4940],\n",
       "         [0.3807],\n",
       "         [0.4110]], grad_fn=<SigmoidBackward0>),\n",
       " None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Training: 100%|██████████| 1563/1563 [01:08<00:00, 22.95it/s, Train Loss=0.696]\n",
      "Epoch 1/50, Validation: 100%|██████████| 1563/1563 [00:33<00:00, 47.30it/s, Test Loss=0.699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.6959, Test Loss: 0.6991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Training: 100%|██████████| 1563/1563 [01:06<00:00, 23.51it/s, Train Loss=0.66] \n",
      "Epoch 2/50, Validation: 100%|██████████| 1563/1563 [00:33<00:00, 46.53it/s, Test Loss=0.674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Train Loss: 0.6604, Test Loss: 0.6738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Training: 100%|██████████| 1563/1563 [01:05<00:00, 23.70it/s, Train Loss=0.506]\n",
      "Epoch 3/50, Validation: 100%|██████████| 1563/1563 [00:33<00:00, 47.28it/s, Test Loss=0.732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Train Loss: 0.5060, Test Loss: 0.7325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Training: 100%|██████████| 1563/1563 [01:05<00:00, 23.98it/s, Train Loss=0.309]\n",
      "Epoch 4/50, Validation: 100%|██████████| 1563/1563 [00:32<00:00, 47.55it/s, Test Loss=0.927]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Train Loss: 0.3089, Test Loss: 0.9269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Training: 100%|██████████| 1563/1563 [01:05<00:00, 23.98it/s, Train Loss=0.158]\n",
      "Epoch 5/50, Validation: 100%|██████████| 1563/1563 [00:33<00:00, 46.21it/s, Test Loss=1.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Train Loss: 0.1578, Test Loss: 1.1839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Training: 100%|██████████| 1563/1563 [01:06<00:00, 23.66it/s, Train Loss=0.0757]\n",
      "Epoch 6/50, Validation: 100%|██████████| 1563/1563 [00:33<00:00, 46.51it/s, Test Loss=1.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Train Loss: 0.0757, Test Loss: 1.8111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Training: 100%|██████████| 1563/1563 [01:05<00:00, 23.70it/s, Train Loss=0.0385]\n",
      "Epoch 7/50, Validation: 100%|██████████| 1563/1563 [00:33<00:00, 46.99it/s, Test Loss=2.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Train Loss: 0.0385, Test Loss: 2.1470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Training: 100%|██████████| 1563/1563 [01:07<00:00, 23.23it/s, Train Loss=0.0333]\n",
      "Epoch 8/50, Validation: 100%|██████████| 1563/1563 [00:34<00:00, 45.63it/s, Test Loss=2.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Train Loss: 0.0333, Test Loss: 2.6487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Training: 100%|██████████| 1563/1563 [01:06<00:00, 23.50it/s, Train Loss=0.0197]\n",
      "Epoch 9/50, Validation: 100%|██████████| 1563/1563 [00:35<00:00, 43.79it/s, Test Loss=2.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Train Loss: 0.0197, Test Loss: 2.8400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Training: 100%|██████████| 1563/1563 [01:07<00:00, 23.29it/s, Train Loss=0.0156]\n",
      "Epoch 10/50, Validation: 100%|██████████| 1563/1563 [00:32<00:00, 47.62it/s, Test Loss=4.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Train Loss: 0.0156, Test Loss: 4.2452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Training: 100%|██████████| 1563/1563 [01:07<00:00, 23.24it/s, Train Loss=0.0182]\n",
      "Epoch 11/50, Validation: 100%|██████████| 1563/1563 [00:33<00:00, 46.07it/s, Test Loss=4.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Train Loss: 0.0182, Test Loss: 4.0159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Training: 100%|██████████| 1563/1563 [01:08<00:00, 22.91it/s, Train Loss=0.0182] \n",
      "Epoch 12/50, Validation: 100%|██████████| 1563/1563 [00:33<00:00, 46.43it/s, Test Loss=6.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Train Loss: 0.0182, Test Loss: 6.2814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Training: 100%|██████████| 1563/1563 [01:07<00:00, 23.15it/s, Train Loss=0.0142]\n",
      "Epoch 13/50, Validation: 100%|██████████| 1563/1563 [00:34<00:00, 45.86it/s, Test Loss=3.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Train Loss: 0.0142, Test Loss: 3.4420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, Training: 100%|██████████| 1563/1563 [01:07<00:00, 23.01it/s, Train Loss=0.0123] \n",
      "Epoch 14/50, Validation: 100%|██████████| 1563/1563 [00:33<00:00, 46.54it/s, Test Loss=4.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, Train Loss: 0.0123, Test Loss: 4.9190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, Training: 100%|██████████| 1563/1563 [01:07<00:00, 23.33it/s, Train Loss=0.00606]\n",
      "Epoch 15/50, Validation: 100%|██████████| 1563/1563 [00:34<00:00, 45.37it/s, Test Loss=4.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, Train Loss: 0.0061, Test Loss: 4.3888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, Training: 100%|██████████| 1563/1563 [01:08<00:00, 22.94it/s, Train Loss=0.0101] \n",
      "Epoch 16/50, Validation: 100%|██████████| 1563/1563 [00:34<00:00, 44.88it/s, Test Loss=5.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, Train Loss: 0.0101, Test Loss: 5.1439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, Training: 100%|██████████| 1563/1563 [01:09<00:00, 22.56it/s, Train Loss=0.01]  \n",
      "Epoch 17/50, Validation: 100%|██████████| 1563/1563 [00:34<00:00, 44.98it/s, Test Loss=5.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, Train Loss: 0.0100, Test Loss: 5.0272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, Training: 100%|██████████| 1563/1563 [01:07<00:00, 23.23it/s, Train Loss=0.0101] \n",
      "Epoch 18/50, Validation: 100%|██████████| 1563/1563 [00:34<00:00, 45.00it/s, Test Loss=4.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, Train Loss: 0.0101, Test Loss: 4.7051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, Training: 100%|██████████| 1563/1563 [01:08<00:00, 22.80it/s, Train Loss=0.00268]\n",
      "Epoch 19/50, Validation: 100%|██████████| 1563/1563 [00:34<00:00, 45.25it/s, Test Loss=4.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, Train Loss: 0.0027, Test Loss: 4.4875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Training: 100%|██████████| 1563/1563 [01:08<00:00, 22.95it/s, Train Loss=0.0123] \n",
      "Epoch 20/50, Validation: 100%|██████████| 1563/1563 [00:34<00:00, 45.08it/s, Test Loss=4.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Train Loss: 0.0123, Test Loss: 4.5786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, Training: 100%|██████████| 1563/1563 [01:06<00:00, 23.51it/s, Train Loss=0.00835]\n",
      "Epoch 21/50, Validation: 100%|██████████| 1563/1563 [00:34<00:00, 45.61it/s, Test Loss=4.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, Train Loss: 0.0084, Test Loss: 4.1698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, Training: 100%|██████████| 1563/1563 [01:09<00:00, 22.56it/s, Train Loss=0.00472]\n",
      "Epoch 22/50, Validation: 100%|██████████| 1563/1563 [00:33<00:00, 47.14it/s, Test Loss=5.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, Train Loss: 0.0047, Test Loss: 5.7566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, Training: 100%|██████████| 1563/1563 [01:06<00:00, 23.63it/s, Train Loss=0.00902]\n",
      "Epoch 23/50, Validation: 100%|██████████| 1563/1563 [00:33<00:00, 46.70it/s, Test Loss=4.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, Train Loss: 0.0090, Test Loss: 4.8366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, Training: 100%|██████████| 1563/1563 [01:05<00:00, 23.68it/s, Train Loss=0.00851]\n",
      "Epoch 24/50, Validation: 100%|██████████| 1563/1563 [00:33<00:00, 46.49it/s, Test Loss=5.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, Train Loss: 0.0085, Test Loss: 5.1932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, Training: 100%|██████████| 1563/1563 [01:06<00:00, 23.54it/s, Train Loss=0.0046] \n",
      "Epoch 25/50, Validation: 100%|██████████| 1563/1563 [00:34<00:00, 45.72it/s, Test Loss=5.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, Train Loss: 0.0046, Test Loss: 5.8517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, Training: 100%|██████████| 1563/1563 [01:06<00:00, 23.63it/s, Train Loss=0.00857]\n",
      "Epoch 26/50, Validation: 100%|██████████| 1563/1563 [00:33<00:00, 46.94it/s, Test Loss=4.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, Train Loss: 0.0086, Test Loss: 4.4989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, Training: 100%|██████████| 1563/1563 [01:07<00:00, 23.26it/s, Train Loss=0.000234]\n",
      "Epoch 27/50, Validation: 100%|██████████| 1563/1563 [00:33<00:00, 46.42it/s, Test Loss=4.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, Train Loss: 0.0002, Test Loss: 4.8526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, Training: 100%|██████████| 1563/1563 [01:06<00:00, 23.50it/s, Train Loss=1.25e-5]\n",
      "Epoch 28/50, Validation: 100%|██████████| 1563/1563 [00:33<00:00, 47.02it/s, Test Loss=4.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, Train Loss: 0.0000, Test Loss: 4.9817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, Training: 100%|██████████| 1563/1563 [01:07<00:00, 23.30it/s, Train Loss=6.63e-6]\n",
      "Epoch 29/50, Validation: 100%|██████████| 1563/1563 [00:33<00:00, 46.72it/s, Test Loss=5.09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, Train Loss: 0.0000, Test Loss: 5.0899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Training: 100%|██████████| 1563/1563 [01:06<00:00, 23.60it/s, Train Loss=3.3e-6] \n",
      "Epoch 30/50, Validation: 100%|██████████| 1563/1563 [00:34<00:00, 45.15it/s, Test Loss=5.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Train Loss: 0.0000, Test Loss: 5.2729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, Training: 100%|██████████| 1563/1563 [01:10<00:00, 22.31it/s, Train Loss=1.69e-6]\n",
      "Epoch 31/50, Validation: 100%|██████████| 1563/1563 [00:33<00:00, 47.19it/s, Test Loss=5.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, Train Loss: 0.0000, Test Loss: 5.5273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, Training: 100%|██████████| 1563/1563 [01:06<00:00, 23.52it/s, Train Loss=8.81e-7]\n",
      "Epoch 32/50, Validation: 100%|██████████| 1563/1563 [00:33<00:00, 46.01it/s, Test Loss=5.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, Train Loss: 0.0000, Test Loss: 5.9073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50, Training: 100%|██████████| 1563/1563 [01:06<00:00, 23.36it/s, Train Loss=4.06e-7]\n",
      "Epoch 33/50, Validation: 100%|██████████| 1563/1563 [00:33<00:00, 46.10it/s, Test Loss=6.42]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50, Train Loss: 0.0000, Test Loss: 6.4163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50, Training: 100%|██████████| 1563/1563 [01:06<00:00, 23.40it/s, Train Loss=1.77e-7]\n",
      "Epoch 34/50, Validation: 100%|██████████| 1563/1563 [00:34<00:00, 45.52it/s, Test Loss=6.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50, Train Loss: 0.0000, Test Loss: 6.6737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50, Training: 100%|██████████| 1563/1563 [01:06<00:00, 23.40it/s, Train Loss=8.55e-8]\n",
      "Epoch 35/50, Validation: 100%|██████████| 1563/1563 [00:35<00:00, 44.09it/s, Test Loss=6.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50, Train Loss: 0.0000, Test Loss: 6.7951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50, Training: 100%|██████████| 1563/1563 [01:09<00:00, 22.46it/s, Train Loss=4.32e-8]\n",
      "Epoch 36/50, Validation: 100%|██████████| 1563/1563 [00:35<00:00, 44.62it/s, Test Loss=7.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50, Train Loss: 0.0000, Test Loss: 7.1483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50, Training: 100%|██████████| 1563/1563 [01:09<00:00, 22.65it/s, Train Loss=1.94e-8]\n",
      "Epoch 37/50, Validation: 100%|██████████| 1563/1563 [00:33<00:00, 46.05it/s, Test Loss=7.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50, Train Loss: 0.0000, Test Loss: 7.4460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50, Training: 100%|██████████| 1563/1563 [01:05<00:00, 23.69it/s, Train Loss=9.77e-9]\n",
      "Epoch 38/50, Validation: 100%|██████████| 1563/1563 [00:34<00:00, 45.93it/s, Test Loss=7.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50, Train Loss: 0.0000, Test Loss: 7.7878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50, Training: 100%|██████████| 1563/1563 [01:08<00:00, 22.96it/s, Train Loss=4.65e-9]\n",
      "Epoch 39/50, Validation: 100%|██████████| 1563/1563 [00:35<00:00, 44.37it/s, Test Loss=7.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50, Train Loss: 0.0000, Test Loss: 7.9777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Training: 100%|██████████| 1563/1563 [01:09<00:00, 22.51it/s, Train Loss=2.3e-9] \n",
      "Epoch 40/50, Validation: 100%|██████████| 1563/1563 [00:34<00:00, 45.32it/s, Test Loss=8.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Train Loss: 0.0000, Test Loss: 8.3861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50, Training: 100%|██████████| 1563/1563 [01:08<00:00, 22.79it/s, Train Loss=1.21e-9]\n",
      "Epoch 41/50, Validation: 100%|██████████| 1563/1563 [00:34<00:00, 45.97it/s, Test Loss=8.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50, Train Loss: 0.0000, Test Loss: 8.3935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50, Training: 100%|██████████| 1563/1563 [01:07<00:00, 23.14it/s, Train Loss=4.72e-10]\n",
      "Epoch 42/50, Validation: 100%|██████████| 1563/1563 [00:33<00:00, 46.63it/s, Test Loss=8.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50, Train Loss: 0.0000, Test Loss: 8.6523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50, Training: 100%|██████████| 1563/1563 [01:07<00:00, 23.15it/s, Train Loss=2.26e-10]\n",
      "Epoch 43/50, Validation: 100%|██████████| 1563/1563 [00:34<00:00, 44.66it/s, Test Loss=8.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50, Train Loss: 0.0000, Test Loss: 8.6607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50, Training: 100%|██████████| 1563/1563 [01:08<00:00, 22.73it/s, Train Loss=1.28e-10]\n",
      "Epoch 44/50, Validation: 100%|██████████| 1563/1563 [00:34<00:00, 45.56it/s, Test Loss=8.9] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50, Train Loss: 0.0000, Test Loss: 8.8967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50, Training: 100%|██████████| 1563/1563 [01:07<00:00, 23.01it/s, Train Loss=9.39e-11]\n",
      "Epoch 45/50, Validation: 100%|██████████| 1563/1563 [00:34<00:00, 45.32it/s, Test Loss=9.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50, Train Loss: 0.0000, Test Loss: 9.2325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50, Training: 100%|██████████| 1563/1563 [01:09<00:00, 22.58it/s, Train Loss=2.05e-10]\n",
      "Epoch 46/50, Validation: 100%|██████████| 1563/1563 [00:33<00:00, 46.46it/s, Test Loss=10.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50, Train Loss: 0.0000, Test Loss: 10.1056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50, Training: 100%|██████████| 1563/1563 [01:06<00:00, 23.44it/s, Train Loss=6.63e-11]\n",
      "Epoch 47/50, Validation: 100%|██████████| 1563/1563 [00:33<00:00, 46.14it/s, Test Loss=10.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50, Train Loss: 0.0000, Test Loss: 10.3239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50, Training: 100%|██████████| 1563/1563 [01:07<00:00, 23.19it/s, Train Loss=1.64e-11]\n",
      "Epoch 48/50, Validation: 100%|██████████| 1563/1563 [00:34<00:00, 45.44it/s, Test Loss=10.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50, Train Loss: 0.0000, Test Loss: 10.1226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50, Training: 100%|██████████| 1563/1563 [01:06<00:00, 23.43it/s, Train Loss=4.34e-12]\n",
      "Epoch 49/50, Validation: 100%|██████████| 1563/1563 [00:33<00:00, 46.24it/s, Test Loss=9.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50, Train Loss: 0.0000, Test Loss: 9.9912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Training: 100%|██████████| 1563/1563 [01:07<00:00, 23.18it/s, Train Loss=2.27e-12]\n",
      "Epoch 50/50, Validation: 100%|██████████| 1563/1563 [00:34<00:00, 45.54it/s, Test Loss=9.94]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.0000, Test Loss: 9.9386\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, test_loader, optimizer, loss_function, num_epochs=10, device='cuda'):\n",
    "    model.to(device)  # Move the model to the specified device (GPU or CPU)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        # Use tqdm for progress bar\n",
    "        train_loader = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}, Training')\n",
    "\n",
    "        for batch in train_loader:\n",
    "            try:\n",
    "                idx = batch[\"input_ids\"]\n",
    "                targets = batch[\"label\"].unsqueeze(1).float()\n",
    "                idx, targets = idx.to(device), targets.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                results, loss = model(idx)\n",
    "                loss = loss_function(results, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item() * len(idx)\n",
    "                total_samples += len(idx)\n",
    "\n",
    "                # Update tqdm progress bar\n",
    "                train_loader.set_postfix({'Train Loss': total_loss / total_samples})\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        average_train_loss = total_loss / total_samples\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            total_samples = 0\n",
    "\n",
    "            # Use tqdm for progress bar\n",
    "            test_loader = tqdm(test_loader, desc=f'Epoch {epoch + 1}/{num_epochs}, Validation')\n",
    "\n",
    "            for batch in test_loader:\n",
    "                try:\n",
    "                    idx = batch[\"input_ids\"]\n",
    "                    targets = batch[\"label\"].unsqueeze(1).float()\n",
    "                    idx, targets = idx.to(device), targets.to(device)\n",
    "\n",
    "                    results, loss = model(idx)\n",
    "                    loss = loss_function(results, targets)\n",
    "\n",
    "                    total_loss += loss.item() * len(idx)\n",
    "                    total_samples += len(idx)\n",
    "\n",
    "                    # Update tqdm progress bar\n",
    "                    test_loader.set_postfix({'Test Loss': total_loss / total_samples})\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        average_test_loss = total_loss / total_samples\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {average_train_loss:.4f}, Test Loss: {average_test_loss:.4f}')\n",
    "\n",
    "    print('Training complete!')\n",
    "\n",
    "model = Quickformer()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "train_model(model, dataloader_train, dataloader_test, optimizer, loss_function, num_epochs=epochs, device=device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SyedHamza",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
