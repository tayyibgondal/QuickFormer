{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 512 # max number of words going into the model?\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "learning_rate = 0.5e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "n_embd = 576  # It's square root should be divisible by n_head\n",
    "sqrt_d = torch.sqrt(torch.tensor(n_embd)).int().item()\n",
    "n_head = 4\n",
    "\n",
    "n_layer = 6\n",
    "dropout = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feebler(nn.Module):\n",
    "    ''' \n",
    "    T: Number of words going into the model\n",
    "    C: Embedding dimension\n",
    "    B: Batch size\n",
    "    \n",
    "    input: B, T, C\n",
    "    output: B, T, sqrt(C)\n",
    "    '''\n",
    "    def __init__(self, sqrt_d):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(\n",
    "            torch.randn(sqrt_d, sqrt_d, block_size)\n",
    "        )\n",
    "        self.sqrt_d = sqrt_d\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Data is of shape (b, n, d)\n",
    "        data_reshaped = data.view(batch_size, n_embd, block_size)  # set up data for feebler\n",
    "        data_reshaped = data.view(batch_size, self.sqrt_d, self.sqrt_d, block_size)  # reshape incoming data\n",
    "        product = data_reshaped * self.weights  # multiply data with weights\n",
    "        # perform columnwise sum inside each window\n",
    "        updated_product = torch.sum(product, dim=2, keepdim=False)  # finally we have converted from dxn to sqrt(d)xn\n",
    "        return updated_product.view(batch_size, block_size, self.sqrt_d)\n",
    "    \n",
    "\n",
    "class Booster(nn.Module):\n",
    "    ''' \n",
    "    input: B, T, sqrt(C)\n",
    "    output: B, T, C\n",
    "    '''\n",
    "    def __init__(self, sqrt_d):\n",
    "        super(Booster, self).__init__()\n",
    "        self.weights = nn.Parameter(\n",
    "            torch.randn(sqrt_d, sqrt_d, block_size)\n",
    "        )\n",
    "        self.sqrt_d = sqrt_d\n",
    "\n",
    "    def forward(self, attention_output):\n",
    "        # attention_output is of shape (batch, n, sqrt_d)\n",
    "        # set up data shape for the booster\n",
    "        attention_output = attention_output.view(batch_size, self.sqrt_d, block_size)\n",
    "        attention_output_reshaped = attention_output.view(batch_size, 1, -1) # flatten all rows into one row\n",
    "        attention_output_reshaped = attention_output_reshaped.repeat(1, self.sqrt_d, 1)  # repeat each row sqrt_d times\n",
    "        attention_output_reshaped = attention_output_reshaped.view(batch_size, self.sqrt_d, self.sqrt_d, block_size)\n",
    "        # multiply\n",
    "        revived_output = self.weights * attention_output_reshaped\n",
    "        revived_output = revived_output.view(-1, block_size)\n",
    "        return revived_output.view(batch_size, block_size, n_embd)\n",
    "\n",
    "class QuickHead(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(sqrt_d, head_size, bias=False)\n",
    "        self.query = nn.Linear(sqrt_d, head_size, bias=False)\n",
    "        self.value = nn.Linear(sqrt_d, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape (batch_size, n, sqrt_d)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "\n",
    "        collective_k = k.sum(1, keepdim=True)\n",
    "        # Broadcast explicitly\n",
    "        collective_k_bc = collective_k.repeat(1, block_size, 1)\n",
    "        # q multiply k\n",
    "        qk = q * collective_k_bc\n",
    "        attention_weights = torch.softmax(qk, dim=1)\n",
    "        collective_v = v.sum(dim=1, keepdim=True)\n",
    "        collective_v_bc = collective_v.repeat(1, block_size, 1)\n",
    "        output = collective_v_bc * attention_weights\n",
    "        return output\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([QuickHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(sqrt_d, sqrt_d) # global variable sqrt_d\n",
    "        self.dropout = nn.Dropout(dropout)  # global variable dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, sqrt_d):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(sqrt_d, 4 * sqrt_d),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * sqrt_d, sqrt_d),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = sqrt_d // n_head\n",
    "        self.feebler = Feebler(sqrt_d)\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(sqrt_d)\n",
    "        self.ln1 = nn.LayerNorm(sqrt_d)\n",
    "        self.ln2 = nn.LayerNorm(sqrt_d)\n",
    "        self.booster = Booster(sqrt_d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feebler(x)\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        x = self.booster(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512, 576])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = Block(n_embd, n_head)\n",
    "b(torch.rand(batch_size, block_size, n_embd)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100000\n",
    "\n",
    "# super simple quickformer model\n",
    "class Quickformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, 1)\n",
    "        self.logits_maker = nn.Linear(block_size, 1)\n",
    "        self.classifier = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        x = self.lm_head(x) # (B,T,1)\n",
    "        x = x.squeeze(2) # (B,T)\n",
    "        logits = self.logits_maker(x) # (B,1)\n",
    "        results = self.classifier(logits) # (B, 1)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.binary_cross_entropy(results, targets)\n",
    "\n",
    "        return results, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = Quickformer()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.rand(batch_size, block_size).long().to(device)\n",
    "print(inp.shape)\n",
    "l, ll = model(inp)\n",
    "l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Load IMDb dataset from Hugging Face\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Use a pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define a custom PyTorch Dataset\n",
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=block_size):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        text = item[\"text\"]\n",
    "        label = item[\"label\"]\n",
    "\n",
    "        # Tokenize and encode the text\n",
    "        inputs = self.tokenizer(\n",
    "            text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"].squeeze()\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze()\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"label\": torch.tensor(label)}\n",
    "\n",
    "# Create Train dataset (pytorch)\n",
    "imdb_dataset_train = IMDbDataset(dataset[\"train\"], tokenizer)\n",
    "# Create Test dataset (pytorch)\n",
    "imdb_dataset_test = IMDbDataset(dataset[\"test\"], tokenizer)\n",
    "\n",
    "# Create PyTorch DataLoader for train set\n",
    "dataloader_train = DataLoader(imdb_dataset_train, batch_size=batch_size, shuffle=True)\n",
    "# Create PyTorch DataLoader for test set\n",
    "dataloader_test = DataLoader(imdb_dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetInfo(description='', citation='', homepage='', license='', features={'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='parquet', dataset_name='imdb', config_name='plain_text', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=33435948, num_examples=25000, shard_lengths=None, dataset_name='imdb'), 'test': SplitInfo(name='test', num_bytes=32653810, num_examples=25000, shard_lengths=None, dataset_name='imdb'), 'unsupervised': SplitInfo(name='unsupervised', num_bytes=67113044, num_examples=50000, shard_lengths=None, dataset_name='imdb')}, download_checksums={'hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/train-00000-of-00001.parquet': {'num_bytes': 20979968, 'checksum': None}, 'hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/test-00000-of-00001.parquet': {'num_bytes': 20470363, 'checksum': None}, 'hf://datasets/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/unsupervised-00000-of-00001.parquet': {'num_bytes': 41996509, 'checksum': None}}, download_size=83446840, post_processing_size=None, dataset_size=133202802, size_in_bytes=216649642)\n",
      "dict_keys(['text', 'label'])\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dataset info to see the details\n",
    "print(dataset[\"train\"].info)  # shows 0 = neg, 1 = pos\n",
    "\n",
    "print(dataset[\"train\"][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input ids: torch.Size([16, 512])\n",
      "labels: torch.Size([16, 1])\n",
      "loss: tensor(0.7341, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Testing if my model works well with this dataset\n",
    "# Example of how to iterate through the dataloader\n",
    "for batch in dataloader_train:\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"label\"].unsqueeze(1).float().to(device)\n",
    "\n",
    "    print('input ids:', input_ids.shape)\n",
    "    print('labels:', labels.shape)\n",
    "\n",
    "    print('loss:', model(input_ids, labels)[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS DOES NOT WORK\n",
    "# # Testing if my model works well with this dataset\n",
    "# # Example of how to iterate through the dataloader\n",
    "# count = 0\n",
    "# for idx, targets in dataloader_train:\n",
    "#     idx = idx.to(device)\n",
    "#     targets = targets.to(device)\n",
    "#     print('idx:', idx.shape)\n",
    "#     print('targets:', targets.shape)\n",
    "\n",
    "#     print(model(idx, targets)[1])\n",
    "\n",
    "#     count += 1\n",
    "\n",
    "#     if count > 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512])\n"
     ]
    }
   ],
   "source": [
    "input_ids = input_ids.to(device)\n",
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4906],\n",
       "         [0.5014],\n",
       "         [0.4540],\n",
       "         [0.4883],\n",
       "         [0.5317],\n",
       "         [0.4396],\n",
       "         [0.4997],\n",
       "         [0.4276],\n",
       "         [0.4118],\n",
       "         [0.4704],\n",
       "         [0.4700],\n",
       "         [0.4676],\n",
       "         [0.4620],\n",
       "         [0.4811],\n",
       "         [0.4686],\n",
       "         [0.4996]], device='cuda:0', grad_fn=<SigmoidBackward0>),\n",
       " None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Training: 100%|██████████| 1563/1563 [01:08<00:00, 22.83it/s, Train Loss=0.695]\n",
      "Epoch 1/50, Validation: 100%|██████████| 1563/1563 [00:34<00:00, 45.89it/s, Test Loss=0.69] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.6948, Test Loss: 0.6899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Training:  71%|███████   | 1108/1563 [00:48<00:19, 23.21it/s, Train Loss=0.66] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Training:  73%|███████▎  | 1147/1563 [00:50<00:19, 21.26it/s, Train Loss=0.661]"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, test_loader, optimizer, loss_function, num_epochs=10, device='cuda', save_path='model', save_interval=20):\n",
    "    model.to(device)  # Move the model to the specified device (GPU or CPU)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        # Use tqdm for progress bar\n",
    "        train_loader = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}, Training')\n",
    "\n",
    "        for batch in train_loader:\n",
    "            try:\n",
    "                idx = batch[\"input_ids\"]\n",
    "                targets = batch[\"label\"].unsqueeze(1).float()\n",
    "                idx, targets = idx.to(device), targets.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                results, loss = model(idx)\n",
    "                loss = loss_function(results, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item() * len(idx)\n",
    "                total_samples += len(idx)\n",
    "\n",
    "                # Update tqdm progress bar\n",
    "                train_loader.set_postfix({'Train Loss': total_loss / total_samples})\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        average_train_loss = total_loss / total_samples\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            total_samples = 0\n",
    "\n",
    "            # Use tqdm for progress bar\n",
    "            test_loader = tqdm(test_loader, desc=f'Epoch {epoch + 1}/{num_epochs}, Validation')\n",
    "\n",
    "            for batch in test_loader:\n",
    "                try:\n",
    "                    idx = batch[\"input_ids\"]\n",
    "                    targets = batch[\"label\"].unsqueeze(1).float()\n",
    "                    idx, targets = idx.to(device), targets.to(device)\n",
    "\n",
    "                    results, loss = model(idx)\n",
    "                    loss = loss_function(results, targets)\n",
    "\n",
    "                    total_loss += loss.item() * len(idx)\n",
    "                    total_samples += len(idx)\n",
    "\n",
    "                    # Update tqdm progress bar\n",
    "                    test_loader.set_postfix({'Test Loss': total_loss / total_samples})\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        average_test_loss = total_loss / total_samples\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {average_train_loss:.4f}, Test Loss: {average_test_loss:.4f}')\n",
    "        \n",
    "        # Save model\n",
    "        if (epoch + 1) % save_interval == 0 or epoch + 1 == num_epochs:\n",
    "            torch.save(model.state_dict(), f\"{save_path}_epoch_{epoch + 1}.pth\")\n",
    "            print(f\"Model saved at epoch {epoch + 1}\")\n",
    "\n",
    "    print('Training complete!')\n",
    "\n",
    "model = Quickformer()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "train_model(model, dataloader_train, dataloader_test, optimizer, loss_function, num_epochs=epochs, device=device, save_path='model', save_interval=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Accuracy:  94%|█████████▍| 1475/1563 [00:32<00:01, 46.75it/s, Accuracy=0.5]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Accuracy:  96%|█████████▌| 1495/1563 [00:32<00:01, 41.83it/s, Accuracy=0.5]"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(model, data_loader, device='cuda'):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data_loader = tqdm(data_loader, desc='Calculating Accuracy')\n",
    "        for batch in data_loader:\n",
    "            try:\n",
    "                idx = batch[\"input_ids\"]\n",
    "                targets = batch[\"label\"].unsqueeze(1).float()\n",
    "                idx, targets = idx.to(device), targets.to(device)\n",
    "\n",
    "                predictions, _ = model(idx)\n",
    "                predictions = torch.round(predictions)\n",
    "\n",
    "                correct += (predictions == targets).sum().item()\n",
    "                total += targets.size(0)\n",
    "\n",
    "                data_loader.set_postfix({'Accuracy': correct / total})\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Usage:\n",
    "train_accuracy = calculate_accuracy(model, dataloader_train)\n",
    "test_accuracy = calculate_accuracy(model, dataloader_test)\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SyedHamza",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
